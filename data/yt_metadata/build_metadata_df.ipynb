{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video-level metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a video-level metadata df using all the possibly interesting metadata available from the scraped info.jsons. Selected data from this df can then be joined with the matched extracted recommendations df as desired (this df will not contain any extraction-related info). \n",
    "\n",
    "We make use of the ``add_infojsons()`` function defined in ``scraping/scraping_utils.py`` to extract data from the info.jsons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('')), '..', 'scraping'))\n",
    "from scraping_utils import add_infojson_fields\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('')), '..', 'LLM_information_extraction'))\n",
    "from data_prep_utils import clean_text\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uploader_id', 'video_id', 'upload_date', 'yt_video_type', 'view_count',\n",
      "       'duration', 'language', 'title', 'description', 'yt_auto_categories',\n",
      "       'tags', 'first_three_tags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read in already existing metadata df from scraping section (which we will expand upon)\n",
    "df = pd.read_csv(\"../../scraping/6_filtered_videos_final/filtered_metadata.csv\", sep=\";\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Adding fields from info jsons to df (nrows: 45968, fields: ['like_count', 'comment_count', 'age_limit', 'chapters', 'uploader'])...\n",
      "2500/45968 rows processed\n",
      "5000/45968 rows processed\n",
      "7500/45968 rows processed\n",
      "10000/45968 rows processed\n",
      "12500/45968 rows processed\n",
      "15000/45968 rows processed\n",
      "17500/45968 rows processed\n",
      "20000/45968 rows processed\n",
      "22500/45968 rows processed\n",
      "25000/45968 rows processed\n",
      "27500/45968 rows processed\n",
      "30000/45968 rows processed\n",
      "32500/45968 rows processed\n",
      "35000/45968 rows processed\n",
      "37500/45968 rows processed\n",
      "40000/45968 rows processed\n",
      "42500/45968 rows processed\n",
      "45000/45968 rows processed\n",
      "adding intermediate lists to df...\n",
      "postprocessing...\n"
     ]
    }
   ],
   "source": [
    "infojsons_dir = \"../../scraping/5_transcripts_and_metadata/infojsons\"\n",
    "# define fields to be added from info jsons (if available)\n",
    "fields_to_add = [\"like_count\", \n",
    "                 \"comment_count\", \n",
    "                 \"age_limit\", \n",
    "                 \"chapters\", \n",
    "                 \"uploader\", # channel name (not id)\n",
    "                 ]\n",
    "\n",
    "# add fields\n",
    "df = add_infojson_fields(df, fields_to_add, infojsons_dir=infojsons_dir, print_missing_fields=False)\n",
    "\n",
    "# add has_chapters field\n",
    "if \"chapters\" in df.columns and not \"has_chapters\" in df.columns:\n",
    "    df[\"has_chapters\"] = df[\"chapters\"].apply(lambda x: True if x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 transcripts.\n",
      "Processed 5000 transcripts.\n",
      "Processed 7500 transcripts.\n",
      "Processed 10000 transcripts.\n",
      "Processed 12500 transcripts.\n",
      "Processed 15000 transcripts.\n",
      "Processed 17500 transcripts.\n",
      "Processed 20000 transcripts.\n",
      "Processed 22500 transcripts.\n",
      "Processed 25000 transcripts.\n",
      "Processed 27500 transcripts.\n",
      "Processed 30000 transcripts.\n",
      "Processed 32500 transcripts.\n",
      "Processed 35000 transcripts.\n",
      "Processed 37500 transcripts.\n",
      "Processed 40000 transcripts.\n",
      "Processed 42500 transcripts.\n",
      "Processed 45000 transcripts.\n",
      "Finished processing transcripts.\n"
     ]
    }
   ],
   "source": [
    "# add transcripts field (very memory intensive!)\n",
    "\n",
    "# import text cleaning function\n",
    "\n",
    "\n",
    "transcripts_path = \"../../scraping/5_transcripts_and_metadata/transcripts_csvs\"\n",
    "\n",
    "transcripts_list = []\n",
    "for i, row in df.iterrows():\n",
    "    uploader_id = row[\"uploader_id\"]\n",
    "    video_id = row[\"video_id\"]\n",
    "\n",
    "    # load transcript csv\n",
    "    if not os.path.exists(f\"{transcripts_path}/{uploader_id}_{video_id}.csv\"):\n",
    "        print(f\"Transcript for {uploader_id}_{video_id} does not exist.\")\n",
    "        transcripts_list.append(None)\n",
    "    else:\n",
    "        transcript_csv = pd.read_csv(f\"{transcripts_path}/{uploader_id}_{video_id}.csv\", sep=\";\")\n",
    "        # convert transcript to single string \n",
    "        transcript_text = \" \".join([line for line in transcript_csv.text if isinstance(line, str)]) # filter out empty lines\n",
    "        # clean\n",
    "        transcript_text = clean_text(transcript_text)\n",
    "        \n",
    "        if transcript_text == \"\":\n",
    "            transcript_text = None\n",
    "        transcripts_list.append(transcript_text)\n",
    "    # progress\n",
    "    if (i+1) % 2500 == 0:\n",
    "        print(f\"Processed {i+1} transcripts.\")\n",
    "        \n",
    "print(f\"Finished processing transcripts.\")\n",
    "# add column to df\n",
    "df[\"transcript\"] = transcripts_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save (only if file doesn't exist yet)\n",
    "save_path = \"video_metadata.csv\"\n",
    "if not os.path.exists(save_path):\n",
    "    df.to_csv(save_path, sep=\";\", index=False)\n",
    "else:\n",
    "    print(f\"File {save_path} already exists. Not saving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "#df_loaded = pd.read_csv(save_path, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
