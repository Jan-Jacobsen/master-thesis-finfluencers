{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Data\n",
    "\n",
    "We try to download available price data for all assets in our universe, as defined in the 'names_and_tickers' folder. The APIs return the following data: ``date``, ``open``, ``high``, ``low``, ``close``, ``adj_close``, ``volume`` (column names may differ between APIs). We only keep the ``adj_close`` column for each ticker. \n",
    "\n",
    "We join all data for one asset type into a single dataframe and save it to ``data/asset_data/raw/prices`` as csv. Then further processing (computing returns, missing data statistics etc.) is done and the results are saved to ``data/asset_data/prices`` and ``data/asset_data/returns``.\n",
    "\n",
    "### Sources\n",
    "\n",
    "- Stocks, ETFs and Cryptocurrencies: eodhd\n",
    "- Commodities (futures only): yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asset_data_utils as adu\n",
    "\n",
    "# timeframe of requested data\n",
    "start_date = \"2016-01-01\"\n",
    "end_date = \"2024-06-04\" # we need data AT LEAST to the end of 2023 (to have +1 year of performance data after the video scraping cutoff date). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads\n",
    "\n",
    "### Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_path = \"names_and_tickers/eodhd_stocks.csv\"\n",
    "tickers_colname = \"Code\" # for cryptos: \"Code_clean\"\n",
    "save_path = \"raw/prices/stocks_adj_close.csv\"\n",
    "progress_path = \"raw/prices/temp_stocks_progress.csv\"\n",
    "exchange_symbol =\"US\" # stocks/etfs\n",
    "min_time_between_requests = 0.1 # 0.06 sec is the eodhd minute limit (1k/min)\n",
    "######################################################################################################################################################\n",
    "# load tickers\n",
    "tickers = pd.read_csv(tickers_path, sep=\";\")[tickers_colname].tolist()\n",
    "print(f\"Loaded {len(tickers)} tickers.\")\n",
    "\n",
    "# download\n",
    "exceptions_list = adu.full_download(tickers=tickers, \n",
    "                                exchange_symbol=exchange_symbol, \n",
    "                                start_date=start_date, \n",
    "                                end_date=end_date, \n",
    "                                save_path=save_path, \n",
    "                                progress_path=progress_path, \n",
    "                                min_time_between_requests=min_time_between_requests,\n",
    "                                return_exceptions_list=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_path = \"names_and_tickers/eodhd_etfs.csv\"\n",
    "tickers_colname = \"Code\" # for cryptos: \"Code_clean\"\n",
    "save_path = \"raw/prices/etfs_adj_close.csv\"\n",
    "progress_path = \"raw/prices/temp_etfs_progress.csv\"\n",
    "exchange_symbol =\"US\" # stocks/etfs\n",
    "min_time_between_requests = 0.1 # 0.06 sec is the eodhd minute limit (1k/min)\n",
    "######################################################################################################################################################\n",
    "# load tickers\n",
    "tickers = pd.read_csv(tickers_path, sep=\";\")[tickers_colname].tolist()\n",
    "print(f\"Loaded {len(tickers)} tickers.\")\n",
    "\n",
    "\n",
    "# download\n",
    "exceptions_list = adu.full_download(tickers=tickers, \n",
    "                                exchange_symbol=exchange_symbol, \n",
    "                                start_date=start_date, \n",
    "                                end_date=end_date, \n",
    "                                save_path=save_path, \n",
    "                                progress_path=progress_path, \n",
    "                                min_time_between_requests=min_time_between_requests,\n",
    "                                return_exceptions_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_path = \"names_and_tickers/eodhd_cryptos.csv\"\n",
    "tickers_colname = \"Code_clean\" # for cryptos: \"Code_clean\"\n",
    "save_path = \"raw/prices/cryptos_adj_close.csv\"\n",
    "progress_path = \"raw/prices/temp_cryptos_progress.csv\"\n",
    "exchange_symbol =\"CC\" # stocks/etfs\n",
    "min_time_between_requests = 0.1 # 0.06 sec is the eodhd minute limit (1k/min)\n",
    "######################################################################################################################################################\n",
    "# load tickers\n",
    "tickers = pd.read_csv(tickers_path, sep=\";\")[tickers_colname].tolist()\n",
    "print(f\"Loaded {len(tickers)} tickers.\")\n",
    "\n",
    "\n",
    "# download\n",
    "exceptions_list = adu.full_download(tickers=tickers, \n",
    "                                exchange_symbol=exchange_symbol, \n",
    "                                start_date=start_date, \n",
    "                                end_date=end_date, \n",
    "                                save_path=save_path, \n",
    "                                progress_path=progress_path, \n",
    "                                min_time_between_requests=min_time_between_requests,\n",
    "                                return_exceptions_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commodities\n",
    "\n",
    "For commodities, we have to combine data from eodhd (etfs/stocks) and yahoo finance (futures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 tickers.\n",
      "Starting download of prices for 26 tickers from 2016-01-01 to 2024-06-04.\n",
      "------------------------------\n",
      "Completed all downloads and saved final results to raw/prices/commodities_adj_close.csv.\n",
      "------------------------------\n",
      "List of exceptions which occured: []\n"
     ]
    }
   ],
   "source": [
    "tickers_path = \"names_and_tickers/yahoo_eodhd_commodities.csv\"\n",
    "tickers_colname = \"Code\" # for cryptos: \"Code_clean\"\n",
    "data_source_colname = \"pricing_source\"\n",
    "save_path = \"raw/prices/commodities_adj_close.csv\"\n",
    "progress_path = \"raw/prices/temp_commodities_progress.csv\"\n",
    "exchange_symbol =\"US\" # stocks/etfs\n",
    "min_time_between_requests = 0.1 # 0.06 sec is the eodhd minute limit (1k/min)\n",
    "######################################################################################################################################################\n",
    "# load tickers and data sources\n",
    "df = pd.read_csv(tickers_path, sep=\";\")\n",
    "tickers = df[tickers_colname].tolist()\n",
    "data_source_list = df[data_source_colname].tolist()\n",
    "print(f\"Loaded {len(tickers)} tickers.\")\n",
    "\n",
    "\n",
    "# download\n",
    "exceptions_list = adu.full_download(tickers=tickers, \n",
    "                                exchange_symbol=exchange_symbol, \n",
    "                                start_date=start_date, \n",
    "                                end_date=end_date, \n",
    "                                save_path=save_path, \n",
    "                                progress_path=progress_path, \n",
    "                                min_time_between_requests=min_time_between_requests,\n",
    "                                data_source_list=data_source_list,\n",
    "                                return_exceptions_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "- Filter out non-trading days (some stocks/etfs errorenously have prices on weekends/holidays)\n",
    "  - Note: Cryptocurrencies trade continuously -> We keep two versions: one with all days and one with only US trading days (unclear at this point whether we will allow crypto trading on weekends/holidays in the portfolio simulation). \n",
    "- Add price availability information to names_and_tickers data\n",
    "  - Date of first/last available price\n",
    "  - Indicator for completely missing data during observation timeframe (should be mostly assets delisted before start of observation period)\n",
    "    - Can be used to remove these as matching candidates! \n",
    "- Compute and save returns\n",
    "    - discard assets with > x % missing price data or longest consecutive missing sequence > y (between first and last available price)\n",
    "    - for the remaining assets, use forward fill to fill in missing prices before computing returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2016-01-01\"\n",
    "end_date = \"2023-12-31\" # we have ~ 5 months of data from 2024 but here we make the cutoff at the end of 2023, which is sufficient for our analysis (1 year post video scraping cutoff date).  \n",
    "# get column of official US trading dates\n",
    "US_trading_dates = adu.get_US_trading_dates(start_date, end_date)\n",
    "\n",
    "# load raw price data\n",
    "stocks = pd.read_csv(\"raw/prices/stocks_adj_close.csv\", sep=\";\")\n",
    "etfs = pd.read_csv(\"raw/prices/etfs_adj_close.csv\", sep=\";\")\n",
    "commodities = pd.read_csv(\"raw/prices/commodities_adj_close.csv\", sep=\";\")\n",
    "cryptos = pd.read_csv(\"raw/prices/cryptos_adj_close.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Processing stocks data...\n",
      "-- 100/13288 tickers processed.\n",
      "-- 200/13288 tickers processed.\n",
      "-- 300/13288 tickers processed.\n",
      "-- 400/13288 tickers processed.\n",
      "-- 500/13288 tickers processed.\n",
      "-- 600/13288 tickers processed.\n",
      "-- 700/13288 tickers processed.\n",
      "-- 800/13288 tickers processed.\n",
      "-- 900/13288 tickers processed.\n",
      "-- 1000/13288 tickers processed.\n",
      "-- 1100/13288 tickers processed.\n",
      "-- 1200/13288 tickers processed.\n",
      "-- 1300/13288 tickers processed.\n",
      "-- 1400/13288 tickers processed.\n",
      "-- 1500/13288 tickers processed.\n",
      "-- 1600/13288 tickers processed.\n",
      "-- 1700/13288 tickers processed.\n",
      "-- 1800/13288 tickers processed.\n",
      "-- 1900/13288 tickers processed.\n",
      "-- 2000/13288 tickers processed.\n",
      "-- 2100/13288 tickers processed.\n",
      "-- 2200/13288 tickers processed.\n",
      "-- 2300/13288 tickers processed.\n",
      "-- 2400/13288 tickers processed.\n",
      "-- 2500/13288 tickers processed.\n",
      "-- 2600/13288 tickers processed.\n",
      "-- 2700/13288 tickers processed.\n",
      "-- 2800/13288 tickers processed.\n",
      "-- 2900/13288 tickers processed.\n",
      "-- 3000/13288 tickers processed.\n",
      "-- 3100/13288 tickers processed.\n",
      "-- 3200/13288 tickers processed.\n",
      "-- 3300/13288 tickers processed.\n",
      "-- 3400/13288 tickers processed.\n",
      "-- 3500/13288 tickers processed.\n",
      "-- 3600/13288 tickers processed.\n",
      "-- 3700/13288 tickers processed.\n",
      "-- 3800/13288 tickers processed.\n",
      "-- 3900/13288 tickers processed.\n",
      "-- 4000/13288 tickers processed.\n",
      "-- 4100/13288 tickers processed.\n",
      "-- 4200/13288 tickers processed.\n",
      "-- 4300/13288 tickers processed.\n",
      "-- 4400/13288 tickers processed.\n",
      "-- 4500/13288 tickers processed.\n",
      "-- 4600/13288 tickers processed.\n",
      "-- 4700/13288 tickers processed.\n",
      "-- 4800/13288 tickers processed.\n",
      "-- 4900/13288 tickers processed.\n",
      "-- 5000/13288 tickers processed.\n",
      "-- 5100/13288 tickers processed.\n",
      "-- 5200/13288 tickers processed.\n",
      "-- 5300/13288 tickers processed.\n",
      "-- 5400/13288 tickers processed.\n",
      "-- 5500/13288 tickers processed.\n",
      "-- 5600/13288 tickers processed.\n",
      "-- 5700/13288 tickers processed.\n",
      "-- 5800/13288 tickers processed.\n",
      "-- 5900/13288 tickers processed.\n",
      "-- 6000/13288 tickers processed.\n",
      "-- 6100/13288 tickers processed.\n",
      "-- 6200/13288 tickers processed.\n",
      "-- 6300/13288 tickers processed.\n",
      "-- 6400/13288 tickers processed.\n",
      "-- 6500/13288 tickers processed.\n",
      "-- 6600/13288 tickers processed.\n",
      "-- 6700/13288 tickers processed.\n",
      "-- 6800/13288 tickers processed.\n",
      "-- 6900/13288 tickers processed.\n",
      "-- 7000/13288 tickers processed.\n",
      "-- 7100/13288 tickers processed.\n",
      "-- 7200/13288 tickers processed.\n",
      "-- 7300/13288 tickers processed.\n",
      "-- 7400/13288 tickers processed.\n",
      "-- 7500/13288 tickers processed.\n",
      "-- 7600/13288 tickers processed.\n",
      "-- 7700/13288 tickers processed.\n",
      "-- 7800/13288 tickers processed.\n",
      "-- 7900/13288 tickers processed.\n",
      "-- 8000/13288 tickers processed.\n",
      "-- 8100/13288 tickers processed.\n",
      "-- 8200/13288 tickers processed.\n",
      "-- 8300/13288 tickers processed.\n",
      "-- 8400/13288 tickers processed.\n",
      "-- 8500/13288 tickers processed.\n",
      "-- 8600/13288 tickers processed.\n",
      "-- 8700/13288 tickers processed.\n",
      "-- 8800/13288 tickers processed.\n",
      "-- 8900/13288 tickers processed.\n",
      "-- 9000/13288 tickers processed.\n",
      "-- 9100/13288 tickers processed.\n",
      "-- 9200/13288 tickers processed.\n",
      "-- 9300/13288 tickers processed.\n",
      "-- 9400/13288 tickers processed.\n",
      "-- 9500/13288 tickers processed.\n",
      "-- 9600/13288 tickers processed.\n",
      "-- 9700/13288 tickers processed.\n",
      "-- 9800/13288 tickers processed.\n",
      "-- 9900/13288 tickers processed.\n",
      "-- 10000/13288 tickers processed.\n",
      "-- 10100/13288 tickers processed.\n",
      "-- 10200/13288 tickers processed.\n",
      "-- 10300/13288 tickers processed.\n",
      "-- 10400/13288 tickers processed.\n",
      "-- 10500/13288 tickers processed.\n",
      "-- 10600/13288 tickers processed.\n",
      "-- 10700/13288 tickers processed.\n",
      "-- 10800/13288 tickers processed.\n",
      "-- 10900/13288 tickers processed.\n",
      "-- 11000/13288 tickers processed.\n",
      "-- 11100/13288 tickers processed.\n",
      "-- 11200/13288 tickers processed.\n",
      "-- 11300/13288 tickers processed.\n",
      "-- 11400/13288 tickers processed.\n",
      "-- 11500/13288 tickers processed.\n",
      "-- 11600/13288 tickers processed.\n",
      "-- 11700/13288 tickers processed.\n",
      "-- 11800/13288 tickers processed.\n",
      "-- 11900/13288 tickers processed.\n",
      "-- 12000/13288 tickers processed.\n",
      "-- 12100/13288 tickers processed.\n",
      "-- 12200/13288 tickers processed.\n",
      "-- 12300/13288 tickers processed.\n",
      "-- 12400/13288 tickers processed.\n",
      "-- 12500/13288 tickers processed.\n",
      "-- 12600/13288 tickers processed.\n",
      "-- 12700/13288 tickers processed.\n",
      "-- 12800/13288 tickers processed.\n",
      "-- 12900/13288 tickers processed.\n",
      "-- 13000/13288 tickers processed.\n",
      "-- 13100/13288 tickers processed.\n",
      "-- 13200/13288 tickers processed.\n",
      "Return computation prep: Dropped 9238 tickers due to missing data.\n",
      "Return computation: Dropped 333 tickers due to likely presence of errors in price data.\n",
      "stocks data processed and saved.\n",
      "------------------------------\n",
      "Processing etfs data...\n",
      "-- 100/3841 tickers processed.\n",
      "-- 200/3841 tickers processed.\n",
      "-- 300/3841 tickers processed.\n",
      "-- 400/3841 tickers processed.\n",
      "-- 500/3841 tickers processed.\n",
      "-- 600/3841 tickers processed.\n",
      "-- 700/3841 tickers processed.\n",
      "-- 800/3841 tickers processed.\n",
      "-- 900/3841 tickers processed.\n",
      "-- 1000/3841 tickers processed.\n",
      "-- 1100/3841 tickers processed.\n",
      "-- 1200/3841 tickers processed.\n",
      "-- 1300/3841 tickers processed.\n",
      "-- 1400/3841 tickers processed.\n",
      "-- 1500/3841 tickers processed.\n",
      "-- 1600/3841 tickers processed.\n",
      "-- 1700/3841 tickers processed.\n",
      "-- 1800/3841 tickers processed.\n",
      "-- 1900/3841 tickers processed.\n",
      "-- 2000/3841 tickers processed.\n",
      "-- 2100/3841 tickers processed.\n",
      "-- 2200/3841 tickers processed.\n",
      "-- 2300/3841 tickers processed.\n",
      "-- 2400/3841 tickers processed.\n",
      "-- 2500/3841 tickers processed.\n",
      "-- 2600/3841 tickers processed.\n",
      "-- 2700/3841 tickers processed.\n",
      "-- 2800/3841 tickers processed.\n",
      "-- 2900/3841 tickers processed.\n",
      "-- 3000/3841 tickers processed.\n",
      "-- 3100/3841 tickers processed.\n",
      "-- 3200/3841 tickers processed.\n",
      "-- 3300/3841 tickers processed.\n",
      "-- 3400/3841 tickers processed.\n",
      "-- 3500/3841 tickers processed.\n",
      "-- 3600/3841 tickers processed.\n",
      "-- 3700/3841 tickers processed.\n",
      "-- 3800/3841 tickers processed.\n",
      "Return computation prep: Dropped 844 tickers due to missing data.\n",
      "Return computation: Dropped 31 tickers due to likely presence of errors in price data.\n",
      "etfs data processed and saved.\n",
      "------------------------------\n",
      "Processing commodities data...\n",
      "Return computation prep: Dropped 0 tickers due to missing data.\n",
      "Return computation: Dropped 0 tickers due to likely presence of errors in price data.\n",
      "commodities data processed and saved.\n",
      "------------------------------\n",
      "Processing cryptos data...\n",
      "Applied manual price fix for SAFEMOON on 2018-08-07.\n",
      "Applied manual price fix for AXS on 2022-02-10.\n",
      "Applied manual price fix for LUNA on 2022-05-11.\n",
      "Applied manual price fix for GALA on 2018-01-31.\n",
      "Applied manual price fix for ICP on 2021-05-06.\n",
      "-- 100/6867 tickers processed.\n",
      "-- 200/6867 tickers processed.\n",
      "-- 300/6867 tickers processed.\n",
      "-- 400/6867 tickers processed.\n",
      "-- 500/6867 tickers processed.\n",
      "-- 600/6867 tickers processed.\n",
      "-- 700/6867 tickers processed.\n",
      "-- 800/6867 tickers processed.\n",
      "-- 900/6867 tickers processed.\n",
      "-- 1000/6867 tickers processed.\n",
      "-- 1100/6867 tickers processed.\n",
      "-- 1200/6867 tickers processed.\n",
      "-- 1300/6867 tickers processed.\n",
      "-- 1400/6867 tickers processed.\n",
      "-- 1500/6867 tickers processed.\n",
      "-- 1600/6867 tickers processed.\n",
      "-- 1700/6867 tickers processed.\n",
      "-- 1800/6867 tickers processed.\n",
      "-- 1900/6867 tickers processed.\n",
      "-- 2000/6867 tickers processed.\n",
      "-- 2100/6867 tickers processed.\n",
      "-- 2200/6867 tickers processed.\n",
      "-- 2300/6867 tickers processed.\n",
      "-- 2400/6867 tickers processed.\n",
      "-- 2500/6867 tickers processed.\n",
      "-- 2600/6867 tickers processed.\n",
      "-- 2700/6867 tickers processed.\n",
      "-- 2800/6867 tickers processed.\n",
      "-- 2900/6867 tickers processed.\n",
      "-- 3000/6867 tickers processed.\n",
      "-- 3100/6867 tickers processed.\n",
      "-- 3200/6867 tickers processed.\n",
      "-- 3300/6867 tickers processed.\n",
      "-- 3400/6867 tickers processed.\n",
      "-- 3500/6867 tickers processed.\n",
      "-- 3600/6867 tickers processed.\n",
      "-- 3700/6867 tickers processed.\n",
      "-- 3800/6867 tickers processed.\n",
      "-- 3900/6867 tickers processed.\n",
      "-- 4000/6867 tickers processed.\n",
      "-- 4100/6867 tickers processed.\n",
      "-- 4200/6867 tickers processed.\n",
      "-- 4300/6867 tickers processed.\n",
      "-- 4400/6867 tickers processed.\n",
      "-- 4500/6867 tickers processed.\n",
      "-- 4600/6867 tickers processed.\n",
      "-- 4700/6867 tickers processed.\n",
      "-- 4800/6867 tickers processed.\n",
      "-- 4900/6867 tickers processed.\n",
      "-- 5000/6867 tickers processed.\n",
      "-- 5100/6867 tickers processed.\n",
      "-- 5200/6867 tickers processed.\n",
      "-- 5300/6867 tickers processed.\n",
      "-- 5400/6867 tickers processed.\n",
      "-- 5500/6867 tickers processed.\n",
      "-- 5600/6867 tickers processed.\n",
      "-- 5700/6867 tickers processed.\n",
      "-- 5800/6867 tickers processed.\n",
      "-- 5900/6867 tickers processed.\n",
      "-- 6000/6867 tickers processed.\n",
      "-- 6100/6867 tickers processed.\n",
      "-- 6200/6867 tickers processed.\n",
      "-- 6300/6867 tickers processed.\n",
      "-- 6400/6867 tickers processed.\n",
      "-- 6500/6867 tickers processed.\n",
      "-- 6600/6867 tickers processed.\n",
      "-- 6700/6867 tickers processed.\n",
      "-- 6800/6867 tickers processed.\n",
      "Return computation prep: Dropped 2251 tickers due to missing data.\n",
      "Return computation: Dropped 1281 tickers due to likely presence of errors in price data.\n",
      "cryptos data processed and saved.\n"
     ]
    }
   ],
   "source": [
    "asset_data = [(\"stocks\", stocks), (\"etfs\", etfs), (\"commodities\", commodities), (\"cryptos\", cryptos)]\n",
    "#asset_data = [(\"commodities\", commodities)] # for testing\n",
    "\n",
    "for name, df in asset_data:\n",
    "        \n",
    "    print(f\"{'-'*30}\\nProcessing {name} data...\")\n",
    "\n",
    "    # set date col as index for the process\n",
    "    df = df.set_index(\"date\")\n",
    "\n",
    "    ### price fixes\n",
    "    # - some price series have exact zeros (either rounded from very small numbers or as placeholder for missing data, or errors)\n",
    "    # - negative price values are always errors (except for 2020-04-20 in the crude oil futures contract!)\n",
    "    # -> to avoid issues with returns computation, we replace these values with NaNs\n",
    "\n",
    "    df = df.where(df > 0, other=pd.NA)\n",
    "\n",
    "    # applying manual fixes of faulty data for specific tickers\n",
    "    for fix in adu.manual_price_fixes_dict[name]:\n",
    "        df.loc[fix[\"date\"], fix[\"ticker\"]] = fix[\"new_val\"]\n",
    "        print(f\"Applied manual price fix for {fix['ticker']} on {fix['date']}.\")\n",
    "\n",
    "    ### keep only trading day rows\n",
    "    prices = pd.merge(US_trading_dates.set_index(\"date\"), df, on=\"date\", how=\"left\")\n",
    "    ### save date-filtered prices (to prices folder, not raw/prices!)\n",
    "    prices.to_csv(f\"prices/{name}_adj_close.csv\", sep=\";\", index=True, index_label=\"date\")\n",
    "\n",
    "    ### add data availability to names_and_tickers_with_price_availability files\n",
    "    nat_df = pd.read_csv(f\"names_and_tickers/{'yahoo_' if name=='commodities' else ''}eodhd_{name}.csv\", sep=\";\")\n",
    "    tcol = \"Code\" if name != \"cryptos\" else \"Code_clean\" # column name for tickers in nat_df\n",
    "    # temporarily set ticker col as index for faster lookups\n",
    "    nat_df = nat_df.set_index(tcol)\n",
    "    # create new columns\n",
    "    nat_df[\"earliest_price_date\"] = None\n",
    "    nat_df[\"latest_price_date\"] = None\n",
    "    nat_df[\"has_price_data\"] = False # will be set true if any prices in observation period are available\n",
    "    nat_df[\"n_trading_days_between_earliest_and_latest\"] = None # considering only days between earliest and latest date\n",
    "    nat_df[\"n_prices_between_earliest_and_latest\"] = None\n",
    "    nat_df[\"n_missing_prices_between_earliest_and_latest\"] = None\n",
    "    nat_df[\"missing_prices_ratio\"] = None # considering only days between earliest and latest date\n",
    "    nat_df[\"longest_missing_price_sequence\"] = None # considering only days between earliest and latest date\n",
    "\n",
    "    # compute values for priced tickers and enter them in nat_df\n",
    "    # to-do if time: replace with better code, currently very inefficient\n",
    "    for i, t in enumerate(prices.columns):\n",
    "        \n",
    "        # define shortcut references\n",
    "        p = prices[t] # single price series with date index\n",
    "        \n",
    "        earliest_date = p.first_valid_index() # None if no data\n",
    "        latest_date = p.last_valid_index() # NaN if no data\n",
    "        nat_df.loc[t, \"earliest_price_date\"] = earliest_date\n",
    "        nat_df.loc[t, \"latest_price_date\"] = latest_date\n",
    "        has_price_data = not pd.isnull(earliest_date)\n",
    "        nat_df.loc[t, \"has_price_data\"] = has_price_data\n",
    "        if has_price_data:\n",
    "            p_seq_idx = (p.index >= earliest_date) & (p.index <= latest_date)\n",
    "            nat_df.loc[t, \"n_trading_days_between_earliest_and_latest\"] = p[p_seq_idx].shape[0]\n",
    "            nat_df.loc[t, \"n_prices_between_earliest_and_latest\"] = p[p_seq_idx].notnull().sum()\n",
    "            nat_df.loc[t, \"n_missing_prices_between_earliest_and_latest\"] = p[p_seq_idx].isnull().sum()\n",
    "            nat_df.loc[t, \"missing_prices_ratio\"] = nat_df.loc[t, \"n_missing_prices_between_earliest_and_latest\"] / nat_df.loc[t, \"n_trading_days_between_earliest_and_latest\"]\n",
    "            longest_missing_sequence = p[p_seq_idx].isnull().astype(int).groupby(p.notnull().cumsum()).cumcount().max()\n",
    "            nat_df.loc[t, \"longest_missing_price_sequence\"] = longest_missing_sequence\n",
    "        \n",
    "        if (i+1)%100==0:\n",
    "            print(f\"-- {i+1}/{len(prices.columns)-1} tickers processed.\")\n",
    "\n",
    "    ### compute returns\n",
    "    # drop ineligible tickers (due to too many missing prices)\n",
    "    to_drop = nat_df[(~nat_df[\"has_price_data\"]) | # no price data at all\n",
    "                     (nat_df[\"missing_prices_ratio\"] > 0.2) |# ratio of missing prices too high (we can keep this pretty high if the longest_missing_sequence is low enough. sometimes assets have a period of spotty data at the beginning or end, but also periods of good data which we want to preserve.)\n",
    "                     (nat_df[\"longest_missing_price_sequence\"] > 20) |# too many consecutive missing prices\n",
    "                     (nat_df[\"n_prices_between_earliest_and_latest\"] < 10) # too few prices in total\n",
    "                     ].index.tolist()\n",
    "    prices_df = prices.drop(columns=[t for t in to_drop if t in prices.columns])\n",
    "    print(f\"Return computation prep: Dropped {len(to_drop)} tickers due to missing data.\")\n",
    "    \n",
    "    # get returns df\n",
    "    returns_df = adu.get_returns_from_prices(prices_df, drop_full_na_cols=True)\n",
    "\n",
    "    ### detect likely errors \n",
    "    # (e.g. excessive one-day returns due to price data errors) -> drop the affected tickers\n",
    "    returns_df, dropped_errors = adu.detect_errors_in_returns(returns_df, name, drop_cols_with_likely_errors=True)\n",
    "    print(f\"Return computation: Dropped {len(dropped_errors)} tickers due to likely presence of errors in price data.\")\n",
    "\n",
    "    ### save returns to returns folder\n",
    "    #returns_df = returns_df.reset_index() # get dates back as column\n",
    "    returns_df.to_csv(f\"returns/{name}_returns.csv\", sep=\";\", index=True, index_label=\"date\")\n",
    "\n",
    "    ### save nat df with price info to names_and_tickers_with_price_availability folder\n",
    "\n",
    "    # add column to indicate whether price data is likely to be faulty (-> returns should not be used, or have already been dropped)\n",
    "    nat_df[\"price_data_errors_likely\"] = nat_df.index.isin(dropped_errors)\n",
    "    # add column to indicate whether returns have been computed and are available\n",
    "    nat_df[\"has_returns\"] = nat_df.index.isin(returns_df.columns)\n",
    "    # reset nat index (tcol)\n",
    "    nat_df = nat_df.reset_index()\n",
    "    nat_df.to_csv(f\"names_and_tickers_with_price_availability/{name}.csv\", sep=\";\", index=False)\n",
    "\n",
    "    print(f\"{name} data processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Checks / Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking stocks returns for missing sequences...\n",
      "stocks returns checked.\n",
      "Checking etfs returns for missing sequences...\n",
      "etfs returns checked.\n",
      "Checking commodities returns for missing sequences...\n",
      "commodities returns checked.\n",
      "Checking cryptos returns for missing sequences...\n",
      "cryptos returns checked.\n"
     ]
    }
   ],
   "source": [
    "# check returns dfs: are there any missing values inbetween first and last?\n",
    "for name in [\"stocks\", \"etfs\", \"commodities\", \"cryptos\"]:\n",
    "    returns = pd.read_csv(f\"returns/{name}_returns.csv\", sep=\";\")\n",
    "    print(f\"Checking {name} returns for missing sequences...\")\n",
    "    for i, ticker in enumerate(returns.columns[1:]):\n",
    "        first_non_na = returns[ticker].first_valid_index()\n",
    "        last_non_na = returns[ticker].last_valid_index()\n",
    "        len_missing_sequence_between_first_and_last = returns.loc[first_non_na:last_non_na, ticker].isnull().astype(int).groupby(returns[ticker].notnull().cumsum()).cumcount().max()\n",
    "        if len_missing_sequence_between_first_and_last > 0:\n",
    "            print(f\"{ticker}: {len_missing_sequence_between_first_and_last} missing values in sequence.\")\n",
    "    print(f\"{name} returns checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "STOCKS:\n",
      "\n",
      "Total extracted tickers relevant for trading: 29913\n",
      "Of those, 27624 have returns data.\n",
      "\n",
      "Unique extracted tickers relevant for trading: 3525\n",
      "Of those, 2623 have returns data.\n",
      "\n",
      "Top 50 tickers with most extractions:\n",
      "    Code  count  has_return_data\n",
      "0   TSLA   1726             True\n",
      "1   AAPL    894             True\n",
      "2    NIO    754             True\n",
      "3   AMZN    509             True\n",
      "4   META    487             True\n",
      "5    AMC    432             True\n",
      "6   BABA    384             True\n",
      "7   PLTR    382             True\n",
      "8   MSFT    380             True\n",
      "9    DIS    301             True\n",
      "10  NVDA    284             True\n",
      "11  DKNG    257             True\n",
      "12  TTCF    219             True\n",
      "13  CHPT    210             True\n",
      "14   JNJ    206             True\n",
      "15  GOOG    204             True\n",
      "16     T    203             True\n",
      "17    SQ    200             True\n",
      "18   AMD    196             True\n",
      "19  LCID    192             True\n",
      "20    BA    188             True\n",
      "21   GME    187             True\n",
      "22  NFLX    181             True\n",
      "23   MCD    175             True\n",
      "24    KO    174             True\n",
      "25  SBUX    159             True\n",
      "26   MMM    151             True\n",
      "27   JPM    144             True\n",
      "28    HD    136             True\n",
      "29  WKHS    134             True\n",
      "30  CCIV    133             True\n",
      "31  PTON    130             True\n",
      "32  INTC    130             True\n",
      "33   WMT    129             True\n",
      "34  UBER    126             True\n",
      "35    ZM    124             True\n",
      "36     O    121             True\n",
      "37   PEP    118             True\n",
      "38  SHOP    118             True\n",
      "39  SNAP    117             True\n",
      "40     F    117             True\n",
      "41  NNDM    115             True\n",
      "42  BNGO    115             True\n",
      "43   CCL    113             True\n",
      "44  PYPL    111             True\n",
      "45   BAC    111             True\n",
      "46  CRSR    110             True\n",
      "47   PFE    109             True\n",
      "48   XOM    107             True\n",
      "49  ABBV    107             True\n",
      "------------------------------\n",
      "ETFS:\n",
      "\n",
      "Total extracted tickers relevant for trading: 1846\n",
      "Of those, 1768 have returns data.\n",
      "\n",
      "Unique extracted tickers relevant for trading: 431\n",
      "Of those, 393 have returns data.\n",
      "\n",
      "Top 50 tickers with most extractions:\n",
      "    Code  count  has_return_data\n",
      "0    SPY    209             True\n",
      "1    VTI     74             True\n",
      "2    QQQ     50             True\n",
      "3    TLT     50             True\n",
      "4   ARKK     44             True\n",
      "5   QYLD     35             True\n",
      "6   SCHD     32             True\n",
      "7   TQQQ     28             True\n",
      "8    GLD     27             True\n",
      "9   GBTC     27             True\n",
      "10   VYM     24             True\n",
      "11   VNQ     24             True\n",
      "12   IWM     21             True\n",
      "13  ACES     18             True\n",
      "14   UUP     18             True\n",
      "15   BND     18             True\n",
      "16  JPRE     18             True\n",
      "17  RYLD     18             True\n",
      "18   VOO     17             True\n",
      "19   USO     16             True\n",
      "20  IBDZ     15            False\n",
      "21   GDX     15             True\n",
      "22   DIA     15             True\n",
      "23  NUSI     15             True\n",
      "24  AMLP     13             True\n",
      "25   HYG     13             True\n",
      "26  ARKF     13             True\n",
      "27  HYLD     12             True\n",
      "28   EEM     12             True\n",
      "29   VUG     12             True\n",
      "30  AEMB     11             True\n",
      "31   SLV     10             True\n",
      "32   VEU     10             True\n",
      "33   VIG     10             True\n",
      "34   BLV     10             True\n",
      "35  JNUG     10             True\n",
      "36  VXUS     10             True\n",
      "37    VO     10             True\n",
      "38  AUMI     10             True\n",
      "39  GPIQ      9             True\n",
      "40  DGAZ      9             True\n",
      "41   PBW      9             True\n",
      "42  JEPI      9             True\n",
      "43   TAN      9             True\n",
      "44  ARVR      8             True\n",
      "45   XLE      8             True\n",
      "46  FSTA      8             True\n",
      "47  JETS      8             True\n",
      "48  EUFN      8             True\n",
      "49  ARKW      8             True\n",
      "------------------------------\n",
      "COMMODITYS:\n",
      "\n",
      "Total extracted tickers relevant for trading: 916\n",
      "Of those, 916 have returns data.\n",
      "\n",
      "Unique extracted tickers relevant for trading: 22\n",
      "Of those, 22 have returns data.\n",
      "\n",
      "Top 50 tickers with most extractions:\n",
      "     Code  count  has_return_data\n",
      "0    GC=F    393             True\n",
      "1    SI=F    305             True\n",
      "2    CL=F     42             True\n",
      "3    NG=F     42             True\n",
      "4    HG=F     33             True\n",
      "5     URA     28             True\n",
      "6    PL=F     21             True\n",
      "7    PA=F     17             True\n",
      "8     LIT      7             True\n",
      "9     DBC      4             True\n",
      "10    RIO      3             True\n",
      "11   KC=F      3             True\n",
      "12   SB=F      3             True\n",
      "13   BZ=F      3             True\n",
      "14   ZW=F      2             True\n",
      "15   ZC=F      2             True\n",
      "16  ALI=F      2             True\n",
      "17   CT=F      2             True\n",
      "18   OJ=F      1             True\n",
      "19   HE=F      1             True\n",
      "20   ZS=F      1             True\n",
      "21   CC=F      1             True\n",
      "------------------------------\n",
      "CRYPTOS:\n",
      "\n",
      "Total extracted tickers relevant for trading: 12710\n",
      "Of those, 12205 have returns data.\n",
      "\n",
      "Unique extracted tickers relevant for trading: 873\n",
      "Of those, 637 have returns data.\n",
      "\n",
      "Top 50 tickers with most extractions:\n",
      "   Code_clean  count  has_return_data\n",
      "0         BTC   2633             True\n",
      "1         ETH   1790             True\n",
      "2         ADA    805             True\n",
      "3         SOL    406             True\n",
      "4         DOT    375             True\n",
      "5        SHIB    369             True\n",
      "6         XRP    343             True\n",
      "7        DOGE    290             True\n",
      "8        LINK    277             True\n",
      "9         VET    194             True\n",
      "10      MATIC    190             True\n",
      "11        LTC    172             True\n",
      "12       AVAX    133             True\n",
      "13        BNB    111             True\n",
      "14    UNI7083    105             True\n",
      "15       LUNC    100             True\n",
      "16       SAND     99             True\n",
      "17       MANA     97             True\n",
      "18        ZIL     95             True\n",
      "19       ATOM     93             True\n",
      "20   MATICPAD     80             True\n",
      "21       EGLD     67             True\n",
      "22        XLM     65             True\n",
      "23      THETA     64             True\n",
      "24        SNX     63             True\n",
      "25       SOUL     59             True\n",
      "26        NEO     58             True\n",
      "27      CBETH     58             True\n",
      "28       ZOON     51             True\n",
      "29        VGX     50             True\n",
      "30       EGCC     47             True\n",
      "31        RSR     46             True\n",
      "32        SFM     45             True\n",
      "33        XMR     45             True\n",
      "34        XTZ     44             True\n",
      "35        KSM     41             True\n",
      "36       ALGO     41             True\n",
      "37       AAVE     40             True\n",
      "38        BAT     35             True\n",
      "39        UTK     35             True\n",
      "40    ONE3945     34             True\n",
      "41        AXS     33             True\n",
      "42      SAITO     33             True\n",
      "43        TRX     32             True\n",
      "44        EOS     32             True\n",
      "45        BCH     31             True\n",
      "46        ETC     31             True\n",
      "47        INJ     31             True\n",
      "48        FTT     30             True\n",
      "49       CAKE     28             True\n",
      "------------------------------\n",
      "CRYPTOS:\n",
      "\n",
      "Total extracted tickers relevant for trading: 12710\n",
      "Of those, 12205 have returns data.\n",
      "\n",
      "Unique extracted tickers relevant for trading: 873\n",
      "Of those, 637 have returns data.\n",
      "\n",
      "Top 50 tickers with most extractions:\n",
      "   Code_clean  count  has_return_data\n",
      "0         BTC   2633             True\n",
      "1         ETH   1790             True\n",
      "2         ADA    805             True\n",
      "3         SOL    406             True\n",
      "4         DOT    375             True\n",
      "5        SHIB    369             True\n",
      "6         XRP    343             True\n",
      "7        DOGE    290             True\n",
      "8        LINK    277             True\n",
      "9         VET    194             True\n",
      "10      MATIC    190             True\n",
      "11        LTC    172             True\n",
      "12       AVAX    133             True\n",
      "13        BNB    111             True\n",
      "14    UNI7083    105             True\n",
      "15       LUNC    100             True\n",
      "16       SAND     99             True\n",
      "17       MANA     97             True\n",
      "18        ZIL     95             True\n",
      "19       ATOM     93             True\n",
      "20   MATICPAD     80             True\n",
      "21       EGLD     67             True\n",
      "22        XLM     65             True\n",
      "23      THETA     64             True\n",
      "24        SNX     63             True\n",
      "25       SOUL     59             True\n",
      "26        NEO     58             True\n",
      "27      CBETH     58             True\n",
      "28       ZOON     51             True\n",
      "29        VGX     50             True\n",
      "30       EGCC     47             True\n",
      "31        RSR     46             True\n",
      "32        SFM     45             True\n",
      "33        XMR     45             True\n",
      "34        XTZ     44             True\n",
      "35        KSM     41             True\n",
      "36       ALGO     41             True\n",
      "37       AAVE     40             True\n",
      "38        BAT     35             True\n",
      "39        UTK     35             True\n",
      "40    ONE3945     34             True\n",
      "41        AXS     33             True\n",
      "42      SAITO     33             True\n",
      "43        TRX     32             True\n",
      "44        EOS     32             True\n",
      "45        BCH     31             True\n",
      "46        ETC     31             True\n",
      "47        INJ     31             True\n",
      "48        FTT     30             True\n",
      "49       CAKE     28             True\n"
     ]
    }
   ],
   "source": [
    "# check how many extracted & matched tickers have no returns data (at all, we don't consider video upload date here)\n",
    "# if it's too many it might be worth considering to rerun the matching process and omit tickers without returns data. \n",
    "# (would be trivial with the now available has_returns column in the names_and_tickers_with_price_availability files)\n",
    "\n",
    "import json\n",
    "\n",
    "video_df = pd.read_csv(\"../matched/VIDEOS_inf_llama3_ft_v4_q8_0_llamacpp_guided.csv\", sep=\";\")\n",
    "\n",
    "nat_data = [(\"stock\", pd.read_csv(\"names_and_tickers_with_price_availability/stocks.csv\", sep=\";\")),\n",
    "            (\"etf\", pd.read_csv(\"names_and_tickers_with_price_availability/etfs.csv\", sep=\";\")),\n",
    "            (\"commodity\", pd.read_csv(\"names_and_tickers_with_price_availability/commodities.csv\", sep=\";\")),\n",
    "            (\"crypto\", pd.read_csv(\"names_and_tickers_with_price_availability/cryptos.csv\", sep=\";\"))\n",
    "            ]\n",
    "for asset_type, nat in nat_data:\n",
    "\n",
    "    tcol = \"Code\" if asset_type != \"crypto\" else \"Code_clean\" # name of ticker column in nat\n",
    "\n",
    "    # get every ticker extraction relevant for trading (= non-neutral sentiment) in a list (incl. duplicates)\n",
    "    all_tickers = []\n",
    "    for l in video_df[\"trade_info_incl_neutrals\"]:\n",
    "        for a in json.loads(l):\n",
    "            if a[\"asset_type\"] == asset_type and a[\"sentiment\"] != \"neutral\":\n",
    "                all_tickers.append(a[\"ticker\"])\n",
    "    # ticker value counts\n",
    "    ticker_counts = pd.Series(all_tickers).value_counts()\n",
    "    ticker_counts = pd.DataFrame(ticker_counts).reset_index().rename(columns={\"index\": tcol, 0: \"count\"})\n",
    "    tickers_with_returns = nat[nat[\"has_returns\"]][tcol].tolist()\n",
    "    ticker_counts[\"has_return_data\"] = ticker_counts[tcol].isin(tickers_with_returns)\n",
    "\n",
    "    # print results\n",
    "    print(f\"{'-'*30}\\n{asset_type.upper()}S:\\n\")\n",
    "    print(f\"Total extracted tickers relevant for trading: {len(all_tickers)}\")\n",
    "    print(f\"Of those, {ticker_counts[ticker_counts['has_return_data']]['count'].sum()} have returns data.\\n\")\n",
    "    print(f\"Unique extracted tickers relevant for trading: {len(ticker_counts)}\")\n",
    "    print(f\"Of those, {ticker_counts['has_return_data'].sum()} have returns data.\\n\")\n",
    "    print(f\"Top 50 tickers with most extractions:\\n{ticker_counts.head(50)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code_clean</th>\n",
       "      <th>Code</th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Type</th>\n",
       "      <th>Isin</th>\n",
       "      <th>delisted_as_of_may_2024</th>\n",
       "      <th>in_top200_as_of_dec_2022</th>\n",
       "      <th>earliest_price_date</th>\n",
       "      <th>latest_price_date</th>\n",
       "      <th>has_price_data</th>\n",
       "      <th>n_trading_days_between_earliest_and_latest</th>\n",
       "      <th>n_prices_between_earliest_and_latest</th>\n",
       "      <th>n_missing_prices_between_earliest_and_latest</th>\n",
       "      <th>missing_prices_ratio</th>\n",
       "      <th>longest_missing_price_sequence</th>\n",
       "      <th>price_data_errors_likely</th>\n",
       "      <th>has_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AXS</td>\n",
       "      <td>AXS-USD</td>\n",
       "      <td>Axie Infinity</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>CC</td>\n",
       "      <td>USD</td>\n",
       "      <td>Currency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>True</td>\n",
       "      <td>765.0</td>\n",
       "      <td>765.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code_clean     Code           Name  Country Exchange Currency      Type  \\\n",
       "16        AXS  AXS-USD  Axie Infinity  Unknown       CC      USD  Currency   \n",
       "\n",
       "    Isin  delisted_as_of_may_2024  in_top200_as_of_dec_2022  \\\n",
       "16   NaN                    False                      True   \n",
       "\n",
       "   earliest_price_date latest_price_date  has_price_data  \\\n",
       "16          2020-12-15        2023-12-29            True   \n",
       "\n",
       "    n_trading_days_between_earliest_and_latest  \\\n",
       "16                                       765.0   \n",
       "\n",
       "    n_prices_between_earliest_and_latest  \\\n",
       "16                                 765.0   \n",
       "\n",
       "    n_missing_prices_between_earliest_and_latest  missing_prices_ratio  \\\n",
       "16                                           0.0                   0.0   \n",
       "\n",
       "    longest_missing_price_sequence  price_data_errors_likely  has_returns  \n",
       "16                             0.0                      True        False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"AXS-USD\"\n",
    "asset_type = \"cryptos\"\n",
    "nat = pd.read_csv(f\"names_and_tickers_with_price_availability/{asset_type}.csv\", sep=\";\")\n",
    "nat[nat[\"Code\"]==t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Data\n",
    "\n",
    "Separately from the other asset classes, we also want to have a returns data file with assets to be used for benchmarking or as the neutral asset in portfolio computation. The following should be included:\n",
    "\n",
    "- cash (no downloads needed, all zero returns)\n",
    "- 3 month US treasury bill (``^IRX`` on yahoo finance) as proxy for risk-free rate -> needs to be converted to daily returns! See ``get_daily_3m_tbill_returns()`` in ``asset_data_utils.py``.\n",
    "- ``SPY`` as proxy for S&P 500\n",
    "- ``VTI`` as proxy for US total stock market\n",
    "- ``VT`` as proxy for global stock market\n",
    "- ``GLD`` as proxy for gold\n",
    "\n",
    "(etf returns are already availablee, no need to download and compute returns again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2016-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "# initialize returns df with date column\n",
    "benchmark_returns = pd.DataFrame(adu.get_US_trading_dates(start_date, end_date), columns=[\"date\"])\n",
    "\n",
    "# get benchmark data\n",
    "\n",
    "# 1. cash\n",
    "benchmark_returns[\"cash\"] = 0.0\n",
    "# 2. download/compute new: 3 month T-bill returns\n",
    "tbill_returns = adu.get_daily_3m_tbill_returns(start_date, end_date)\n",
    "benchmark_returns = pd.merge(benchmark_returns, tbill_returns, on=\"date\", how=\"left\")\n",
    "# 3. get from etf data\n",
    "tickers = [\"SPY\", \"VTI\", \"VT\", \"GLD\"]\n",
    "etf_returns = pd.read_csv(\"returns/etfs_returns.csv\", sep=\";\")\n",
    "for t in tickers:\n",
    "    benchmark_returns = pd.merge(benchmark_returns, etf_returns[[\"date\", t]], on=\"date\", how=\"left\")\n",
    "\n",
    "# save benchmark returns data\n",
    "benchmark_returns.to_csv(\"returns/benchmarks_returns.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
