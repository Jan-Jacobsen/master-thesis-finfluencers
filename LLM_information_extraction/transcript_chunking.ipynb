{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: \n",
    "- Transcripts (+ prompt and output text) usually exceed maximum acceptable LLM context length. \n",
    "\n",
    "Strategy: \n",
    "\n",
    "1. Split transcript into chunks of a length less than (but probably close to) the maximum context length of the model minus the context size required for prompt + expected output. \n",
    "    - How to choose appropriate split locations (to avoid losing context as much as possible)?\n",
    "      - With sufficiently big context size splitting at almost the exact suggested word should be fine. \n",
    "2. Get LLM output for each chunk.\n",
    "3. Combine the outputs of all chunks into a single output (using a function, i.e. no further LLM call). \n",
    "\n",
    "\n",
    "Required Parameters:\n",
    "- maximum acceptable context length (can be less than the actual model context length, depending on output quality and memory constraints)\n",
    "- prompt context size \n",
    "- max expected (allowed?) output size\n",
    "- model (-> which tokenizer to use?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uploader_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>yt_video_type</th>\n",
       "      <th>channel_avg_view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>i2bUeO1ID30</td>\n",
       "      <td>63002469.0</td>\n",
       "      <td>short</td>\n",
       "      <td>7.569842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>VvEBCXHx-74</td>\n",
       "      <td>48360277.0</td>\n",
       "      <td>short</td>\n",
       "      <td>7.569842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>CEdnanNgS3k</td>\n",
       "      <td>39446140.0</td>\n",
       "      <td>short</td>\n",
       "      <td>7.569842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>jOc1XfFNJTo</td>\n",
       "      <td>30411496.0</td>\n",
       "      <td>short</td>\n",
       "      <td>7.569842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>Gs0QiMVkUAw</td>\n",
       "      <td>29434395.0</td>\n",
       "      <td>short</td>\n",
       "      <td>7.569842e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      uploader_id     video_id  view_count yt_video_type  \\\n",
       "0  @JennyHoyosLOL  i2bUeO1ID30  63002469.0         short   \n",
       "1  @JennyHoyosLOL  VvEBCXHx-74  48360277.0         short   \n",
       "2  @JennyHoyosLOL  CEdnanNgS3k  39446140.0         short   \n",
       "3  @JennyHoyosLOL  jOc1XfFNJTo  30411496.0         short   \n",
       "4  @JennyHoyosLOL  Gs0QiMVkUAw  29434395.0         short   \n",
       "\n",
       "   channel_avg_view_count  \n",
       "0            7.569842e+06  \n",
       "1            7.569842e+06  \n",
       "2            7.569842e+06  \n",
       "3            7.569842e+06  \n",
       "4            7.569842e+06  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data index\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../scraping/6_filtered_videos_final/filtered_index_sorted_avg_channel_views.csv\", sep=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uploader_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>yt_video_type</th>\n",
       "      <th>channel_avg_view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43292</th>\n",
       "      <td>@MarketMobster</td>\n",
       "      <td>BK1CsZf4oOo</td>\n",
       "      <td>17929.0</td>\n",
       "      <td>video</td>\n",
       "      <td>20870.965517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3579</th>\n",
       "      <td>@AndreiJikh</td>\n",
       "      <td>8mmSW8G_oYo</td>\n",
       "      <td>573854.0</td>\n",
       "      <td>video</td>\n",
       "      <td>499474.370192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uploader_id     video_id  view_count yt_video_type  \\\n",
       "43292  @MarketMobster  BK1CsZf4oOo     17929.0         video   \n",
       "3579      @AndreiJikh  8mmSW8G_oYo    573854.0         video   \n",
       "\n",
       "       channel_avg_view_count  \n",
       "43292            20870.965517  \n",
       "3579            499474.370192  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample some videos (we want long examples!)\n",
    "sample = data[data.yt_video_type == \"video\"].sample(2)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transcript for video BK1CsZf4oOo\n",
      "Loading transcript for video 8mmSW8G_oYo\n"
     ]
    }
   ],
   "source": [
    "# load transcripts for the sampled videos\n",
    "\n",
    "transcript_path = \"../scraping/5_transcripts_and_metadata/transcripts_csvs\"\n",
    "transcripts = {}\n",
    "for i, row in sample.iterrows():\n",
    "    video_id = row[\"video_id\"]\n",
    "    uploader_id = row[\"uploader_id\"]\n",
    "    print(f\"Loading transcript for video {video_id}\")\n",
    "    transcripts[video_id] = pd.read_csv(f\"{transcript_path}/{uploader_id}_{video_id}.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# convert transcript dataframes to single strings\n",
    "transcript_texts = {}\n",
    "for video_id, transcript in transcripts.items():\n",
    "    transcript_texts[video_id] = \" \".join(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janja\\Desktop\\DS_Thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# load mistral tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------BK1CsZf4oOo--------------------\n",
      "2889 tokens\n",
      "12210 characters\n",
      "2378 words\n",
      "--------------------8mmSW8G_oYo--------------------\n",
      "3501 tokens\n",
      "15305 characters\n",
      "2813 words\n"
     ]
    }
   ],
   "source": [
    "# tokenize transcript texts and get lengths\n",
    "for video_id, transcript_text in transcript_texts.items():\n",
    "    tokenized = tokenizer(transcript_text)\n",
    "    print(f\"{'-'*20}{video_id}{'-'*20}\")\n",
    "    print(f\"{len(tokenized['input_ids'])} tokens\")\n",
    "    # also get number of characters and words\n",
    "    print(f\"{len(transcript_text)} characters\")\n",
    "    print(f\"{len(transcript_text.split())} words\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# helper for display\n",
    "def print_readable(text_to_print):\n",
    "    words = text_to_print.split()\n",
    "    # print 20 words per line\n",
    "    for i in range(0, len(words), 20):\n",
    "        print(\" \".join(words[i:i+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------BK1CsZf4oOo--------------------\n",
      "hey everyone we're going to talk about silica it's been a long time so hi everyone welcome to the channel\n",
      "if you are new please do subscribe the bell button leave a like and comment it is saturday and i'm\n",
      "doing a video so make sure you have smashed that like button so i want to say thank you for\n",
      "the support over the lives throughout the last few weeks and obviously growth in the channel if you are not\n",
      "subscribed make sure you do because there's quite a lot of people that are not which is obviously nice that\n",
      "you come back but i'd like it to remain mine forever which always helps so silica it's had a big\n",
      "moon we'll go over in terms of price in terms of usd on the beautiful world of coin market cap\n",
      "you can see big massive move a lot of people are getting a little bit anxious with it over the\n",
      "last few weeks i've not been talking about it because it's been consolidating it's been pretty boring but that's good\n",
      "good in a bull market a consolidation accumulation of wealth and then roof rocket which we're seeing now 13 cents\n",
      "or 13 and a half cents potentially could go much higher which is obviously what this title is telling us\n",
      "i still think it's undervalued it's going to go even higher so that's the usd side of it the btc\n",
      "side of it it's majorly undervalued because the dominance chart is still high 286 roughly give or take in terms\n",
      "of satoshis it's had a good little growth over the last 24 hours 21 and 34 in the la 31\n",
      "sorry in the last couple of days essentially so what's kind of sparked it one technicals and two most likely\n",
      "these kind of posts investments are starting to come in now from american companies so this is one we are\n",
      "expecting another 20 to 25 million worth of purchase in the next week so that's important silica doesn't have many\n",
      "eyeballs in america it doesn't have any exchanges really see listen zilliqa there's no coinbase there's no gemini there's no\n",
      "cracking you know gives you a big picture it's mainly a big force in the singapore area in asia when\n",
      "you think this could rocket to even bigger greater greater heights essentially when america get on board with this it\n",
      "could be even interesting more interesting sorry when we see this as well this is the other post i want\n",
      "to allude to another load of money coming through so this is i think this is a dubai based company\n",
      "let me just make sure i get my facts right um e g w capital the blockchain investment bank has\n",
      "bought 10 million worth of silica on the behalf of its dubai based high network investor so yeah these are\n",
      "very much important milestones for any coin but a coin like silica that is just its growth cycle just getting\n",
      "its full feet under the desk in terms of all the elements that it's got in terms of its throughput\n",
      "its sharding ability staking zill swap ethereum bridge is also coming there's a lot of things happening and there's a\n",
      "lot of interest in silica and obviously that industry alone scalability high throughput low fees scalability high throughput low fees\n",
      "what does aetherium not give us well other than headaches nothing nothing like that so we'll do some price analysis\n",
      "so we'll look at btc side later but we'll look at usd obviously this matters more for the world of\n",
      "what bitcoin does and doesn't give us essentially make sure you do follow me on socials i'm very very vocal\n",
      "with the lookup you know if most people know me i recommended this as a buying option in may so\n",
      "when it had its first impulse it pulled back and then we shot you can probably see by this chart\n",
      "here this is where i was buying around one to two cents which is pretty savage considering but in my\n",
      "opinion honest opinion you look at the btc pair and you probably think it hasn't run enough yet you probably\n",
      "will be thinking that so if we're going to do some very very simple chat analysis if we're looking at\n",
      "all-time highs right we're looking at all-time highs to the lows right we've got a lot of areas to look\n",
      "at this has been playing some nice symmetry so we have obviously got levels such as the 618 level on\n",
      "the weekly to keep an eye on right we've also obviously got levels like you know you could probably argue\n",
      "down to these levels here where it's been pretty good you can see how it's reacted certain levels how it's\n",
      "pulled back but this is only levels of resistance that we're worrying about we can obviously go much higher if\n",
      "we're looking at price targets the extensions from this to the other side to a big massive swoop essentially a\n",
      "rounded bottom it could go up to 37 cents which would be pretty nice it could also go much higher\n",
      "if you look at other extensions as well 55 cents potentially 60 cents even higher but let's not get carried\n",
      "away with ourselves we need to break above 14 cents that is the main level here which is obviously if\n",
      "i go into the daily time frame we look all the way left we've got a lot of interest in\n",
      "and around this level so this level's going to be important so this 14 tenths level probably be the one\n",
      "to break first right moving on though we've got to look at where other levels are so if we just\n",
      "do that and we'll keep those on that happy days we've got to then look at the bottom to the\n",
      "top potentially now this hasn't closed yet so this is where the important thing lies but if we go from\n",
      "this level here this is where it gets interesting when you look at this as well we've found a beautiful\n",
      "level of retracement at the 50 and we've obviously shot 16 is potential target here for one of the extensions\n",
      "so we have had a degree level of support found consolidation over a period of these three four weeks which\n",
      "is pretty boring for most people but we have found a new high and it's looking very very strong expect\n",
      "it to pull back at some point but if the momentum is there more people buying it we know what\n",
      "happens when these clients run they just continue going we've seen it so many times it's been ridiculous so in\n",
      "terms of usd prices we've kind of mapped out the basics so we've obviously got key levels to kind of\n",
      "keep an eye on i don't want to go too far above obviously all-time high levels but you got to\n",
      "think a break of this level we've got a big run on a potential big run once it breaks above\n",
      "certain levels and certain points it's going to get very very interesting so you've basically got those levels there i\n",
      "don't want to make it too congested it'll look a bit scruffy but we have got a lot of room\n",
      "to grow and i'm going to back it up now by saying it the reason why i'm saying it on\n",
      "this chart chart is because of this look at that it's disgusting when you think all-time high levels of btc\n",
      "per chalk and cheese it really is and what does that mean that means it is totally different look at\n",
      "how low this chart is it is still on the floor we're still under 300 satoshi so if we do\n",
      "a little look at this if we're looking at say this low to this high here we hit a perfect\n",
      "level 786 levels and we could easily be going up to 500 sats very very easily or even 440. but\n",
      "what i want to look at before this i want to actually look at this all-time high level where it\n",
      "goes all the way up and we could be retracing part of this move it could be unbelievable and as\n",
      "i say we've hardly even moved i don't even know if i can see the bottom on this chart yet\n",
      "let me just have a little look there we go we are at the bottom at some point so when\n",
      "we start looking at this we've got levels to look at and the levels are obvious the first level is\n",
      "obviously this level here right simple when we start thinking what's the bottom well we'll put a bottom line in\n",
      "there as well why not you know at the end of the day makes it a bit easier that's what\n",
      "we've got we've not even touched the surface yet in terms of potential full retracement what would you do let's\n",
      "just say theoretical we did a 400 move up to the 618 levels that is possible if this dominance of\n",
      "bitcoin keeps dropping and these keep going up and up and up you're going to start seeing things get very\n",
      "very interesting very very quickly so i don't want to get too excited and go well actually it could go\n",
      "up to there yes it could obviously but we've got to be we've got to be careful and we've got\n",
      "a lot of good confluence here with certain levels so i'm going to just put those price structures on there\n",
      "but in terms of levels of current price structure we've obviously got this level here this level looks the most\n",
      "important we've got multiple levels of support hits and obviously it has been resistance as well in the past i'll\n",
      "just move that down a tad there we go so that is where we are currently i'll remove some of\n",
      "the noise we'll zoom in we've got a retest and break above this level obviously condo closures are always good\n",
      "and if we do break above this level basically 300 satoshi's\n",
      "\n",
      "\n",
      "\n",
      "and break above this level obviously condo closures are always good and if we do break above this level basically\n",
      "300 satoshi's give or take if we close over 300 satoshi's or the high 200s basically 295 and above things\n",
      "are going to get very very interesting because of this price data in terms of that but what is it\n",
      "showing us we've got an impulse obviously looks pretty good it's pulled back we've formed a nice lower a higher\n",
      "low high we've not formed another low it's looking quite good chances are this candle could go way beyond and\n",
      "then break above this level and then send us that's where it gets interesting but if we're looking at short-term\n",
      "levels we've obviously got to think as well well what about this low this low here to this impulse from\n",
      "may june time what happened well it did a perfect move it pulled back it hit a level it retraced\n",
      "right we're looking for the same sort of thing from this point now what's happening we pulled back we've hit\n",
      "a level i think we're gonna break above and we're gonna go and see little levels up here these little\n",
      "levels up here essentially when you start putting the maps together in terms of where it could hit the confluence\n",
      "is all there with daily levels of structure you can see it like weekly structure sorry if i just put\n",
      "my cursor across there look at it it's perfect again look at the 618 level you know are the stars\n",
      "aligned for us to hit the fibonacci's all the way up there most likely so i'm going to leave that\n",
      "level up there i'm also going to duplicate this level and put it there because i think that is the\n",
      "next leg to go for it's not going to happen overnight obviously but when we start zooming out and looking\n",
      "at this chart that is where we've got you know obviously we've got levels all the way up here which\n",
      "i will remove because it will look a bit congested but when you start thinking where can we go we've\n",
      "got fibonacci levels in confluence with all of these weekly levels so we could be seeing some nice growth if\n",
      "it is the case even up to this top level is 26 in btc by the way this is a\n",
      "very very big move if you've got a couple of bitcoin in silica from the past you know you're growing\n",
      "a lot of bitcoin in these periods again it's 80 and even up to 100 so there could be a\n",
      "100 move up to 660 633 sorry satoshi's but when you zoom out and you look at that we've hardly\n",
      "even moved realistically we've hardly moved when we start zooming in look at this when you go into the daily\n",
      "time frame needle and looking at this previous levels of history and looking at it and zooming in on it\n",
      "you can see why we've got these levels of structure of how they're working so well and when you zoom\n",
      "in on it they're still big moves they are still very very big moves so i think that's it anyone\n",
      "telling me that silica's overpriced it's not going to go any further yeah we'll see there will be market retracements\n",
      "there will be pullbacks but it's looking good it's looking solid when you start thinking of how this has kind\n",
      "of moved since november for a high lower high we're on our way now i think we're on our way\n",
      "a break above this 300 level we could get very very interesting very very fast so if you enjoyed this\n",
      "saturday video let me know leave a like leave a comment and yeah i think the market is really starting\n",
      "to ramp up now obviously depends on what bitcoin does we need bitcoin to be nice and stable it is\n",
      "pulling back a little bit so far but opportunity will probably lie with us next week once it has been\n",
      "a bit of a correction in bitcoin we may well see further money and capital flow into the top performing\n",
      "coins silica being one of them [Music] [Music] bye\n",
      "\n",
      "\n",
      "\n",
      "--------------------8mmSW8G_oYo--------------------\n",
      "so apparently ethereum's about to self-destruct and i wanted to make this video to explain exactly what's gonna happen because\n",
      "when i read this headline uh my heart sank and i thought to myself i'm gonna lose one percent of\n",
      "my net worth no i'm just kidding i'm not meet kevin i have met kevin though and he's awesome so\n",
      "let me just be transparent with you for a moment because i started to make more money this year with\n",
      "youtube and so i understandably started to take more risks with my investing so a small-ish part of my net\n",
      "worth i invested into cryptocurrency and it turned into a relatively large-ish part of my net worth so i took\n",
      "a hundred thousand dollars and i spread it between bitcoin and ethereum about 80 20 and that 100 000 is\n",
      "now worth today something like 820 000 which let me just be real here because i know everyone on the\n",
      "internet is a multi-millionaire but this to me is a ton of money because that is way more money than\n",
      "i've ever made in my entire working career and it would be a shame if i lost 120 ethereums because\n",
      "if i sold them today they'd be worth something like 230 000 i would cry because that's enough to buy\n",
      "a house now i wanted to tell you all that up front not to show off or anything like that\n",
      "but to tell you that obviously i have a lot of money invested and i'm biased and i love cryptocurrency\n",
      "and obviously this self-destruct situation has me worried so i wanted to make this video to explain exactly what's going\n",
      "on so let's talk about it all right so this is how the ethereum network self-destruct feature works imagine a\n",
      "countdown with a deck of cards five four three two one let's begin hi my name is andre jack hope\n",
      "you're doing well come for the finance and stay for the this became a crypto channel comments i promise i'm\n",
      "not a crypto channel but i do want to talk more about crypto this year especially because it is still\n",
      "very relevant and still very dangerous and i want to make sure to protect people's best interests but also because\n",
      "the crypto space has the highest cpm rates i've ever seen which is awesome because i get to make more\n",
      "money but unfortunate because i have to spend it all on my dog who is the pickiest eater i've ever\n",
      "seen i put rito on a human food grade diet that is so ridiculously expensive so i hate him i\n",
      "don't recommend a chihuahua so anyway beside the point let's talk about self-destruct and then how it's going to be\n",
      "used and then what it's going to mean for ethereum's price in the future because there's some good implications and\n",
      "there's some bad ones so let's just start with self-destruct what is it so it's a feature that the ethereum\n",
      "developers put inside of smart contracts in order to be able to upgrade what are called dows these decentralized autonomous\n",
      "organizations which is just a fancy word for saying a decentralized business platform that can run by itself without the\n",
      "input from its creators and when you upgrade the smart contract from the old to the new you have to\n",
      "destroy the old smart contract now there's a few other ways that this self-destruct feature is used that i'll talk\n",
      "about later in the video but understand that ethereum is run on a programming language called solidity and once a\n",
      "developer writes this smart contract they lose the ability to change anything about it as soon as they launch it\n",
      "and make it public because once they do it becomes distributed and it becomes decentralized and it's one of the\n",
      "reasons why we love cryptocurrency that's the trade-off is because then nobody owns it and that is awesome it's one\n",
      "of the reasons why this cryptocat is so valuable because it is provably verifiably rare aka the developers can't go\n",
      "in and change this smart contract but another reason why it's valuable is because people just have nothing better to\n",
      "do with their money but that's beside the point i'm not judging because i did spend way too much money\n",
      "on a piece of paper called the charizard so i'm not judging but when it comes to decentralization that comes\n",
      "at the expense of network security because there's always a flaw in the system waiting to be exploited and once\n",
      "it happens i can't go in and change the code like if i'm a private database like a bank or\n",
      "a credit card where i can just go in and get rid of an attacker that cannot happen on a\n",
      "decentralized platform like ethereum and that's sort of where this self-destruct feature comes in the idea behind it is that\n",
      "if an attacker gets access to a smart contract or a doubt that the programmers would be able to go\n",
      "in and just self-destruct and then reduce any damage done to people's money that is locked inside of the smart\n",
      "contract it's sort of like the equivalent of the eject button on an airplane and it's also sometimes called burning\n",
      "tokens and you know what that means obligatory visual analogy that no one asks for burning tokens in my wallet\n",
      "so perfect no it's not overused now in this case ethereum is not using self-destruct to self-destruct and protect itself\n",
      "from attackers so i don't want to scare anyone instead it's using self-destruct to destroy its own coins and here's\n",
      "why it's doing that all right this is about to get really complicated but bear with me i'll try to\n",
      "keep it really simple so ethereum wants to change the way that it handles and processes its transactions because in\n",
      "ethereum 1.0 which is what i have right here we use a concept called proof of work to secure the\n",
      "network and transact with so anytime i want to send money to my wallet or i want to send it\n",
      "to you then i have to pay miners what are called gas fees in order to do what i want\n",
      "them to do and in return they secure our network and they make ethereum safe to use but in this\n",
      "next upgrade which is called eip 1559 we want to change what's called the consensus model which is just a\n",
      "fancy phrase for describing a framework for how something works that we all agree on so we're going to change\n",
      "it from a proof of work to a proof of stake consensus model so now instead of paying the miners\n",
      "the fees to do the stuff we want those fees are going to be paid to the network itself and\n",
      "a portion of those fees are then going to be self-destructed or burned out of existence forever that's the technical\n",
      "explanation of what's going to happen and now let's talk about the fun part as far as why it's gonna\n",
      "self-destruct and what's gonna happen to the price so for starters it's gonna start to compete with bitcoin on a\n",
      "level that i didn't think it would any time soon because here's the thing at the beginning of this year\n",
      "in january we had something like 114 million 78 000 ethereum coins in existence now today that number is closer\n",
      "to like 115 million 39 000 coins what does that mean that means we've digitally printed something like a million\n",
      "coins in just the last two and a half months that means the price of ethereum has been kept somewhat\n",
      "stable and in check and it hasn't been increasing in value quite as much as bitcoin because it has no\n",
      "theoretical limit to how many ethereums can exist unlike bitcoin which obviously has a maximum supply of 21 million and\n",
      "so this upgrade could mean that it could transform ethereum from an inflationary currency to one that looks a little\n",
      "bit more like a deflationary currency or at least one that has more control over its inflation because of self-destruct\n",
      "but what is self-destruct originally it was an attack from a pokemon called golem which was like this rock shaped\n",
      "circle thing and any time it used it it would be basically game over so when i read self-destruct i\n",
      "kind of panic because i've been scarred since pokemon blue version but that's beside the point the fact that ethereum\n",
      "is going to self-destruct should mean that our investments within ethereum should hypothetically increase in value by a lot so\n",
      "let me tie everything together and make sense of why this is gonna happen like i've said in the beginning\n",
      "of this video i have a huge reason personally not to say anything badly about bitcoin and not necessarily promote\n",
      "any other crypto because that would obviously affect my investment portfolio but i'm gonna be truthful and objective as possible\n",
      "because that's the premise i've set myself since i've started my youtube channel so here it goes i still love\n",
      "bitcoin the most objectively i mean i'm not biased or anything right that's because i can understand it the best\n",
      "all it wants to do is to be a store of value and it doesn't promise anything more than that\n",
      "it is digital gold but if bitcoin is digital gold and we always compare bitcoin to gold's market cap of\n",
      "10 trillion dollars then it stands to reason that we compare ethereum's market cap to something that is way bigger\n",
      "than gold something like m1 or even m2 the global monetary currency supply and circulation which is many many trillions\n",
      "of dollars more than gold's market cap but at the very least it can also be i guess compared to\n",
      "gold's market cap too this is why\n",
      "\n",
      "\n",
      "\n",
      "dollars more than gold's market cap but at the very least it can also be i guess compared to gold's\n",
      "market cap too this is why i've said so many times that ethereum has more potential in terms of multiplying\n",
      "its price from today's levels because when people buy bitcoin they buy it and then they huddle it right they\n",
      "actually hold on to it they don't really use it i mean yes some people use it for the most\n",
      "part they just use it as a hedge against the real inflation i say that in air quotes because just\n",
      "recently the central banks have told us that the personal expense consumption inflation rate is something like 1.5 and the\n",
      "core inflation is at roughly 1.3 which i'm not sure how to believe that considering that bitcoin has exploded over\n",
      "several hundred percent in price and other asset classes have also grown insanely high so okay let's go with that\n",
      "though ethereum's strength is the fact that people are using it it is the most useful blockchain in the world\n",
      "today people are using it to build adapts these decentralized apps these dowels decentralized autonomous organizations these digitized pieces of\n",
      "artwork called nfts decentralized finance defy i mean the list goes on and on and on people are using it\n",
      "so it's more than just a proof of concept it works today which means in the future ethereum's velocity of\n",
      "money the rate at which we exchange money is going to be a lot higher than bitcoins and so when\n",
      "the network upgrades here's what i think will happen the rate at which we create ethereum coins could be outweighed\n",
      "by the rate with which we self-destruct or burn them out of the system and so we could create a\n",
      "reality where there's more demand than we are creating and this isn't just a hypothetical example because just last month\n",
      "in february the transaction fees spiked and they were higher than the block reward for miners some days and so\n",
      "you can have this potential example where there is more demand to build businesses and use the ethereum network than\n",
      "we're actually creating tokens to be used in the first place and this obviously drives the price higher because it\n",
      "creates this basic supply and demand economic structure that exists in bitcoin that doesn't really exist in ethereum today even\n",
      "though it's worth roughly two thousand dollars still there was a really interesting point brought up by vitalik buterin who's\n",
      "the creator of ethereum who said this he said the reason people love bitcoin and the reason it goes up\n",
      "in value is because it goes up in value and people buy more bitcoin and the blockchain aspect of bitcoin\n",
      "is just a byproduct of its existence in other words it's just there to secure the network but that's not\n",
      "the point of bitcoin and so ethereum's approach is completely the opposite in that blockchain is the end result and\n",
      "ethereum is nothing more than the asset that gets used on top of the blockchain and it's only increasing in\n",
      "value because it is a byproduct of the blockchain doing its job successfully which is to build decentralized platforms and\n",
      "businesses on top of and i thought that was super interesting and i'm not saying that one approach is better\n",
      "than the other because let's face it pokemon cards still have a ton of value even though no one's using\n",
      "their base set charizard cards to duel with yu-gi-oh reference people are just collecting them and they're still appreciating a\n",
      "ton in value but ethereum's approach is completely the opposite which is why i hold both tokens because i think\n",
      "both can co-exist peacefully and successfully but when ethereum upgrades the network soon there are a few problems that it's\n",
      "going to encounter with this self-destruct feature that you definitely need to know about when it comes time to upgrade\n",
      "the network which will happen sometime in either july or august we could see what's called a chain split now\n",
      "a chain split happens when a coin forks into two different versions and this usually happens when a group of\n",
      "people feel marginalized and left out of some deal and that would be the miners because think about it right\n",
      "now in ethereum 1.0 the miners use their computers to secure the network and get paid fees but what if\n",
      "in the future somebody was to tell you thanks for all you've done you're fired we don't need you anymore\n",
      "you'd probably be upset and you might rebel so on april 1st allegedly a bunch of miners are going to\n",
      "try to get together to get a 51 majority now if that happens the miners then become neo they can\n",
      "do whatever they want to the network they can attack it they can create more coins they can double spend\n",
      "they can do anything and that's really bad for the network but the likelihood of this happening is probably going\n",
      "to be zero the reason why ethereum's gonna be safe is because miners have very little incentive to attack the\n",
      "network that's making them rich it's all part of the game theory that makes kryptos so brilliant because why bite\n",
      "the hand that feeds you why destroy the golden goose that lays the golden eggs you just wouldn't the real\n",
      "reason they're getting together is just to prove a point that hey if economic incentives don't align then we could\n",
      "potentially fracture the network and that's not something anybody wants but honestly that's part of the fun of being a\n",
      "crypto investor you just never know what apocalyptic event is right around the corner to destroy all of your coins\n",
      "so i do think that if ethereum manages to upgrade the network successfully we're gonna see the beginning of the\n",
      "biggest bull run of 2021. that's when the price is really going to increase i'm betting money on it now\n",
      "literally i'm betting a lot of money on it over 220 000 so i'm really hoping it goes well in\n",
      "the meantime don't forget to grab up to 250 dollars worth of free bitcoin on block fi using this block\n",
      "file link right here block fi.com forward slash andre and when you do go get two free stocks with weibull\n",
      "by depositing 100 you'll get two free stocks each of which can be worth up to 1 850 using the\n",
      "link down below and when you get those two free stocks don't forget to track them automatically with the spreadsheet\n",
      "link down below my patreon love you thank you so much for watching this video i will see you back\n",
      "here on monday and friday sometimes a wednesday you can follow me on instagram if you haven't already and don't\n",
      "forget sometimes i sound like a salesman so uh go subscribe to millennial money it's my friends jeremy kevin graham\n",
      "and i who post on tuesday at 6 p.m pacific we talk about everything money related and investing anyway love\n",
      "you thank you so much for watching i'll see you soon bye\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chunk and display samples\n",
    "from data_prep_utils import get_text_chunks\n",
    "\n",
    "chunks = {}\n",
    "for video_id, transcript_text in transcript_texts.items():\n",
    "    print(f\"{'-'*20}{video_id}{'-'*20}\")\n",
    "    chunks[video_id] = get_text_chunks(transcript_text, tokenizer)\n",
    "    for chunk in chunks[video_id]:\n",
    "        print_readable(chunk)\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform chunking for entire dataset\n",
    "\n",
    "We proceed as follows:\n",
    "- load transcript csvs and build string from transcript data\n",
    "- split transcript strings into chunks\n",
    "- store chunks in a dataframe with 3 columns: ``video_id``, ``chunk_number``, ``chunk_text``\n",
    "  - chunks will be numbered 1, 2, 3, ... for each video\n",
    "- save dataframe to single csv file\n",
    "  - Since the csv data we start from is only a little over 1 GB (with timestamp data), our single file should be < 1GB, which is workable (and more convenient than multiple files). \n",
    "  - filename will include parameters used for chunking and data info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index with 45968 video ids.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "########################################################################################################\n",
    "# data source folders <- adjust\n",
    "index_path = \"../scraping/6_filtered_videos_final\"\n",
    "transcripts_path = \"../scraping/5_transcripts_and_metadata/transcripts_csvs\"\n",
    "# chunking parameters <- adjust\n",
    "max_chunk_tokens = 2048\n",
    "overlap = 50\n",
    "# tokenizer <- adjust\n",
    "tokenizer_hf_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer_filename_str = \"Mistral\"\n",
    "# load index\n",
    "index = pd.read_csv(f\"{index_path}/filtered_index_sorted_avg_channel_views.csv\", sep=\";\")\n",
    "print(f\"Loaded index with {len(index)} video ids.\")\n",
    "# result file saving info <- adjust\n",
    "result_filename = f\"transcript_chunks_nvids{len(index)}_chunksize{max_chunk_tokens}_overlap{overlap}_tok{tokenizer_filename_str}.csv\"\n",
    "result_path = f\"../data/transcript_chunks\"\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed transcript 999 of 45968\n",
      "texts memory: 8840\n",
      "Processed transcript 1999 of 45968\n",
      "texts memory: 18216\n",
      "Processed transcript 2999 of 45968\n",
      "texts memory: 37192\n",
      "Processed transcript 3999 of 45968\n",
      "texts memory: 47144\n",
      "Empty transcript for video 84wsLwCvNxo. Skipping.\n",
      "Processed transcript 4999 of 45968\n",
      "texts memory: 59720\n",
      "Processed transcript 5999 of 45968\n",
      "texts memory: 75656\n",
      "Processed transcript 6999 of 45968\n",
      "texts memory: 95848\n",
      "Processed transcript 7999 of 45968\n",
      "texts memory: 107880\n",
      "Processed transcript 8999 of 45968\n",
      "texts memory: 121416\n",
      "Processed transcript 9999 of 45968\n",
      "texts memory: 136616\n",
      "Processed transcript 10999 of 45968\n",
      "texts memory: 153736\n",
      "Processed transcript 11999 of 45968\n",
      "texts memory: 153736\n",
      "Processed transcript 12999 of 45968\n",
      "texts memory: 173000\n",
      "Processed transcript 13999 of 45968\n",
      "texts memory: 194664\n",
      "Processed transcript 14999 of 45968\n",
      "texts memory: 194664\n",
      "Processed transcript 15999 of 45968\n",
      "texts memory: 194664\n",
      "Processed transcript 16999 of 45968\n",
      "texts memory: 219048\n",
      "Processed transcript 17999 of 45968\n",
      "texts memory: 219048\n",
      "Processed transcript 18999 of 45968\n",
      "texts memory: 246472\n",
      "Processed transcript 19999 of 45968\n",
      "texts memory: 246472\n",
      "Processed transcript 20999 of 45968\n",
      "texts memory: 277320\n",
      "Processed transcript 21999 of 45968\n",
      "texts memory: 277320\n",
      "Processed transcript 22999 of 45968\n",
      "texts memory: 312008\n",
      "Processed transcript 23999 of 45968\n",
      "texts memory: 312008\n",
      "Processed transcript 24999 of 45968\n",
      "texts memory: 351048\n",
      "Processed transcript 25999 of 45968\n",
      "texts memory: 351048\n",
      "Processed transcript 26999 of 45968\n",
      "texts memory: 351048\n",
      "Processed transcript 27999 of 45968\n",
      "texts memory: 394952\n",
      "Processed transcript 28999 of 45968\n",
      "texts memory: 394952\n",
      "Processed transcript 29999 of 45968\n",
      "texts memory: 394952\n",
      "Processed transcript 30999 of 45968\n",
      "texts memory: 444360\n",
      "Processed transcript 31999 of 45968\n",
      "texts memory: 444360\n",
      "Processed transcript 32999 of 45968\n",
      "texts memory: 444360\n",
      "Processed transcript 33999 of 45968\n",
      "texts memory: 499944\n",
      "Processed transcript 34999 of 45968\n",
      "texts memory: 499944\n",
      "Processed transcript 35999 of 45968\n",
      "texts memory: 499944\n",
      "Processed transcript 36999 of 45968\n",
      "texts memory: 562472\n",
      "Processed transcript 37999 of 45968\n",
      "texts memory: 562472\n",
      "Processed transcript 38999 of 45968\n",
      "texts memory: 562472\n",
      "Processed transcript 39999 of 45968\n",
      "texts memory: 562472\n",
      "Processed transcript 40999 of 45968\n",
      "texts memory: 632808\n",
      "Processed transcript 41999 of 45968\n",
      "texts memory: 632808\n",
      "Processed transcript 42999 of 45968\n",
      "texts memory: 632808\n",
      "Processed transcript 43999 of 45968\n",
      "texts memory: 632808\n",
      "Processed transcript 44999 of 45968\n",
      "texts memory: 632808\n",
      "----------------------------------------\n",
      "Finished processing 45968 transcripts.\n"
     ]
    }
   ],
   "source": [
    "from data_prep_utils import clean_text, get_text_chunks\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_hf_model)\n",
    "\n",
    "# initialize result lists\n",
    "video_ids = []\n",
    "chunk_numbers = []\n",
    "chunk_texts = []\n",
    "\n",
    "# iterate over videos\n",
    "for i, (video_id, uploader_id) in enumerate(zip(index.video_id, index.uploader_id)):\n",
    "\n",
    "    # load transcript csv\n",
    "    transcript_csv = pd.read_csv(f\"{transcripts_path}/{uploader_id}_{video_id}.csv\", sep=\";\")\n",
    "    # convert transcript to single string \n",
    "    # (apparently there are NaN lines we need to filter out first)\n",
    "    transcript_lines = [line for line in transcript_csv.text if isinstance(line, str)]\n",
    "    transcript_text = \" \".join(transcript_lines)\n",
    "    # clean\n",
    "    transcript_text = clean_text(transcript_text)\n",
    "    \n",
    "    # get chunks and store them\n",
    "    if len(transcript_text) == 0:\n",
    "        print(f\"Empty transcript for video {video_id}. Skipping.\")\n",
    "    else:\n",
    "        chunks = get_text_chunks(transcript_text, tokenizer)\n",
    "        # store data in result lists\n",
    "        for chunk_number, chunk in enumerate(chunks, start=1):\n",
    "            video_ids.append(video_id)\n",
    "            chunk_numbers.append(chunk_number)\n",
    "            chunk_texts.append(chunk)\n",
    "\n",
    "    # print progress\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"Processed transcript {i} of {len(index)}\")\n",
    "        print(f\"texts memory: {chunk_texts.__sizeof__()}\")\n",
    "\n",
    "print(f\"{'-'*40}\\nFinished processing {len(index)} transcripts.\")\n",
    "\n",
    "# save in dataframe/csv\n",
    "chunks_df = pd.DataFrame(columns=[\"video_id\", \"chunk_number\", \"chunk_text\"])\n",
    "chunks_df[\"video_id\"] = video_ids\n",
    "chunks_df[\"chunk_number\"] = chunk_numbers\n",
    "chunks_df[\"chunk_text\"] = chunk_texts\n",
    "\n",
    "chunks_df.to_csv(f\"{result_path}/{result_filename}\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80176 transcript chunks for 45967 videos.\n",
      "df memory: 0.46 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# delete variables to free memory\n",
    "for var in [\"video_ids\", \"chunk_numbers\", \"chunk_texts\", \"chunks_df\"]:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "\n",
    "# load saved file\n",
    "filepath = \"../data/transcript_chunks/transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral.csv\"\n",
    "#filepath = f\"{result_path}/{result_filename}\"\n",
    "chunks_df = pd.read_csv(filepath, sep=\";\")\n",
    "print(f\"Loaded {len(chunks_df)} transcript chunks for {len(chunks_df.video_id.unique())} videos.\")\n",
    "print(f\"df memory: {chunks_df.__sizeof__() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i2bUeO1ID30</td>\n",
       "      <td>1</td>\n",
       "      <td>my grandma thinks Christmas is expensive so I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VvEBCXHx-74</td>\n",
       "      <td>1</td>\n",
       "      <td>you can find golden dirt this is a 25 bag of d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CEdnanNgS3k</td>\n",
       "      <td>1</td>\n",
       "      <td>one dollar chicken sandwich now Chick-fil-A ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jOc1XfFNJTo</td>\n",
       "      <td>1</td>\n",
       "      <td>Logan Paul made from Prime apparently over 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gs0QiMVkUAw</td>\n",
       "      <td>1</td>\n",
       "      <td>two dollar pumpkin spice lattes apparently you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  chunk_number  \\\n",
       "0  i2bUeO1ID30             1   \n",
       "1  VvEBCXHx-74             1   \n",
       "2  CEdnanNgS3k             1   \n",
       "3  jOc1XfFNJTo             1   \n",
       "4  Gs0QiMVkUAw             1   \n",
       "\n",
       "                                          chunk_text  \n",
       "0  my grandma thinks Christmas is expensive so I'...  \n",
       "1  you can find golden dirt this is a 25 bag of d...  \n",
       "2  one dollar chicken sandwich now Chick-fil-A ha...  \n",
       "3  Logan Paul made from Prime apparently over 100...  \n",
       "4  two dollar pumpkin spice lattes apparently you...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    45967.000000\n",
       "mean         1.744208\n",
       "std          0.900603\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          2.000000\n",
       "max          7.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_per_video = chunks_df.groupby(\"video_id\").size()\n",
    "chunks_per_video.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check transcripts for ids: {'84wsLwCvNxo'}\n"
     ]
    }
   ],
   "source": [
    "# check the few empty transcripts\n",
    "\n",
    "# video_ids in index but not in chunks_df\n",
    "index = pd.read_csv(f\"../scraping/6_filtered_videos_final/filtered_index_sorted_avg_channel_views.csv\", sep=\";\")\n",
    "missing_ids = set(index.video_id) - set(chunks_df.video_id)\n",
    "print(f\"Check transcripts for ids: {missing_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 duplicate video_ids.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: video_id, dtype: object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.read_csv(f\"../scraping/6_filtered_videos_final/filtered_index_sorted_avg_channel_views.csv\", sep=\";\")\n",
    "# check for duplicate video_ids\n",
    "duplicates = index.video_id[index.video_id.duplicated()]\n",
    "print(f\"Found {len(duplicates)} duplicate video_ids.\")\n",
    "duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
