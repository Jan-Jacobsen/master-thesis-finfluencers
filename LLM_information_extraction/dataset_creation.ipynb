{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df with 80176 rows (=chunks) and 45967 unique video_ids.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>uploader_id</th>\n",
       "      <th>yt_video_type</th>\n",
       "      <th>title</th>\n",
       "      <th>first_three_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i2bUeO1ID30</td>\n",
       "      <td>1</td>\n",
       "      <td>my grandma thinks Christmas is expensive so I'...</td>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>short</td>\n",
       "      <td>$5 Christmas Gift</td>\n",
       "      <td>christmas, Christmas present, christmas presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VvEBCXHx-74</td>\n",
       "      <td>1</td>\n",
       "      <td>you can find golden dirt this is a 25 bag of d...</td>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>short</td>\n",
       "      <td>I Bought $25 Dirt to Find Gold</td>\n",
       "      <td>pay dirt, gold prospecting, gold mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CEdnanNgS3k</td>\n",
       "      <td>1</td>\n",
       "      <td>one dollar chicken sandwich now Chick-fil-A ha...</td>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>short</td>\n",
       "      <td>$1 Chicken Sandwich vs Chick-Fil-A</td>\n",
       "      <td>chick fil a, chicken sandwich, food hacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jOc1XfFNJTo</td>\n",
       "      <td>1</td>\n",
       "      <td>Logan Paul made from Prime apparently over 100...</td>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>short</td>\n",
       "      <td>How Much Logan Paul Made From Prime</td>\n",
       "      <td>logan paul, ksi prime, drink prime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gs0QiMVkUAw</td>\n",
       "      <td>1</td>\n",
       "      <td>two dollar pumpkin spice lattes apparently you...</td>\n",
       "      <td>@JennyHoyosLOL</td>\n",
       "      <td>short</td>\n",
       "      <td>$2 Pumpkin Spice Latte at Starbucks</td>\n",
       "      <td>pumpkin spice latte, pumpkin spice, starbucks ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  chunk_number  \\\n",
       "0  i2bUeO1ID30             1   \n",
       "1  VvEBCXHx-74             1   \n",
       "2  CEdnanNgS3k             1   \n",
       "3  jOc1XfFNJTo             1   \n",
       "4  Gs0QiMVkUAw             1   \n",
       "\n",
       "                                          chunk_text     uploader_id  \\\n",
       "0  my grandma thinks Christmas is expensive so I'...  @JennyHoyosLOL   \n",
       "1  you can find golden dirt this is a 25 bag of d...  @JennyHoyosLOL   \n",
       "2  one dollar chicken sandwich now Chick-fil-A ha...  @JennyHoyosLOL   \n",
       "3  Logan Paul made from Prime apparently over 100...  @JennyHoyosLOL   \n",
       "4  two dollar pumpkin spice lattes apparently you...  @JennyHoyosLOL   \n",
       "\n",
       "  yt_video_type                                title  \\\n",
       "0         short                    $5 Christmas Gift   \n",
       "1         short       I Bought $25 Dirt to Find Gold   \n",
       "2         short   $1 Chicken Sandwich vs Chick-Fil-A   \n",
       "3         short  How Much Logan Paul Made From Prime   \n",
       "4         short  $2 Pumpkin Spice Latte at Starbucks   \n",
       "\n",
       "                                    first_three_tags  \n",
       "0  christmas, Christmas present, christmas presen...  \n",
       "1            pay dirt, gold prospecting, gold mining  \n",
       "2          chick fil a, chicken sandwich, food hacks  \n",
       "3                 logan paul, ksi prime, drink prime  \n",
       "4  pumpkin spice latte, pumpkin spice, starbucks ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from local csv file\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral\"\n",
    "path = f\"../data/transcript_chunks/{filename}.csv\"\n",
    "\n",
    "data = pd.read_csv(path, sep=\";\")\n",
    "print(f\"Loaded df with {len(data)} rows (=chunks) and {data['video_id'].nunique()} unique video_ids.\")\n",
    "#print(data.head(5))\n",
    "\n",
    "# add metadata info which we might want to include in the prompt\n",
    "# handle loading of lists from csv with ast.literal_eval()\n",
    "import ast\n",
    "def load_list(x):\n",
    "    return ast.literal_eval(x) if x else None\n",
    "metadata = pd.read_csv(\"../scraping/6_filtered_videos_final/filtered_metadata.csv\", sep=\";\", header=0, converters={\"tags\": load_list, \n",
    "                                                                                                        \"yt_auto_categories\": load_list})\n",
    "#metadata.head()\n",
    "# for now we're only interested in the uploader_id and title as well as the first three tags (although these don't exist for all videos!)\n",
    "nrows_before_merge = len(data)\n",
    "data = data.merge(metadata[[\"video_id\", \"uploader_id\", \"yt_video_type\", \"title\", \"first_three_tags\"]], on=\"video_id\", how=\"left\")\n",
    "\n",
    "if nrows_before_merge != len(data):\n",
    "    print(\"Warning: Check merging, number of rows changed. Possibly duplicates in metadata dfs?\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# save to csv, ready for huggingface upload\n",
    "#data.to_csv(f\"../data/transcript_chunks/{filename}_with_metadata_for_prompt.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janja\\Desktop\\DS_Thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['video_id', 'chunk_number', 'chunk_text', 'uploader_id', 'yt_video_type', 'title', 'first_three_tags'],\n",
      "    num_rows: 80176\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load from csv\n",
    "import pandas as pd\n",
    "#filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral\"\n",
    "data = pd.read_csv(f\"../data/transcript_chunks/{filename}_with_metadata_for_prompt.csv\", sep=\";\")\n",
    "\n",
    "# convert to huggingface dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "print(dataset)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janja\\Desktop\\DS_Thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['video_id', 'chunk_number', 'chunk_text', 'uploader_id', 'view_count', 'yt_video_type', 'title', 'first_three_tags'],\n",
      "        num_rows: 82991\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset from hf\n",
    "\n",
    "from datasets import load_dataset\n",
    "#filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral\"\n",
    "dataset = load_dataset(\"JanJacobsen/youtube_finfluencer_transcripts\", \n",
    "                       data_files=f\"{filename}_with_metadata_for_prompt.csv\",\n",
    "                       sep=\";\")\n",
    "dataset = dataset[\"train\"] # if there are no splits hf loads as \"train\" by default\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "- if labelling and/or finetuning turn out to be too slow, we can add back some examples to the inference set later (while being careful not to introduce bias of course)\n",
    "- note: data is shuffled in the process, so if we want to have the descending avg views by channel ordering for inference we need to merge again with the ordered index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference set: 79626 samples\n",
      "Finetuning set: 400 samples\n",
      "Validation set: 150 samples\n"
     ]
    }
   ],
   "source": [
    "# split dataset into finetuning, test and inference set\n",
    "\n",
    "seed = 42\n",
    "ft_size = 400\n",
    "valid_size = 150\n",
    "dataset = dataset.train_test_split(test_size=ft_size+valid_size, seed=seed)\n",
    "\n",
    "ds_inference = dataset[\"train\"]\n",
    "ds_finetuning = dataset[\"test\"].train_test_split(test_size=valid_size, seed=seed)[\"train\"]\n",
    "ds_validation = dataset[\"test\"].train_test_split(test_size=valid_size, seed=seed)[\"test\"]\n",
    "\n",
    "print(f\"Inference set: {len(ds_inference)} samples\")\n",
    "print(f\"Finetuning set: {len(ds_finetuning)} samples\")\n",
    "print(f\"Validation set: {len(ds_validation)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formatting function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# implemented in LLM_utils.py\n",
    "from LLM_utils import format_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] The triple-quoted text below is part of a youtube video transcript by channel @josephhogue with the title '5 Growth Stocks to Buy Now for the 2022 Rebound'. The top tags for the video are: 'growth stocks, growth stocks 2022, best growth stocks'. Read the transcript carefully in order to perform the asset name extraction task specified below the transcript.\n",
      "\n",
      "\"\"\"growth stocks are getting slammed again in 2022 with shares of disruptive companies like shopify down 35 percent and zoom plunging 60 over the last year these are companies changing the world and anyone holds the potential to 10x your money but the pain is expected to continue throughout this year so you need to know how to find these best stocks in this video i'll show you exactly how to find the growth stocks to buy in 2022 with a simple stock screener to start your list and how to narrow it down i'll then reveal the five grow stocks to buy now one with a 75 return just to the average analyst target before we get started you know i've got to send that special shout out to all you out there in the bowtie nation thank you for spending a part of your day to be here if you're not part of that community yet just click that little red subscribe button it's free and you'll never miss an episode nation as painful as the sell-off in growth stocks has been you cannot ignore this side of the market amazon investors lost 94 of their money following the bursting of the internet bubble to six dollars and seventy cents a share but those shares are now worth more than five hundred times that amount an annual return of thirty six percent for twenty years some of these growth stocks are no less transformative and will allow you to do the same thing in the next 20 years but with those interest rates expected to go higher you need a strategy for finding the best growth stocks to buy because there will be a huge difference in returns in fact david costan chief equity strategist at goldman sachs calls the difference the single greatest mispricing in u.s stocks now whenever someone says the word mispricing what i hear them say is actually opportunity and i think this is the number one piece of advice for separating out those gross stocks to buy in 2022. now costing defines that mispricing as the difference between companies with high expected revenue so growth stocks but those with low or negative margins versus those with positive margins and now there is some investing jargon in there that we need to unpack but it's going to show you exactly what to look for in finding these growth stocks before we get into that list of stocks to buy growth stocks are just companies growing their sales or their total revenue by double digits each year usually at least 20 or 30 sales growth year over year margins here margins are a measure of profitability here we'll be talking about the net margin which is the net income or or a company's earnings divided by its sales how much of the company's revenue is management able to turn into profits so then the difference here that historic mispricing and the key to finding gross stocks to buy is going to be looking for companies that are not only growing their sales at that growth rate pace but also already profitable the growth stocks already making investors money versus the ones that are still trading on a hope and a dream i want to get into that list of best growth stocks to buy though so let's get started and later i'm going to show you how to find those quality profitable companies and how to narrow your list to only the best we're going to be using fundamental analysis to find our growth stocks but another way i like to look for stocks is through technical analysis looking for the trends and the buy signals in stocks it's exactly the kind of strategy i put together with thomas carvo in a workshop on stock trading and technical analysis 35 videos and more than four hours of instruction on stock signals you need to know you'll learn everything from trading time frames to using stock charts and finding those trends that send a stock higher you'll be able to download a trade journal template along with trade cheat sheets and lots of other bonus material we just launched the workshop a couple of months ago and are offering a special 35 discount more than 150 off to everyone out there in the community so check out the link i'll leave in the description below i'm starting here with a stock that is already revolutionizing e-commerce taking market share away from amazon shopify inc ticker s-h-o-p and here i gotta admit i was skeptical that shopify could even compete against the amazon giant but it's created a brand for itself and a niche in helping people launch their business through the site and the results have been amazing 56 revenue growth over the last three years and 44 in the trailing 12 months now that's still just one percent the size of amazon in revenue so years of growth left and the margin numbers here are just unbelievable not only has the company been profitable since last year but it increased its net margin to 81 which is pretty much unheard of at that level of profitability the sales are expected to rise another 32 percent this year to 6.1 billion dollars and while earnings growth is expected to slow it's still forecast to grow by 18 annually over the next two years which is going to continue that growth stock theme among the 22 analysts here we have an average price target of just over sixteen hundred dollars a share which would be a forty two percent higher from here but like all of these growth stocks i think that long term return over the years could be on the order of five and ten times that and next year you don't normally think of advanced micro devices ticker amd as a gross stock but it has definitely put up the numbers lately the cpu semiconductor maker saw revenue jump on laptop sales during the pandemic and while those are looking to cool over the next year its data center business is still producing strong growth in and this new line of ryzen chips continues to take market share from intel sales growth accelerated into the last year at a 52 rate from what was already a pretty fast pace beforehand and the net margin increased to 26.7 percent in the trailing 12 months and with the backlog and semiconductors amd could still book solid sales growth for years to come in fact analysts expect the company to post 19 sales growth this year and i'd actually expect that to be closer to 20 or 21 growth and earnings are expected to rise by 26.5 percent to 3.34 a share now that faster growth rate in earnings means analysts expect amd to continue to improve on its net margin profitability throughout the year now the average analyst target only has amd up four percent from here but this is one of the cheapest stocks in our growth stock list shares trade for just 11 times on that price to sales basis half the average multiple of the five stocks were all highlighted and i think this one could go much higher than that average target now all you out there in the nation know i'm not about just dropping five stock picks in your lap and telling you what to buy that does nothing for you and i don't want you to be dependent on any old yahoo on youtube for your investments even if they do rock a bow tie so i want to take you through how i found these gross stocks how to screen for the best to buy and how to narrow your list i'll be using the stock screener on morningstar here but i'll explain it so you can use the screener on whichever platform you use the whole point of growth stocks is the fast sales growth so first you want to start by screening for companies growing their sales or total revenue by at least 20 or 30 percent a year you can play around with how much you screen for on these but that's the classic definition of a growth stock around that 20 or 30 percent annual growth in revenue and you can see here just limiting those to companies with more than 15 annual growth already puts us in the top quartile the best 25 of companies in the market so we're starting out on the right foot here but that still leaves us with over 3 000 stocks and we want to narrow our list of those growth stocks that are also profitable so here we're also going to screen for stocks with positive net income that's positive earnings or profits per share now remember net income is the earnings what's left over of sales after paying everything including taxes and interest so so this is where we'll find those profits so we'll just set this for the net income over the last 12 months to be greater than zero or a positive net income and that's going to narrow our list quite a bit but it's still way too many to research so let's go one step further and let's screen for growth stocks that are not only already profitable but also getting more profitable as they go so here we're talking about that net margin that net income divided by sales as a measure of profitability and we're looking for growth companies that are increasing in that measure so what we'll look for here is the net margin over the last 12 months that is higher than where it was over the last year that improving profitability from the last year to current here we're down to less than a thousand companies but since we're talking growth stocks and the companies changing the world i'm going to focus here just those in the technology sector those fast growing tech stocks obviously there are growth stocks in any other sector but for the sake of narrowing our list we're just going to look at tech stocks for now so here we can just screen in the stock sector and select only those in technology and finally that leaves us with a much more manageable list of 70 growth stocks to watch and we can sort these by size to start zoom video communications ticker zm didn'\"\"\"\n",
      "\n",
      "Your task is to extract trade recommendations from the text, if it contains any. Use the transcript content and the information provided before the transcript to determine whether the video is actually about financial topics and contains concrete trading/investment recommendations. If it does not, simply return an empty list. Please return a json list with an object for each mentioned asset, including the following fields:\n",
      "\n",
      "- asset name: name of the mentioned asset (e.g. company name). For etfs, provide either a ticker name, sector name or country/region name.\n",
      "- asset type: either \"stock\", \"crypto\", \"etf\", \"commodity\" or \"other\" if none of the previous apply\n",
      "- sentiment: corresponding recommendation, according to the speaker's sentiment in the transcript (positive -> \"buy\", neutral -> \"neutral\", negative -> \"sell\").\n",
      "\n",
      "The output MUST follow this exact format: [{\"asset_name\": \"name of asset 1\", \"asset_type\": \"stock/etf/crypto/commodity/other\", \"sentiment\": \"buy/sell/hold\"}, \"asset_name\": \"name of asset 2\", \"asset_type\": \"...\", \"sentiment\": \"...\"}, ...] [/INST]\n",
      "\n",
      "Sure, here's the json list with extracted trade recommendations:\n"
     ]
    }
   ],
   "source": [
    "# test prompt formatting\n",
    "prompt = format_prompt(feature_row=ds_inference[2345],\n",
    "                       prompt_format=\"mistral\",\n",
    "                       include_answer_tease=True,\n",
    "                       include_eos=False)['prompt']\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the three datasets (+ adding prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 80/80 [00:08<00:00,  9.51ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 25.05ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 63.32ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913199"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save splits to csv\n",
    "path = \"../data/transcript_chunks/splits\"\n",
    "filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt\"\n",
    "\n",
    "ds_inference.to_csv(f\"{path}/INF_{filename}.csv\", sep=\";\", index=False)\n",
    "ds_finetuning.to_csv(f\"{path}/FT_{filename}.csv\", sep=\";\", index=False)\n",
    "ds_validation.to_csv(f\"{path}/VAL_{filename}.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# check to make sure csv/pandas/arrow conversions don't introduce any discrepancies\n",
    "path = \"../data/transcript_chunks/splits\"\n",
    "filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt\"\n",
    "loaded_inf = Dataset.from_pandas(pd.read_csv(f\"{path}/INF_{filename}.csv\", sep=\";\"))\n",
    "loaded_ft = Dataset.from_pandas(pd.read_csv(f\"{path}/FT_{filename}.csv\", sep=\";\"))\n",
    "loaded_val = Dataset.from_pandas(pd.read_csv(f\"{path}/VAL_{filename}.csv\", sep=\";\"))\n",
    "\n",
    "if not loaded_inf.to_pandas().equals(ds_inference.to_pandas()):\n",
    "    diff = loaded_inf.to_pandas().compare(ds_inference.to_pandas())\n",
    "    print(f\"Discrepancies found in the following columns of inference data: {diff.columns.levels[0][0]}\")\n",
    "    print(diff)\n",
    "\n",
    "if not loaded_ft.to_pandas().equals(ds_finetuning.to_pandas()):\n",
    "    diff = loaded_ft.to_pandas().compare(ds_finetuning.to_pandas())\n",
    "    print(f\"Discrepancies found in the following columns of finetuning data: {diff.columns.levels[0][0]}\")\n",
    "    print(diff)\n",
    "\n",
    "if not loaded_val.to_pandas().equals(ds_validation.to_pandas()):\n",
    "    diff = loaded_val.to_pandas().compare(ds_validation.to_pandas())\n",
    "    print(f\"Discrepancies found in the following columns of validation data: {diff.columns.levels[0][0]}\")\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load finetuning and validation sets\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "path = \"../data/transcript_chunks/splits\"\n",
    "filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt\"\n",
    "\n",
    "ds_finetuning = Dataset.from_pandas(pd.read_csv(f\"{path}/FT_{filename}.csv\", sep=\";\"))\n",
    "ds_validation = Dataset.from_pandas(pd.read_csv(f\"{path}/VAL_{filename}.csv\", sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 6921.24 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  8.83ba/s]\n",
      "Map: 100%|██████████| 150/150 [00:00<00:00, 8795.30 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 30.36ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2037876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save an adjusted version of ft and valid splits to csv for labelling \n",
    "\n",
    "# add plain prompts, select columns for excel sheets and save\n",
    "ds = ds_finetuning.map(format_prompt, fn_kwargs={\"prompt_format\": \"plain\", \"include_answer_tease\": True, \"include_label\": False, \"include_eos\": False})\n",
    "ds = ds.select_columns(['video_id', 'chunk_number', 'yt_video_type', 'uploader_id', 'prompt', 'title', 'first_three_tags', 'chunk_text'])\n",
    "ds.to_csv(f\"../data/transcript_chunks/labeling/FT_{filename}_for_labeling.csv\", sep=\";\", index=False)\n",
    "\n",
    "ds = ds_validation.map(format_prompt, fn_kwargs={\"prompt_format\": \"plain\", \"include_answer_tease\": True, \"include_label\": False, \"include_eos\": False})\n",
    "ds = ds.select_columns(['video_id', 'chunk_number', 'yt_video_type', 'uploader_id', 'prompt', 'title', 'first_three_tags', 'chunk_text'])\n",
    "ds.to_csv(f\"../data/transcript_chunks/labeling/VAL_{filename}_for_labeling.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from excel sheet with labels\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "labeled_path = \"../data/transcript_chunks/labeling/labeling_sheet.xlsx\"\n",
    "val_labeled = pd.read_excel(labeled_path, sheet_name=\"VAL_labeling\", header=1)\n",
    "ft_labeled = pd.read_excel(labeled_path, sheet_name=\"FT_labeling\", header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only required columns\n",
    "cols = ['video_id', 'chunk_number', 'label']\n",
    "val_labeled = val_labeled[cols]\n",
    "ft_labeled = ft_labeled[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all labels match json schema\n",
    "import json\n",
    "from jsonschema import validate\n",
    "from LLM_utils import output_json_schema_string\n",
    "\n",
    "output_json_schema = json.loads(output_json_schema_string)\n",
    "\n",
    "def validate_labels(df, desc_str):\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            loaded = json.loads(row['label'])\n",
    "        except:\n",
    "            print(f\"Non-loadable label found in {desc_str} labels (video_id {row['video_id']}, chunk_number {row['chunk_number']}):\\n{row['label']}\")\n",
    "        if loaded:\n",
    "            try:\n",
    "                validate(loaded, output_json_schema)\n",
    "            except:\n",
    "                print(f\"Schema-violating json found in {desc_str} labels (video_id {row['video_id']}, chunk_number {row['chunk_number']}):\\n{row['label']}\")\n",
    "\n",
    "validate_labels(val_labeled, \"VAL\")\n",
    "validate_labels(ft_labeled, \"FT\")\n",
    "# no output -> everything matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before merging - FT: (400, 7), VAL: (150, 7)\n",
      "Shapes after merging - FT: (400, 8), VAL: (150, 8)\n"
     ]
    }
   ],
   "source": [
    "# load original ft and val splits to merge with labels\n",
    "splits_path = \"../data/transcript_chunks/splits\"\n",
    "filename = \"transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt\"\n",
    "ds_ft = pd.read_csv(f\"{splits_path}/FT_{filename}.csv\", sep=\";\")\n",
    "ds_val = pd.read_csv(f\"{splits_path}/VAL_{filename}.csv\", sep=\";\")\n",
    "\n",
    "print(f\"Shapes before merging - FT: {ds_ft.shape}, VAL: {ds_val.shape}\")\n",
    "# merge labels\n",
    "ds_ft = ds_ft.merge(ft_labeled, on=[\"video_id\", \"chunk_number\"], how=\"left\")\n",
    "ds_val = ds_val.merge(val_labeled, on=[\"video_id\", \"chunk_number\"], how=\"left\")\n",
    "\n",
    "print(f\"Shapes after merging - FT: {ds_ft.shape}, VAL: {ds_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "ds_ft.to_csv(f\"{splits_path}/FT_{filename}_with_labels.csv\", sep=\";\", index=False)\n",
    "ds_val.to_csv(f\"{splits_path}/VAL_{filename}_with_labels.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased Version of Finetuning Set\n",
    "\n",
    "- Since the high frequency of \"[]\" labels (i.e. transcripts with no recommendations) is possibly making the model too conservative in extracting recommendations, we create versions of the finetuning set in which the \"[]\" examples make up a lesser fraction of the data (by removing some of them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/400 empty labels found in FT set.\n",
      "Proportion of empty labels: 0.6275\n"
     ]
    }
   ],
   "source": [
    "# load ft set\n",
    "import pandas as pd\n",
    "ft_path = \"../data/transcript_chunks/splits\"\n",
    "ft_filename = \"FT_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt_with_labels\"\n",
    "\n",
    "df_ft = pd.read_csv(f\"{ft_path}/{ft_filename}.csv\", sep=\";\")\n",
    "\n",
    "n_empty = len(df_ft[df_ft['label'] == '[]'])\n",
    "p_empty = n_empty/len(df_ft)\n",
    "print(f\"{n_empty}/{len(df_ft)} empty labels found in FT set.\")\n",
    "print(f\"Proportion of empty labels: {p_empty:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 101 empty labels from FT set.\n",
      "New empty/total examples: 150/299\n",
      "New empty label proportion: 0.5017\n"
     ]
    }
   ],
   "source": [
    "# set desired proportion of empty labels in dataset\n",
    "p_empty_desired = 0.5\n",
    "seed = 42\n",
    "\n",
    "if p_empty_desired > p_empty:\n",
    "    raise(ValueError(\"Desired proportion of empty labels is bigger than the actual proportion of empty labels in the dataset.\"))\n",
    "\n",
    "# calculate number of empty labels to remove\n",
    "n_empty_to_remove = int((p_empty - p_empty_desired) * len(df_ft) / (1 - p_empty_desired))\n",
    "\n",
    "df_ft = df_ft.drop(df_ft[df_ft['label'] == '[]'].sample(n=n_empty_to_remove).index)\n",
    "\n",
    "p_empty_after = len(df_ft[df_ft['label'] == '[]'])/len(df_ft)\n",
    "\n",
    "print(f\"Removed {n_empty_to_remove} empty labels from FT set.\")\n",
    "print(f\"New empty/total examples: {len(df_ft[df_ft['label'] == '[]'])}/{len(df_ft)}\")\n",
    "\n",
    "print(f\"New empty label proportion: {p_empty_after:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save biased version of dataset\n",
    "df_ft.to_csv(f\"{ft_path}/FT_biased{p_empty_after:.2f}empty_{ft_filename[3:]}.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
