{"cells":[{"cell_type":"markdown","metadata":{"id":"IiU93J623t6H"},"source":["This notebook is intended to run on google colab."]},{"cell_type":"markdown","metadata":{"id":"StBCcp4sziw9"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17450,"status":"ok","timestamp":1715652836542,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"_0o0h9FVUAb1","outputId":"d95227f7-49db-4854-f377-970277a9fe25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive and define paths (do this in first cell because colab makes us click to confirm in a context window...)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","dir_path = \"/content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks\"\n","data_path = f\"{dir_path}/data\"\n","output_path = f\"{dir_path}/outputs\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":521137,"status":"ok","timestamp":1715653357678,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"SX1w4ZSm-LIp","outputId":"a4a48574-d5a7-4603-b26c-4006a5b7f27c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","Collecting llama-cpp-python==0.2.72\n","  Downloading llama_cpp_python-0.2.72.tar.gz (49.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Running command pip subprocess to install build dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting scikit-build-core[pyproject]>=0.9.2\n","    Downloading scikit_build_core-0.9.3-py3-none-any.whl (151 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.6/151.6 kB 3.0 MB/s eta 0:00:00\n","  Collecting exceptiongroup>=1.0 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n","  Collecting packaging>=21.3 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading packaging-24.0-py3-none-any.whl (53 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 5.5 MB/s eta 0:00:00\n","  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","  Collecting tomli>=1.2.2 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n","  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core\n","  Successfully installed exceptiongroup-1.2.1 packaging-24.0 pathspec-0.12.1 scikit-build-core-0.9.3 tomli-2.0.1\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Getting requirements to build wheel\n","  Could not determine CMake version via --version, got '' 'Traceback (most recent call last):\\n  File \"/usr/local/bin/cmake\", line 5, in <module>\\n    from cmake import cmake\\nModuleNotFoundError: No module named \\'cmake\\'\\n'\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Running command pip subprocess to install backend dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting cmake>=3.21\n","    Downloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 45.7 MB/s eta 0:00:00\n","  Collecting ninja>=1.5\n","    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 33.6 MB/s eta 0:00:00\n","  Installing collected packages: ninja, cmake\n","    Creating /tmp/pip-build-env-we0iaq7r/normal/local/bin\n","    changing mode of /tmp/pip-build-env-we0iaq7r/normal/local/bin/ninja to 755\n","    changing mode of /tmp/pip-build-env-we0iaq7r/normal/local/bin/cmake to 755\n","    changing mode of /tmp/pip-build-env-we0iaq7r/normal/local/bin/cpack to 755\n","    changing mode of /tmp/pip-build-env-we0iaq7r/normal/local/bin/ctest to 755\n","  Successfully installed cmake-3.29.3 ninja-1.11.1.1\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Preparing metadata (pyproject.toml)\n","  *** scikit-build-core 0.9.3 using CMake 3.29.3 (metadata_wheel)\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.72)\n","  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n","Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.72)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.72)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m191.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python==0.2.72)\n","  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m308.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.72)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Running command Building wheel for llama-cpp-python (pyproject.toml)\n","  *** scikit-build-core 0.9.3 using CMake 3.29.3 (wheel)\n","  *** Configuring CMake...\n","  loading initial cache file /tmp/tmpfkvldacb/build/CMakeInit.txt\n","  -- The C compiler identification is GNU 11.4.0\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","  -- Found Threads: TRUE\n","  CMake Warning at vendor/llama.cpp/CMakeLists.txt:389 (message):\n","    LLAMA_CUBLAS is deprecated and will be removed in the future.\n","\n","    Use LLAMA_CUDA instead\n","\n","\n","  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n","  -- CUDA found\n","  -- The CUDA compiler identification is NVIDIA 12.2.140\n","  -- Detecting CUDA compiler ABI info\n","  -- Detecting CUDA compiler ABI info - done\n","  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","  -- Detecting CUDA compile features\n","  -- Detecting CUDA compile features - done\n","  -- Using CUDA architectures: 52;61;70\n","  -- CUDA host compiler is GNU 11.4.0\n","\n","  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n","  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n","  -- x86 detected\n","  CMake Warning (dev) at CMakeLists.txt:26 (install):\n","    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n","  This warning is for project developers.  Use -Wno-dev to suppress it.\n","\n","  CMake Warning (dev) at CMakeLists.txt:35 (install):\n","    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n","  This warning is for project developers.  Use -Wno-dev to suppress it.\n","\n","  -- Configuring done (4.3s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/tmpfkvldacb/build\n","  *** Building project with Ninja...\n","  Change Dir: '/tmp/tmpfkvldacb/build'\n","\n","  Run Build Command(s): /tmp/pip-build-env-we0iaq7r/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n","  [1/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-alloc.c\n","  [2/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-backend.c\n","  [3/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/acc.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n","  [4/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-quants.c\n","  [5/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/alibi.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n","  [6/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml.c\n","  [7/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/arange.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n","  [8/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/argsort.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n","  [9/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/clamp.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n","  [10/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/binbcast.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n","  [11/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/concat.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n","  [12/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/diagmask.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n","  [13/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/cpy.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n","  [14/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/convert.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n","  [15/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/getrows.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n","  [16/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/dmmv.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n","  [17/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/im2col.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n","  [18/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/mmq.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n","  [19/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/norm.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n","  [20/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/pad.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n","  [21/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/pool2d.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n","  [22/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/quantize.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n","  [23/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/rope.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n","  [24/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/scale.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n","  [25/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/softmax.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n","  [26/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/sumrows.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n","  [27/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/tsembd.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n","  [28/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/unary.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n","  [29/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/upscale.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n","  [30/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n","  [31/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/fattn.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o\n","  [32/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/sgemm.cpp\n","  [33/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o.d -x cu -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/ggml-cuda/mmvq.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n","  [34/55] : && /tmp/pip-build-env-we0iaq7r/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n","  [35/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n","  [36/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/unicode.cpp\n","  [37/55] cd /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp && /tmp/pip-build-env-we0iaq7r/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  [38/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/build-info.cpp\n","  [39/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/unicode-data.cpp\n","  [40/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/sampling.cpp\n","  [41/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/console.cpp\n","  [42/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/grammar-parser.cpp\n","  [43/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/common.cpp\n","  [44/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/train.cpp\n","  [45/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n","  [46/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/ngram-cache.cpp\n","  [47/55] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/llava.cpp\n","  [48/55] /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/common/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/llava-cli.cpp\n","  [49/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/llama.cpp\n","  [50/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n","  [51/55] : && /tmp/pip-build-env-we0iaq7r/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n","  [52/55] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/vendor/llama.cpp/examples/llava/clip.cpp\n","  [53/55] : && /tmp/pip-build-env-we0iaq7r/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n","  [54/55] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmpfkvldacb/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl  /usr/lib/x86_64-linux-gnu/librt.a && :\n","  [55/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o  -Wl,-rpath,/tmp/tmpfkvldacb/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl  /usr/lib/x86_64-linux-gnu/librt.a && :\n","\n","  *** Installing project into wheel...\n","  -- Install configuration: \"Release\"\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/lib/libggml_shared.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpfkvldacb/wheel/platlib/lib/libggml_shared.so\" to \"\"\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/include/ggml.h\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/include/ggml-alloc.h\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/include/ggml-backend.h\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/include/ggml-cuda.h\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/lib/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpfkvldacb/wheel/platlib/lib/libllama.so\" to \"\"\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/include/llama.h\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/bin/convert.py\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/bin/convert-lora-to-ggml.py\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpfkvldacb/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n","  -- Installing: /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/llama_cpp/libllama.so\" to \"\"\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/lib/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpfkvldacb/wheel/platlib/lib/libllava.so\" to \"\"\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/bin/llava-cli\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpfkvldacb/wheel/platlib/bin/llava-cli\" to \"\"\n","  -- Installing: /tmp/tmpfkvldacb/wheel/platlib/llama_cpp/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpfkvldacb/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n","  -- Installing: /tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/llama_cpp/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-a0t0rq2q/llama-cpp-python_a5b454f7b19c45958a3087e591ecc2fe/llama_cpp/libllava.so\" to \"\"\n","  *** Making wheel...\n","  *** Created llama_cpp_python-0.2.72-cp310-cp310-linux_x86_64.whl...\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.72-cp310-cp310-linux_x86_64.whl size=55352858 sha256=e0f89cc9c4527eb20ac40f137817076b0dd381a8ae5b9cd94af7c09e7f104cf0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-rt7sp3d1/wheels/13/46/18/b6a11a030833f8c1c11651a80748bbcb2d567c8530c1b103e5\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.11.0\n","    Uninstalling typing_extensions-4.11.0:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.11.0.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n","      Successfully uninstalled typing_extensions-4.11.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Removing file or directory /usr/local/bin/f2py\n","      Removing file or directory /usr/local/bin/f2py3\n","      Removing file or directory /usr/local/bin/f2py3.10\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n","      Successfully uninstalled numpy-1.25.2\n","  changing mode of /usr/local/bin/f2py to 755\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.4\n","    Uninstalling Jinja2-3.1.4:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2-3.1.4.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n","      Successfully uninstalled Jinja2-3.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.72 numpy-1.26.4 typing-extensions-4.11.0\n","Collecting outlines==0.0.36\n","  Downloading outlines-0.0.36-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting interegular (from outlines==0.0.36)\n","  Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (3.1.4)\n","Collecting lark (from outlines==0.0.36)\n","  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.26.4)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.2.1)\n","Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (5.6.3)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.7.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.11.4)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.2.1+cu121)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (0.58.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.4.2)\n","Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (0.35.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (4.19.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.31.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (4.40.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.36) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.36) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.36) (4.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (3.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.36) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.36) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.36) (2023.12.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.36) (0.18.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.36) (0.41.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (2024.2.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (0.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (4.66.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->outlines==0.0.36) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, outlines\n","Successfully installed interegular-0.3.3 lark-1.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 outlines-0.0.36\n"]}],"source":["# prepare environment\n","!pip install datasets -q\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.72 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install outlines==0.0.36 # use older version to avoid \"cannot convert token to bytes\" error when creating generator (see https://github.com/outlines-dev/outlines/issues/820)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8288,"status":"ok","timestamp":1715653365963,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"_NZQlhG497R0"},"outputs":[],"source":["import torch\n","import outlines\n","\n","import pandas as pd\n","from datasets import Dataset\n","import json\n","import jsonschema\n","import time\n","import os\n","\n","# import stuff from custom LLM_utils module\n","import sys\n","sys.path.append('/content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/modules/')\n","from LLM_utils import output_json_schema_string, format_prompt, extract_json_from_output # needs to be uploaded to colab OR imported from mounted drive OR downloaded from github\n","output_json_schema = json.loads(output_json_schema_string) # will need this for validation when not using guided generation"]},{"cell_type":"markdown","metadata":{"id":"XY3cT-qPzoYq"},"source":["### Model Loading"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e38001d4935f4690bfaa80a418c1cfc5","3b4dbcb101dd449ba40640ca76c7821c","436b71b5b5234e32aa44e5ba85156b5d","d74036fa6c944586aef5da10e6a66971","49ddc954821445f5a94730c9115fdfab","fc6df0d898264766ab61536167e1603c","5f0a3a0550904f86b5934879d8d05ab0","5f82014a78a54926b3c4bd8671c00b26","680ae9bba8f348769989918f87b7900e","1613576ff7054bca89762e0b0a70b837","f7373612e260464494415c6cda168d42"]},"executionInfo":{"elapsed":210464,"status":"ok","timestamp":1715653576418,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"IFw00KXv-ywK","outputId":"c3d7f165-8593-4d07-873e-cc6a2d248739"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38001d4935f4690bfaa80a418c1cfc5","version_major":2,"version_minor":0},"text/plain":["(…)8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--JanJacobsen--llama3_8b_instruct_ft_v4_q8_0/snapshots/4b72fbb22eba29d05d31d002086e9f5696e90c3d/./llama3_8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = llama3_8b_instruct_ft_v4_q8_0\n","llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 7\n","llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n","llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n","llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = llama3_8b_instruct_ft_v4_q8_0\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: PAD token        = 128255 '<|reserved_special_token_250|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_tensors: ggml ctx size =    0.30 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =   532.31 MiB\n","llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB\n",".........................................................................................\n","llama_new_context_with_model: n_ctx      = 4096\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n","llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128255', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'llama3_8b_instruct_ft_v4_q8_0', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n","Available chat formats from metadata: chat_template.default\n","Guessed chat format: llama-3\n"]}],"source":["# loading model\n","from llama_cpp import Llama\n","\n","############################################################################################################\n","# TheBloke repo: mistral quants\n","#repo_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n","#filename = \"mistral-7b-instruct-v0.2.Q8_0.gguf\"\n","\n","\n","# my repo (finetunes)\n","repo_id = \"JanJacobsen/llama3_8b_instruct_ft_v4_q8_0\"\n","filename = \"llama3_8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf\"\n","\n","# my repo (llama3 base f16 and quants)\n","#repo_id = \"JanJacobsen/llama3_8b_instruct_q8_0\"\n","#filename = \"llama3_8b_instruct_q8_0-unsloth.Q8_0.gguf\"\n","\n","\n","############################################################################################################\n","\n","llm = Llama.from_pretrained(\n","    repo_id=repo_id,\n","    filename=filename,\n","    device=\"cuda\",\n","    n_gpu_layers=-1, # offload entire model to gpu\n","    n_ctx = 4*1024\n",")\n","llm.verbose = False\n","# llama_cpp model to outlines model\n","model = outlines.models.LlamaCpp(llm)\n"]},{"cell_type":"markdown","metadata":{"id":"_ZDFI-y-0BNi"},"source":["### Testing generation before main inference loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["aaa65fd117fb4995a1d58c4adf1318ce","60e3b81a8dea432785f51fed722b072b","fb2de11ea0a74a6a9500aac2c3319004","7c48258d73294f1c8d67ec9ea8f41864","6c87b4f2eca44387a31116fe5955d984","a7507fc2c42a48d19ef12309096c5457","60ec28a341164ff6b28393577c0e4804","7b6242123e124f43baca92d190f19d67","ac93e51efe4949c38a2810d79fbf1780","63cc5ae792084162a42ff90c31dc637c","e945eaf96d9c4ef8a5830b4ec42200eb"]},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1715625236967,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"CZOoLtTvhdYX","outputId":"6a0f923d-1839-4be1-a998-beedac253606"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aaa65fd117fb4995a1d58c4adf1318ce","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/150 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["if False:\n","    # load data for testing\n","    data_file_path = f\"{data_path}/VAL_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\"\n","\n","    data = Dataset.from_pandas(pd.read_csv(data_file_path, sep=\";\"))\n","\n","    # add prompts\n","    data = data.map(format_prompt, fn_kwargs={\"prompt_format\": \"llama3\",\n","                                                        \"include_answer_tease\": True,\n","                                                        \"include_label\": False,\n","                                                        \"include_bos\": True,\n","                                                        \"include_eos\": False})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1715625236968,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"Bcz3h_VNJM_e","outputId":"626d8810-58e2-413d-a921-6d2a484f6aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","You are a smart and efficient assistant specialized at extracting relevant information from text and replying in json format. You always follow the user's instructions carefully.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","The triple-quoted text below is part of a youtube video transcript by channel @EverythingMoney with the title 'Verizon Stock Analysis | Top Stocks to Buy Now? | VZ Stock Price'. The top tags for the video are: 'everything money, investing, investing in your 20s'. Read the transcript carefully in order to perform the asset name extraction task specified below the transcript.\n","\n","\"\"\"to 2020 we're spending that additional 18 billion annual catback spending okay so let's look at this look at verizon wireless capital expenditures for 2021. so again guys understand the 63 billion dollar number because it's a big jump but we read reports that for the year they're supposed to do 17 to 18 billion in capital expenditures which actually would have been down from last year but something to understand capital expenditures tend to be one-time items so i would not cry so much about that and i do think it's skews man i don't think excuse 100 skews our five year average if this just goes to zero it makes the five-year average free cash flow over 10 billion dollars if this is this free cash flow just goes to zero are you gonna have a tough time with pillar number eight here yeah i mean this is an x what we usually do is we take the five year average free cash flow multiplied by 20. that's 126 billion dollars what is it actually 210 is the actual market cap now if we added if we did a normal year if we did a normal year of 15 billion in free cash flow that would make this average 13 billion times 20 of 260 billion dollars and maybe that's why they invested in it so it's maybe being skewed right now to the downside now what we're gonna do is we're gonna use so here's our eight pillar tab it just shows you the recap here a lot of x's let's use our stock analyzer tool to make assumptions about the future because every investment's the present value of all future cash flow we have to look at free we have to look at revenue growth profit to determine a value for the future it's all unknown so we want to be conservative so our most famous part of our software the stock analyzer tool i love going on our discord chat and just seeing screenshots of stock analyzers people love it i'm like scrolling through anyways ten-year analysis okay let's be conservative let's say one two and three percent revenue growth oh wow i was to go less you were going to go less yeah where you go to i was going to go negative 1 0 1. let's do it profit margin um i probably put 12 on the high side right not the high side because look they did that acquisition and it really helped position skewing things yeah so maybe go 13 15 13 14 15. okay free cash flow margin oh my that's a problem it's so skewed by that let's go seven eight nine p e ten twelve fourteen price to fee cash flow ten 12 14. and then return on investment 12 desired annual turn twelve and a half percent because guys it's a it's a if you're gonna just go buy an etf and make nine to ten percent in order to buy an individual company have your margin of safety make it worth your while analyze button okay i still don't i mean it's not hitting my sweet spots here low side big skus and numbers 18 to 35 high size 30 to 50 middle range 24 to 40. so it's not to me i look at it going i love that warren buffett has it he sees something i don't i'm okay with that i'm just gonna move on from it i don't see it now if the stock fell in half let's talk yeah four months ago we made some assumptions using the stock analyzer tool we were right in that that spot again paul our mid assumptions got us a buy price of 22 to 45 that's what we need to pay to get our desired return so we're still so if you're interested if these numbers speak to you uh i know uh from from a personal standpoint i used to buy stocks and just hope they went up paul i had a discussion with a dear friend of ours today about the difference of assets like a home and stocks versus collectibles but if this was you and saying well i hope i just buy in my own hope i wanted to just go up and if it doesn't i'm in trouble if this resonates with you you can have this software you can help yourself make better decisions paul tell them how they can get it right now we created the software because our subscribers people who watch their videos said hey how do we analyze companies without waiting for you to make a video on it so we said great let's make the software we spent hundreds of thousands created everything you see here you get everything you see here you get it all on your mobile phone you get 30 years of financial data you get access to seth moe and i you get the whole 8 pillar analysis stock analyzer tool you even get real estate calculator and everything on this main page that is coming soon you'll see right here everything coming soon here on the right you even get the the portfolios you can put up to 50 stocks in a portfolio it'll tell you how they look all together you get exclusive daily content that's two or three videos a day from seth mo and i only for our users who buy the software but most importantly you get access to our chat you get all this point 90 cents per day guys less than a cup of coffee 90 cents a day you get all this and the best part is you get this at the same price forever your price doesn't go up as our price goes up with it you only get increases for inflation you get all this 90 cents a day two ways to sign up everythingmoney.com or patreon the benefit of going directly to everythingmoney.com is we are not large enough yet to charge the sales tax so save that for the time being 90 cents a day if you just increase your returns by one or two percent a year or decrease your losses by one or two percent a year it could lead to hundreds of thousands if not millions 90 cents a day go sign up now you want to trade verizon at a quicker pace there is money to be made on these rises and falls and mo can show you how if you join the bid nast nation go ahead mel guys um so if we just look back to covet that's kind of our that's kind of our support level that we're sitting at let me do a refresh right here that's kind of the support level that we're sitting at um there is still room for it to fall though it's it's very possible that it can just keep falling let me zoom in here there we go this is where i'm looking at it could it i mean it's fallen except with the exception of today being up a little bit i guess you know what guys from a long-term perspective i wouldn't even pay attention to this i would come over here and swing trade this just go with these ups and downs you're under all four major moving averages swing it up going into the 25-day moving average let it do whatever it's gonna do and then try to catch these little blips here and there because this is so to me like a t this is a dinosaur stock this is a very hard stock to trade on a day-to-day basis now with that being said look at this stuff today it's a perfect perfect day trade this is rare but it's a perfect day trade you the the rules are enter over 80 percent with good buying volume and an engulfing candlestick which you got at 10 a.m and you could have bought this thing at 10 a.m at about 50 and 40 cents and you're still in it right now your exit point is going to be when this percent k this number right here drops below 80 and that is your exit so you get lucky today with with a stock like this but to me guys i would wait until a time where you see your long-term stochastic going up just like att this is a great stock to go long on but right now isn't is not that time so if you want rules and you want to find stocks that are good to trade come and join me in the bid and ask nation there are tons of people there's 900 people in there that are all talking about stocks you get my trading 101 series which is a series of videos that teaches you all the rules employ trader series which gives you six stocks that we go through every single day long short long term swing trades that will make you money if you are a person that works and don't have the time to do this exclusive monthly seminars that happen once a month on saturdays and the bidness community like i said with 900 plus people we will keep you updated on verizon hopefully it falls we're hoping for most of the stocks to fall paul right i mean except alibaba which we own a ton of oh we want it to fall still i don't want to buy stock at lower prices i guess that's true fondle a thumbs up join the community you'll get the software and you'll love it see you next video [Applause] [Music]\"\"\"\n","\n","Your task is to extract trade recommendations from the text, if it contains any. Use the transcript content and the information provided before the transcript to determine whether the video is actually about financial topics and contains concrete trading/investment recommendations. If it does not, simply return an empty list. Please return a json list with an object for each mentioned asset, including the following fields:\n","\n","- asset name: name of the mentioned asset (e.g. company name). For etfs (=any fund or index) provide either a ticker, index name, sector name or country/region name. Some names might be mistranscribed in the text, in which case you should infer the correct name from the context.\n","- asset type: either \"stock\", \"crypto\", \"etf\", \"commodity\" or \"other\" if none of the previous apply\n","- sentiment: corresponding recommendation, according to the speaker's sentiment in the transcript (positive -> \"buy\", neutral -> \"neutral\", negative -> \"sell\").\n","\n","The output MUST follow this exact format: [{\"asset_name\": \"name of asset 1\", \"asset_type\": \"stock/etf/crypto/commodity/other\", \"sentiment\": \"buy/sell/hold\"}, \"asset_name\": \"name of asset 2\", \"asset_type\": \"...\", \"sentiment\": \"...\"}, ...]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","Sure, here's the json list with extracted trade recommendations:\n","\n","\n"]}],"source":["if False:\n","    # check an example prompt\n","    prompt = data[20]['prompt']\n","    print(prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29988,"status":"ok","timestamp":1715625969102,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"0dd5U-iL_rsv","outputId":"df1368c8-5eea-4d50-8abd-295c1acaede1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}]\n","[{\"asset_name\": \"Palantir\", \"asset_type\": \"stock\", \"sentiment\": \"buy\"}, {\"asset_name\": \"Ethereum\", \"asset_type\": \"crypto\", \"sentiment\": \"buy\"}]\n"]}],"source":["if False: # test 10 generations (model directly)\n","    prompt = data[22]['prompt']\n","    temp = 0.01\n","    for i in range(10):\n","        result = llm.create_completion(prompt, temperature=temp, top_k=50, max_tokens=1000)\n","        print(result['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TTD5yyud_Cy"},"outputs":[],"source":["if False: # test generator creation\n","    # create outlines generator with custom json schema constraints\n","    generator = outlines.generate.json(model, output_json_schema_string, whitespace_pattern=r\"[ \\n\\t]?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1kIMOxJNfbB"},"outputs":[],"source":["if False:\n","    # test example prompt\n","    result = generator(prompt, temperature=0.01, top_k=50, max_tokens=1000)\n","    result = json.dumps(result)\n","    print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhFan45mkB0R"},"outputs":[],"source":["if False: # test generator in loop (do we have to recreate each time?)\n","    prompt = data[22]['prompt']\n","    test_recreation = False\n","    for i in range(10):\n","        if test_recreation:\n","            generator = outlines.generate.json(model, output_json_schema_string, whitespace_pattern=r\"[ \\n\\t]?\")\n","        result = generator(prompt, temperature=1.5, top_k=50, max_tokens=1000)\n","        result = json.dumps(result)\n","        print(result)"]},{"cell_type":"markdown","metadata":{"id":"67pRJRhD0KPf"},"source":["### Prepare Data & Inference"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141,"referenced_widgets":["2acbb2f95a0c4b64965baeb7d7754c97","1b5acde1106a4326be225469498b2db2","8d4df1123e044c5f8b66d08290935d20","25ec852bf91a4fe1a79c00fa01572556","ab913ae4da7b494c8a96a26c7a7db5fc","135bcaef716f4e1292c55b91665219e2","37efb35234b54d8a877fa1dc076cf396","4a64a20d50324629a64eae8a024c4191","2d5fc4e14da5477c971af80d28d2ca59","906f205e1b50424fa8ab00fe010a853a","b6250bcf714848689e39606a91a19b47"]},"executionInfo":{"elapsed":713,"status":"ok","timestamp":1715653577129,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"laMVh5TPJM_e","outputId":"a4d81555-779f-4369-9a59-fcf94092e609"},"outputs":[{"name":"stdout","output_type":"stream","text":["Progress file /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_llama3_ft_v4_q8_0_llamacpp_unguided.csv does not exist, creating new one.\n","Loaded dataset with 150 samples from /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/data/VAL_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\n","Loaded progress file with 0 processed examples from /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_llama3_ft_v4_q8_0_llamacpp_unguided.csv\n","Examples yet to process in dataset: 150\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2acbb2f95a0c4b64965baeb7d7754c97","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/150 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preparation complete for run: val_llama3_ft_v4_q8_0_llamacpp_unguided\n"]}],"source":["##########################################################################################################################################################\n","# ------------------- run name (adjust for every new run!) ------------------ #\n","run_type = \"val\" # \"val\" (= test) or \"inf\"\n","run_model = \"llama3_ft_v4\" # mistral/llama3/... + _ft_version if finetuned\n","run_model_quant = \"q8_0\"\n","run_framework = \"llamacpp\" # llamacpp/vllm/transformers/...\n","additional_suffix = \"_unguided\"\n","# --------------------------------------------------------------------------- #\n","run_name = f\"{run_type}_{run_model}_{run_model_quant}_{run_framework}{additional_suffix}\"\n","data_file_path = f\"{data_path}/{run_type.upper()}_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\"\n","progress_file_path = f\"{output_path}/{run_name}.csv\" # file does not need to exist yet\n","\n","# prompt formatting arguments (for our custom function)\n","prompt_format_kwargs={\n","    \"prompt_format\": \"llama3\",\n","    \"include_bos\": True, # does llamacpp add it or not?\n","    \"include_answer_tease\": True,\n","    \"include_label\": False, # never for inference, only used for finetuning\n","    \"include_eos\": False}\n","\n","# model parameters (might be model-specific)\n","temperature = 0.01 # very low / zero for our task\n","top_k = 50 # relatively high value to avoid problems with guided generation (i.e. what if token required by schema is not in top k?)\n","seed = 42 # relevant for sampling (CAUTION: can't be used with llamacpp & outlines==0.0.36)\n","max_tokens = 1250 # might have to be set for llamacpp to override default? unsure...\n","\n","# how are we calling the model? with guided generation (outlines) active, or directly via llama_cpp?\n","guided_generation = False\n","\n","# outlines-specific\n","apply_reset_generator_fix = False # in some configurations with outlines & llamacpp the outlines.generator.json() needs to be reset before each new call\n","generator_whitespace_pattern = r\"[ \\n\\t]?\" # should we allow whitespaces/newlines/etc. being generated within the json structure? probably yes, but limit to zero or one at a time\n","def get_generator(whitespace_pattern): # to make sure we call it with the same parameters everywhere (e.g. whitespace pattern)\n","    return outlines.generate.json(model, output_json_schema_string, whitespace_pattern=whitespace_pattern)\n","\n","# other run parameters\n","skip_previous_errors = True # skip examples with True in 'error?' column?\n","replace_previous_errors = True # only relevant if skip_previous_errors is False: should previous errors be replaced with new outputs (if available) in progress file?\n","save_interval = 50 # update progress file every n examples (note: should not be set too low because saving to and loading from drive might not actually update files instantly? note sure...)\n","print_progress_interval = 10 # print progress every n examples\n","print_errors = True # useful to catch issues, but disable for final inference runs with confirmed problem-free models/code (to avoid crashes due to huge cell output)\n","\n","##########################################################################################################################################################\n","\n","# check if files exist\n","import os\n","if not os.path.exists(data_file_path):\n","    raise ValueError(f\"Data file {data_file_path} does not exist!\")\n","if not os.path.exists(progress_file_path):\n","    print(f\"Progress file {progress_file_path} does not exist, creating new one.\")\n","    pd.DataFrame(columns=[\"video_id\", \"chunk_number\", \"output\", \"error?\"]).to_csv(progress_file_path, sep=\";\", index=False)\n","\n","# load data\n","data = pd.read_csv(data_file_path, sep=\";\")\n","print(f\"Loaded dataset with {len(data)} samples from {data_file_path}\")\n","\n","# load progress data\n","progress = pd.read_csv(progress_file_path, sep=\";\")\n","print(f\"Loaded progress file with {len(progress)} processed examples from {progress_file_path}\")\n","\n","# merge progress data with main data (-> adds 'output' and 'error?' columns from progress df)\n","data = pd.merge(data, progress, on=[\"video_id\", \"chunk_number\"], how=\"left\")\n","\n","# filter out already processed examples (and optionally previous examples which caused errors)\n","data = data[data['output'].isnull()]\n","if skip_previous_errors:\n","    data = data[data['error?'] != True] # None and False should both stay in, only filter out True\n","print(f\"Examples yet to process in dataset: {len(data)}\")\n","\n","# convert to hf dataset\n","data = Dataset.from_pandas(data)\n","\n","# add prompts\n","data = data.map(format_prompt, fn_kwargs=prompt_format_kwargs)\n","\n","# create outlines generator (and check time)\n","if guided_generation:\n","    start_time = time.time()\n","    generator = get_generator(generator_whitespace_pattern)\n","    print(f\"Created outlines generator in {round(time.time() - start_time, 2)} seconds.\")\n","\n","print(f\"Preparation complete for run: {run_name}\")"]},{"cell_type":"markdown","metadata":{"id":"CF9dKXx00TFD"},"source":["### Inference Loop"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447919,"status":"ok","timestamp":1715654025046,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"BgdQttaOCHLM","outputId":"460d5b83-0714-4c1c-a32e-a530fc3a016e"},"outputs":[{"name":"stdout","output_type":"stream","text":["*** STARTING INFERENCE FOR 150 EXAMPLES (run name: val_llama3_ft_v4_q8_0_llamacpp_unguided) ***\n","------------------------------------------------------------\n","Processed 10 total examples in 0 h, 0 min, 25.2 sec\n","Processed 20 total examples in 0 h, 0 min, 48.63 sec\n","Processed 30 total examples in 0 h, 1 min, 26.28 sec\n","Processed 40 total examples in 0 h, 2 min, 9.84 sec\n","  - Saved 50 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_llama3_ft_v4_q8_0_llamacpp_unguided.csv\n","Processed 50 total examples in 0 h, 2 min, 33.9 sec\n","Processed 60 total examples in 0 h, 2 min, 54.39 sec\n","Processed 70 total examples in 0 h, 3 min, 34.84 sec\n","Processed 80 total examples in 0 h, 4 min, 8.51 sec\n","Processed 90 total examples in 0 h, 4 min, 47.67 sec\n","  - Saved 50 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_llama3_ft_v4_q8_0_llamacpp_unguided.csv\n","Processed 100 total examples in 0 h, 5 min, 9.44 sec\n","Processed 110 total examples in 0 h, 5 min, 37.73 sec\n","Processed 120 total examples in 0 h, 5 min, 59.87 sec\n","Processed 130 total examples in 0 h, 6 min, 35.58 sec\n","Processed 140 total examples in 0 h, 7 min, 9.07 sec\n","  - Saved 50 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_llama3_ft_v4_q8_0_llamacpp_unguided.csv\n","Processed 150 total examples in 0 h, 7 min, 27.91 sec\n","  - Saved 0 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_llama3_ft_v4_q8_0_llamacpp_unguided.csv\n","------------------------------------------------------------\n","Finished processing 150 examples in 0 h, 7 min, 27.93 sec\n"]}],"source":["# <- STARTS ITERATION\n","\n","# helper function to print elapsed time\n","def get_elapsed_time_str(start_time):\n","    hours, rem = divmod(time.time() - start_time, 3600)\n","    minutes, seconds = divmod(rem, 60)\n","    return f\"{int(hours)} h, {int(minutes)} min, {round(seconds, 2)} sec\"\n","\n","# helper function for saving progress\n","def update_progress_file(new_results, progress_file_path, replace_previous_errors):\n","#    1. convert to pandas df\n","    new_results_df = pd.DataFrame(new_results, columns=[\"video_id\", \"chunk_number\", \"output\", \"error?\"])\n","    # 2. load already processed data and append new results (if it exists)\n","    if os.path.exists(progress_file_path):\n","        progress_df = pd.read_csv(progress_file_path, sep=\";\")\n","        progress_df = pd.concat([progress_df, new_results_df], ignore_index=True)\n","        # drop duplicate error rows (duplicates here should only happen for previous error rows which we included and got errors again)\n","        progress_df = progress_df.drop_duplicates(subset=[\"video_id\", \"chunk_number\", \"error?\"], keep=\"last\")\n","    else:\n","        progress_df = new_results_df\n","\n","    #3. optionally, if there are new results for examples which previously caused errors, replace the error rows (otherwise: keep both the error row and the new result row in the file)\n","    if replace_previous_errors:\n","        # note: since we used pd.concat above we know that new results are appended at the end of the df, which allows us to use keep=\"last\" here to keep the new result row (without sorting first)\n","        progress_df = progress_df.drop_duplicates(subset=[\"video_id\", \"chunk_number\"], keep=\"last\")\n","    # 3. save\n","    progress_df.to_csv(progress_file_path, sep=\";\", index=False)\n","    print(f\"  - Saved {len(new_results)} new results to {progress_file_path}\")\n","\n","print(f\"*** STARTING INFERENCE FOR {len(data)} EXAMPLES (run name: {run_name}) ***\\n{'-'*60}\")\n","\n","# inference loop\n","new_results = []\n","start_time = time.time()\n","for i, example in enumerate(data):\n","\n","    video_id = example['video_id']\n","    uploader_id = example['uploader_id']\n","    chunk_number = example['chunk_number']\n","\n","    # model call\n","    try:\n","        if guided_generation:\n","            if apply_reset_generator_fix:\n","                generator = get_generator(generator_whitespace_pattern)\n","            # outlines model call -> dump json to string\n","            result = generator(example['prompt'], temperature=temperature, top_k=top_k, max_tokens=max_tokens)\n","            result = json.dumps(result)\n","\n","        else:\n","            # llamacpp model call -> try to extract and validate the json (the model output might contain additional text or invalid json)\n","            result = llm(example['prompt'], temperature=temperature, top_k=top_k, max_tokens=max_tokens)['choices'][0]['text']\n","            result = extract_json_from_output(result) # returns string or raises error\n","            # validate\n","            result = json.loads(result)\n","            jsonschema.validate(instance=result, schema=output_json_schema) # raises error if instance doesn't match schema\n","            result = json.dumps(result) # we convert back to string to ensure clean structure (no newlines etc.)\n","        # no error: append result\n","        new_results.append((video_id, chunk_number, result, False))\n","\n","    except Exception as e:\n","\n","        if print_errors:\n","            print(f\"Error at iteration {i} (video_id: {video_id}, chunk_number: {chunk_number}) - {e} \")\n","\n","        new_results.append((video_id, chunk_number, None, True))\n","\n","    # save progress\n","    if (i+1) % save_interval == 0:\n","        update_progress_file(new_results, progress_file_path, replace_previous_errors)\n","        new_results = []\n","\n","\n","\n","    # print progress\n","    if (i+1) % print_progress_interval == 0:\n","        print(f\"Processed {i+1} total examples in {get_elapsed_time_str(start_time)}\")\n","\n","\n","# save remaining examples\n","update_progress_file(new_results, progress_file_path, replace_previous_errors)\n","\n","print(f\"{'-'*60}\\nFinished processing {len(data)} examples in {get_elapsed_time_str(start_time)}\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb","timestamp":1710611387839}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"135bcaef716f4e1292c55b91665219e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1613576ff7054bca89762e0b0a70b837":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b5acde1106a4326be225469498b2db2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_135bcaef716f4e1292c55b91665219e2","placeholder":"​","style":"IPY_MODEL_37efb35234b54d8a877fa1dc076cf396","value":"Map: 100%"}},"25ec852bf91a4fe1a79c00fa01572556":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_906f205e1b50424fa8ab00fe010a853a","placeholder":"​","style":"IPY_MODEL_b6250bcf714848689e39606a91a19b47","value":" 150/150 [00:00&lt;00:00, 1651.12 examples/s]"}},"2acbb2f95a0c4b64965baeb7d7754c97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b5acde1106a4326be225469498b2db2","IPY_MODEL_8d4df1123e044c5f8b66d08290935d20","IPY_MODEL_25ec852bf91a4fe1a79c00fa01572556"],"layout":"IPY_MODEL_ab913ae4da7b494c8a96a26c7a7db5fc"}},"2d5fc4e14da5477c971af80d28d2ca59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37efb35234b54d8a877fa1dc076cf396":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b4dbcb101dd449ba40640ca76c7821c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc6df0d898264766ab61536167e1603c","placeholder":"​","style":"IPY_MODEL_5f0a3a0550904f86b5934879d8d05ab0","value":"(…)8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf: 100%"}},"436b71b5b5234e32aa44e5ba85156b5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f82014a78a54926b3c4bd8671c00b26","max":8540770656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_680ae9bba8f348769989918f87b7900e","value":8540770656}},"49ddc954821445f5a94730c9115fdfab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a64a20d50324629a64eae8a024c4191":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f0a3a0550904f86b5934879d8d05ab0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f82014a78a54926b3c4bd8671c00b26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60e3b81a8dea432785f51fed722b072b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7507fc2c42a48d19ef12309096c5457","placeholder":"​","style":"IPY_MODEL_60ec28a341164ff6b28393577c0e4804","value":"Map: 100%"}},"60ec28a341164ff6b28393577c0e4804":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63cc5ae792084162a42ff90c31dc637c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"680ae9bba8f348769989918f87b7900e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c87b4f2eca44387a31116fe5955d984":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b6242123e124f43baca92d190f19d67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c48258d73294f1c8d67ec9ea8f41864":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63cc5ae792084162a42ff90c31dc637c","placeholder":"​","style":"IPY_MODEL_e945eaf96d9c4ef8a5830b4ec42200eb","value":" 150/150 [00:00&lt;00:00, 1917.52 examples/s]"}},"8d4df1123e044c5f8b66d08290935d20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a64a20d50324629a64eae8a024c4191","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d5fc4e14da5477c971af80d28d2ca59","value":150}},"906f205e1b50424fa8ab00fe010a853a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7507fc2c42a48d19ef12309096c5457":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa65fd117fb4995a1d58c4adf1318ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60e3b81a8dea432785f51fed722b072b","IPY_MODEL_fb2de11ea0a74a6a9500aac2c3319004","IPY_MODEL_7c48258d73294f1c8d67ec9ea8f41864"],"layout":"IPY_MODEL_6c87b4f2eca44387a31116fe5955d984"}},"ab913ae4da7b494c8a96a26c7a7db5fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac93e51efe4949c38a2810d79fbf1780":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6250bcf714848689e39606a91a19b47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d74036fa6c944586aef5da10e6a66971":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1613576ff7054bca89762e0b0a70b837","placeholder":"​","style":"IPY_MODEL_f7373612e260464494415c6cda168d42","value":" 8.54G/8.54G [02:48&lt;00:00, 49.0MB/s]"}},"e38001d4935f4690bfaa80a418c1cfc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b4dbcb101dd449ba40640ca76c7821c","IPY_MODEL_436b71b5b5234e32aa44e5ba85156b5d","IPY_MODEL_d74036fa6c944586aef5da10e6a66971"],"layout":"IPY_MODEL_49ddc954821445f5a94730c9115fdfab"}},"e945eaf96d9c4ef8a5830b4ec42200eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7373612e260464494415c6cda168d42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb2de11ea0a74a6a9500aac2c3319004":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b6242123e124f43baca92d190f19d67","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac93e51efe4949c38a2810d79fbf1780","value":150}},"fc6df0d898264766ab61536167e1603c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
