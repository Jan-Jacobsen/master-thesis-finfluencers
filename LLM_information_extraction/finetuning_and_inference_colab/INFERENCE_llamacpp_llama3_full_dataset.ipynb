{"cells":[{"cell_type":"markdown","metadata":{"id":"IiU93J623t6H"},"source":["This notebook is intended to run on google colab."]},{"cell_type":"markdown","metadata":{"id":"StBCcp4sziw9"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19635,"status":"ok","timestamp":1717002782557,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"_0o0h9FVUAb1","outputId":"aa39b9b7-2ce8-4000-cccf-92627ac41fdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive and define paths (do this in first cell because colab makes us click to confirm in a context window...)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","dir_path = \"/content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks\"\n","data_path = f\"{dir_path}/data\"\n","output_path = f\"{dir_path}/outputs\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":573390,"status":"ok","timestamp":1717003355945,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"SX1w4ZSm-LIp","outputId":"3d122562-5516-4b2c-c08a-447c442d3874"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","Collecting llama-cpp-python==0.2.72\n","  Downloading llama_cpp_python-0.2.72.tar.gz (49.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Running command pip subprocess to install build dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting scikit-build-core[pyproject]>=0.9.2\n","    Downloading scikit_build_core-0.9.4-py3-none-any.whl (151 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.0/152.0 kB 472.0 kB/s eta 0:00:00\n","  Collecting exceptiongroup>=1.0 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n","  Collecting packaging>=21.3 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading packaging-24.0-py3-none-any.whl (53 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 6.7 MB/s eta 0:00:00\n","  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","  Collecting tomli>=1.2.2 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n","  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core\n","  Successfully installed exceptiongroup-1.2.1 packaging-24.0 pathspec-0.12.1 scikit-build-core-0.9.4 tomli-2.0.1\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Getting requirements to build wheel\n","  Could not determine CMake version via --version, got '' 'Traceback (most recent call last):\\n  File \"/usr/local/bin/cmake\", line 5, in <module>\\n    from cmake import cmake\\nModuleNotFoundError: No module named \\'cmake\\'\\n'\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Running command pip subprocess to install backend dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting cmake>=3.21\n","    Downloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 8.1 MB/s eta 0:00:00\n","  Collecting ninja>=1.5\n","    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 9.3 MB/s eta 0:00:00\n","  Installing collected packages: ninja, cmake\n","    Creating /tmp/pip-build-env-21qckkzb/normal/local/bin\n","    changing mode of /tmp/pip-build-env-21qckkzb/normal/local/bin/ninja to 755\n","    changing mode of /tmp/pip-build-env-21qckkzb/normal/local/bin/cmake to 755\n","    changing mode of /tmp/pip-build-env-21qckkzb/normal/local/bin/cpack to 755\n","    changing mode of /tmp/pip-build-env-21qckkzb/normal/local/bin/ctest to 755\n","  Successfully installed cmake-3.29.3 ninja-1.11.1.1\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Preparing metadata (pyproject.toml)\n","  *** scikit-build-core 0.9.4 using CMake 3.29.3 (metadata_wheel)\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.72)\n","  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n","Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.72)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.72)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m173.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python==0.2.72)\n","  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m292.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.72)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Running command Building wheel for llama-cpp-python (pyproject.toml)\n","  *** scikit-build-core 0.9.4 using CMake 3.29.3 (wheel)\n","  *** Configuring CMake...\n","  loading initial cache file /tmp/tmpwf1hb6ok/build/CMakeInit.txt\n","  -- The C compiler identification is GNU 11.4.0\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","  -- Found Threads: TRUE\n","  CMake Warning at vendor/llama.cpp/CMakeLists.txt:389 (message):\n","    LLAMA_CUBLAS is deprecated and will be removed in the future.\n","\n","    Use LLAMA_CUDA instead\n","\n","\n","  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n","  -- CUDA found\n","  -- The CUDA compiler identification is NVIDIA 12.2.140\n","  -- Detecting CUDA compiler ABI info\n","  -- Detecting CUDA compiler ABI info - done\n","  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","  -- Detecting CUDA compile features\n","  -- Detecting CUDA compile features - done\n","  -- Using CUDA architectures: 52;61;70\n","  -- CUDA host compiler is GNU 11.4.0\n","\n","  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n","  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n","  -- x86 detected\n","  CMake Warning (dev) at CMakeLists.txt:26 (install):\n","    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n","  This warning is for project developers.  Use -Wno-dev to suppress it.\n","\n","  CMake Warning (dev) at CMakeLists.txt:35 (install):\n","    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n","  This warning is for project developers.  Use -Wno-dev to suppress it.\n","\n","  -- Configuring done (5.0s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/tmpwf1hb6ok/build\n","  *** Building project with Ninja...\n","  Change Dir: '/tmp/tmpwf1hb6ok/build'\n","\n","  Run Build Command(s): /tmp/pip-build-env-21qckkzb/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n","  [1/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-alloc.c\n","  [2/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-backend.c\n","  [3/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/acc.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n","  [4/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-quants.c\n","  [5/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/alibi.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n","  [6/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml.c\n","  [7/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/arange.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n","  [8/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/argsort.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n","  [9/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/clamp.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n","  [10/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/binbcast.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n","  [11/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/concat.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n","  [12/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/diagmask.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n","  [13/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/cpy.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n","  [14/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/convert.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n","  [15/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/dmmv.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n","  [16/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/getrows.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n","  [17/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/im2col.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n","  [18/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/mmq.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n","  [19/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/norm.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n","  [20/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/pad.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n","  [21/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/pool2d.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n","  [22/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/quantize.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n","  [23/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/rope.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n","  [24/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/scale.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n","  [25/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/softmax.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n","  [26/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/sumrows.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n","  [27/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/tsembd.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n","  [28/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/unary.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n","  [29/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/upscale.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n","  [30/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n","  [31/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/fattn.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o\n","  [32/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/sgemm.cpp\n","  [33/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o.d -x cu -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/ggml-cuda/mmvq.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n","  [34/55] : && /tmp/pip-build-env-21qckkzb/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n","  [35/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n","  [36/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/unicode-data.cpp\n","  [37/55] cd /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp && /tmp/pip-build-env-21qckkzb/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  [38/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/build-info.cpp\n","  [39/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/unicode.cpp\n","  [40/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/sampling.cpp\n","  [41/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/console.cpp\n","  [42/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/grammar-parser.cpp\n","  [43/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/common.cpp\n","  [44/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n","  [45/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/train.cpp\n","  [46/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/ngram-cache.cpp\n","  [47/55] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/llava.cpp\n","  [48/55] /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/common/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/llava-cli.cpp\n","  [49/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/llama.cpp\n","  [50/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n","  [51/55] : && /tmp/pip-build-env-21qckkzb/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n","  [52/55] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/vendor/llama.cpp/examples/llava/clip.cpp\n","  [53/55] : && /tmp/pip-build-env-21qckkzb/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n","  [54/55] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmpwf1hb6ok/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl  /usr/lib/x86_64-linux-gnu/librt.a && :\n","  [55/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o  -Wl,-rpath,/tmp/tmpwf1hb6ok/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl  /usr/lib/x86_64-linux-gnu/librt.a && :\n","\n","  *** Installing project into wheel...\n","  -- Install configuration: \"Release\"\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/lib/libggml_shared.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpwf1hb6ok/wheel/platlib/lib/libggml_shared.so\" to \"\"\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/include/ggml.h\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/include/ggml-alloc.h\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/include/ggml-backend.h\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/include/ggml-cuda.h\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/lib/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpwf1hb6ok/wheel/platlib/lib/libllama.so\" to \"\"\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/include/llama.h\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/bin/convert.py\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/bin/convert-lora-to-ggml.py\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpwf1hb6ok/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n","  -- Installing: /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/llama_cpp/libllama.so\" to \"\"\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/lib/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpwf1hb6ok/wheel/platlib/lib/libllava.so\" to \"\"\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/bin/llava-cli\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpwf1hb6ok/wheel/platlib/bin/llava-cli\" to \"\"\n","  -- Installing: /tmp/tmpwf1hb6ok/wheel/platlib/llama_cpp/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmpwf1hb6ok/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n","  -- Installing: /tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/llama_cpp/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-_30bsn1h/llama-cpp-python_1ec156455e4c4e78b9ca0f0d366cd223/llama_cpp/libllava.so\" to \"\"\n","  *** Making wheel...\n","  *** Created llama_cpp_python-0.2.72-cp310-cp310-linux_x86_64.whl...\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.72-cp310-cp310-linux_x86_64.whl size=55353692 sha256=2c8a5e2e283ecc4ce0c8d83d673fd5f79e85c1113e2a37bc6fadc412591d55fa\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jmd2x2dn/wheels/13/46/18/b6a11a030833f8c1c11651a80748bbcb2d567c8530c1b103e5\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.11.0\n","    Uninstalling typing_extensions-4.11.0:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.11.0.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n","      Successfully uninstalled typing_extensions-4.11.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Removing file or directory /usr/local/bin/f2py\n","      Removing file or directory /usr/local/bin/f2py3\n","      Removing file or directory /usr/local/bin/f2py3.10\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n","      Successfully uninstalled numpy-1.25.2\n","  changing mode of /usr/local/bin/f2py to 755\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.4\n","    Uninstalling Jinja2-3.1.4:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2-3.1.4.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n","      Successfully uninstalled Jinja2-3.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.3.0+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.0+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.72 numpy-1.26.4 typing-extensions-4.12.0\n","Collecting outlines==0.0.36\n","  Downloading outlines-0.0.36-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting interegular (from outlines==0.0.36)\n","  Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (3.1.4)\n","Collecting lark (from outlines==0.0.36)\n","  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.26.4)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.2.1)\n","Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (5.6.3)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.7.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.11.4)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.3.0+cu121)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (0.58.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (1.4.2)\n","Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (0.35.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (4.19.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (2.31.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.36) (4.41.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.36) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.36) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.36) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (3.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->outlines==0.0.36)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->outlines==0.0.36) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->outlines==0.0.36)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.36) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.36) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.36) (2023.12.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.36) (0.18.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.36) (0.41.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.36) (2024.2.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (0.23.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->outlines==0.0.36) (4.66.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->outlines==0.0.36) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, outlines\n","Successfully installed interegular-0.3.3 lark-1.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 outlines-0.0.36\n"]}],"source":["# prepare environment\n","!pip install datasets -q\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.72 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install outlines==0.0.36 # use older version to avoid \"cannot convert token to bytes\" error when creating generator (see https://github.com/outlines-dev/outlines/issues/820)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8933,"status":"ok","timestamp":1717003364868,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"_NZQlhG497R0"},"outputs":[],"source":["import torch\n","import outlines\n","\n","import pandas as pd\n","from datasets import Dataset\n","import json\n","import jsonschema\n","import time\n","import os\n","\n","# import stuff from custom LLM_utils module\n","import sys\n","sys.path.append('/content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/modules/')\n","from LLM_utils import output_json_schema_string, format_prompt, extract_json_from_output # needs to be uploaded to colab OR imported from mounted drive OR downloaded from github\n","output_json_schema = json.loads(output_json_schema_string) # will need this for validation when not using guided generation"]},{"cell_type":"markdown","metadata":{"id":"XY3cT-qPzoYq"},"source":["### Model Loading"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4ad8fc9d6caa471ebe66234468903a44","89fde86e21924f188a0ef82e18c67910","5f1847a1815b4b23b37a7eeaf5feafeb","767dbf51c3414bf3993ae26152a8676f","7cb6b062b49d4195a92c74668bc8623c","ab0a3fac07fb4809a92c8b6fea892f3b","2e624b39851b472db4be90a8610a1e92","af400bf729aa4c48894681896f69d45b","047cd3bef411436ca400b93d4a8fafc9","4c279b9be46c45fe94db1ed4e6ddfdfd","62b8ce395fc44632a9ddbdbe2e2abc3c"]},"executionInfo":{"elapsed":231878,"status":"ok","timestamp":1717003596743,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"IFw00KXv-ywK","outputId":"c3a36869-d8f7-4998-88a8-10f202cb55d1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ad8fc9d6caa471ebe66234468903a44","version_major":2,"version_minor":0},"text/plain":["(…)8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--JanJacobsen--llama3_8b_instruct_ft_v4_q8_0/snapshots/4b72fbb22eba29d05d31d002086e9f5696e90c3d/./llama3_8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = llama3_8b_instruct_ft_v4_q8_0\n","llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 7\n","llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n","llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n","llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = llama3_8b_instruct_ft_v4_q8_0\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: PAD token        = 128255 '<|reserved_special_token_250|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_tensors: ggml ctx size =    0.30 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =   532.31 MiB\n","llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB\n",".........................................................................................\n","llama_new_context_with_model: n_ctx      = 4096\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n","llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128255', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'llama3_8b_instruct_ft_v4_q8_0', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n","Available chat formats from metadata: chat_template.default\n","Guessed chat format: llama-3\n"]}],"source":["# loading model\n","from llama_cpp import Llama\n","\n","############################################################################################################\n","# TheBloke repo: mistral quants\n","#repo_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n","#filename = \"mistral-7b-instruct-v0.2.Q8_0.gguf\"\n","\n","# my repo (finetunes)\n","repo_id = \"JanJacobsen/llama3_8b_instruct_ft_v4_q8_0\"\n","filename = \"llama3_8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf\"\n","\n","# my repo (llama3 base f16 and quants)\n","#repo_id = \"JanJacobsen/llama3_8b_instruct_q8_0\"\n","#filename = \"llama3_8b_instruct_q8_0-unsloth.Q8_0.gguf\"\n","\n","\n","############################################################################################################\n","\n","llm = Llama.from_pretrained(\n","    repo_id=repo_id,\n","    filename=filename,\n","    device=\"cuda\",\n","    n_gpu_layers=-1, # offload entire model to gpu\n","    n_ctx = 4*1024\n",")\n","llm.verbose = False\n","# llama_cpp model to outlines model\n","model = outlines.models.LlamaCpp(llm)\n"]},{"cell_type":"markdown","metadata":{"id":"_ZDFI-y-0BNi"},"source":["### Testing generation before main inference loop"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["122cfec467474fefb08127ed93fcaa48","c670430ab28042c1bb427824b7c079f2","feaf6b4d940744c3b5c4384ea2e16c0e","d37451fdaeaa4b2fb88e562eb96c749b","6b8748f4e0d64a4fae3b59d56d441ceb","f17f705e6586414d886c3ac37995b859","f2b012730faf4a7c9d71e6a6d7936a3d","859d3abac4f546019e5257fbc81b1703","27786aab484549839111ab7e58c9c969","fdb95683a629419eb2f4caf591f1bc32","ea04c0d4cda24ec88921fb1b7e809359"]},"executionInfo":{"elapsed":7374,"status":"ok","timestamp":1717009348398,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"CZOoLtTvhdYX","outputId":"d08da923-409f-46b1-b0c4-d6bbae430ace"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"122cfec467474fefb08127ed93fcaa48","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["if False:\n","    # load data for testing\n","    data_file_path = f\"{data_path}/INF_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\"\n","    data = pd.read_csv(data_file_path, sep=\";\")\n","\n","    # select examples\n","    data = data[data[\"video_id\"].isin([\"jXGooRDXRgs\", \"DpzJygBwnMg\"])]\n","\n","    # hf dataset\n","    data = Dataset.from_pandas(data)\n","\n","    # add prompts\n","    data = data.map(format_prompt, fn_kwargs={\"prompt_format\": \"llama3\",\n","                                                        \"include_answer_tease\": True,\n","                                                        \"include_label\": False,\n","                                                        \"include_bos\": True,\n","                                                        \"include_eos\": False})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPHQEPm-gECt"},"outputs":[],"source":["# run inference on data (non-guided)\n","if False: # test 10 generations (model directly)\n","    temp = 0.01\n","    for ex in data:\n","        print(f\"Starting inference for video_id {ex['video_id']}, chunk number {ex['chunk_number']} ...\")\n","        result = llm.create_completion(ex['prompt'], temperature=temp, top_k=50, max_tokens=1000, repeat_penalty=1)\n","        print(\"Result:\\n\\n\")\n","        print(result['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bcz3h_VNJM_e"},"outputs":[],"source":["if False:\n","    # check an example prompt\n","    prompt = data[20]['prompt']\n","    print(prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dd5U-iL_rsv"},"outputs":[],"source":["if False: # test 10 generations (model directly)\n","    prompt = data[22]['prompt']\n","    temp = 0.01\n","    for i in range(10):\n","        result = llm.create_completion(prompt, temperature=temp, top_k=50, max_tokens=1000)\n","        print(result['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TTD5yyud_Cy"},"outputs":[],"source":["if False: # test generator creation\n","    # create outlines generator with custom json schema constraints\n","    generator = outlines.generate.json(model, output_json_schema_string, whitespace_pattern=r\"[ \\n\\t]?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1kIMOxJNfbB"},"outputs":[],"source":["if False:\n","    # test example prompt\n","    result = generator(prompt, temperature=0.01, top_k=50, max_tokens=1000)\n","    result = json.dumps(result)\n","    print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhFan45mkB0R"},"outputs":[],"source":["if False: # test generator in loop (do we have to recreate each time?)\n","    prompt = data[22]['prompt']\n","    test_recreation = False\n","    for i in range(10):\n","        if test_recreation:\n","            generator = outlines.generate.json(model, output_json_schema_string, whitespace_pattern=r\"[ \\n\\t]?\")\n","        result = generator(prompt, temperature=1.5, top_k=50, max_tokens=1000)\n","        result = json.dumps(result)\n","        print(result)"]},{"cell_type":"markdown","metadata":{"id":"67pRJRhD0KPf"},"source":["### Prepare Data & Inference"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160,"referenced_widgets":["5b90cbf4005e4772a0919fb8ef646396","9e2eee0cacb74a6098dc7c2e7c46fb0b","6bda70f8b8274abfa5f5b3a2af1f6ccc","bf1ff710157c4990bd6d3687187781ea","30986d0d2b1a44559e7b98dd8e04ce53","6142eb8262c847e6b3f331387b409794","204eed6fa243409181334a3985920cb9","5a30de45790d4aad9b1d5fd311fbb6f3","e5af385619e047e8bb1c9449e4ec09a7","b2eafc028aa246698536d04ad5f56e3b","b0258df1777c4e448325b7812f0b5ee0"]},"executionInfo":{"elapsed":10721,"status":"ok","timestamp":1717006504702,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"laMVh5TPJM_e","outputId":"42ef1c50-7def-4e95-b047-ff35e286ead1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded dataset with 79626 samples from /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/data/INF_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\n","Loaded progress file with 79626 processed examples from /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/inf_llama3_ft_v4_q8_0_llamacpp_guided.csv\n","Examples yet to process in dataset: 2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b90cbf4005e4772a0919fb8ef646396","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Created outlines generator in 3.89 seconds.\n","Preparation complete for run: inf_llama3_ft_v4_q8_0_llamacpp_guided\n"]}],"source":["##########################################################################################################################################################\n","# ------------------- run name (adjust for every new run!) ------------------ #\n","run_type = \"inf\" # \"val\" (= test) or \"inf\"\n","run_model = \"llama3_ft_v4\" # mistral/llama3/... + _ft_version if finetuned\n","run_model_quant = \"q8_0\"\n","run_framework = \"llamacpp\" # llamacpp/vllm/transformers/...\n","additional_suffix = \"_guided\"\n","# --------------------------------------------------------------------------- #\n","run_name = f\"{run_type}_{run_model}_{run_model_quant}_{run_framework}{additional_suffix}\"\n","data_file_path = f\"{data_path}/{run_type.upper()}_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\"\n","progress_file_path = f\"{output_path}/{run_name}.csv\" # file does not need to exist yet\n","\n","# prompt formatting arguments (for our custom function)\n","prompt_format_kwargs={\n","    \"prompt_format\": \"llama3\",\n","    \"include_bos\": True, # does llamacpp add it or not?\n","    \"include_answer_tease\": True,\n","    \"include_label\": False, # never for inference, only used for finetuning\n","    \"include_eos\": False}\n","\n","# model parameters (might be model-specific)\n","temperature = 0.01 # very low / zero for our task\n","top_k = 50 # relatively high value to avoid problems with guided generation (i.e. what if token required by schema is not in top k?)\n","seed = 42 # relevant for sampling (CAUTION: can't be used with llamacpp & outlines==0.0.36)\n","max_tokens = 1250 # might have to be set for llamacpp to override default? unsure...\n","\n","# how are we calling the model? with guided generation (outlines) active, or directly via llama_cpp?\n","guided_generation = True\n","\n","# outlines-specific\n","apply_reset_generator_fix = False # in some configurations with outlines & llamacpp the outlines.generator.json() needs to be reset before each new call\n","generator_whitespace_pattern = r\"[ \\n\\t]?\" # should we allow whitespaces/newlines/etc. being generated within the json structure? probably yes, but limit to zero or one at a time\n","def get_generator(whitespace_pattern): # to make sure we call it with the same parameters everywhere (e.g. whitespace pattern)\n","    return outlines.generate.json(model, output_json_schema_string, whitespace_pattern=whitespace_pattern)\n","\n","# other run parameters\n","skip_previous_errors = True # skip examples with True in 'error?' column?\n","replace_previous_errors = True # only relevant if skip_previous_errors is False: should previous errors be replaced with new outputs (if available) in progress file?\n","\n","save_interval = 250 # update progress file every n examples (note: should not be set too low because saving to and loading from drive might not actually update files instantly? note sure...)\n","print_progress_interval = 50 # print progress every n examples\n","print_errors = True # useful to catch issues, but disable for final inference runs with confirmed problem-free models/code (to avoid crashes due to huge cell output)\n","\n","##########################################################################################################################################################\n","\n","# check if files exist\n","import os\n","if not os.path.exists(data_file_path):\n","    raise ValueError(f\"Data file {data_file_path} does not exist!\")\n","if not os.path.exists(progress_file_path):\n","    print(f\"Progress file {progress_file_path} does not exist, creating new one.\")\n","    pd.DataFrame(columns=[\"video_id\", \"chunk_number\", \"output\", \"error?\"]).to_csv(progress_file_path, sep=\";\", index=False)\n","\n","# load data\n","data = pd.read_csv(data_file_path, sep=\";\")\n","print(f\"Loaded dataset with {len(data)} samples from {data_file_path}\")\n","\n","# load progress data\n","progress = pd.read_csv(progress_file_path, sep=\";\")\n","print(f\"Loaded progress file with {len(progress)} processed examples from {progress_file_path}\")\n","\n","# merge progress data with main data (-> adds 'output' and 'error?' columns from progress df)\n","data = pd.merge(data, progress, on=[\"video_id\", \"chunk_number\"], how=\"left\")\n","\n","# filter out already processed examples (and optionally previous examples which caused errors)\n","data = data[data['output'].isnull()]\n","if skip_previous_errors:\n","    data = data[data['error?'] != True] # None and False should both stay in, only filter out True\n","print(f\"Examples yet to process in dataset: {len(data)}\")\n","\n","# convert to hf dataset\n","data = Dataset.from_pandas(data)\n","\n","# add prompts\n","data = data.map(format_prompt, fn_kwargs=prompt_format_kwargs)\n","\n","# create outlines generator (and check time)\n","if guided_generation:\n","    start_time = time.time()\n","    generator = get_generator(generator_whitespace_pattern)\n","    print(f\"Created outlines generator in {round(time.time() - start_time, 2)} seconds.\")\n","\n","print(f\"Preparation complete for run: {run_name}\")"]},{"cell_type":"markdown","metadata":{"id":"CF9dKXx00TFD"},"source":["### Inference Loop"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96348,"status":"ok","timestamp":1717006620388,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"BgdQttaOCHLM","outputId":"f7945903-a440-4ec5-e43c-85efdc6a00ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["*** STARTING INFERENCE FOR 2 EXAMPLES (run name: inf_llama3_ft_v4_q8_0_llamacpp_guided) ***\n","------------------------------------------------------------\n","  - Saved 2 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/inf_llama3_ft_v4_q8_0_llamacpp_guided.csv\n","------------------------------------------------------------\n","Finished processing 2 examples in 0 h, 1 min, 36.05 sec\n"]}],"source":["# <- STARTS ITERATION\n","\n","# helper function to print elapsed time\n","def get_elapsed_time_str(start_time):\n","    hours, rem = divmod(time.time() - start_time, 3600)\n","    minutes, seconds = divmod(rem, 60)\n","    return f\"{int(hours)} h, {int(minutes)} min, {round(seconds, 2)} sec\"\n","\n","# helper function for saving progress\n","def update_progress_file(new_results, progress_file_path, replace_previous_errors):\n","#    1. convert to pandas df\n","    new_results_df = pd.DataFrame(new_results, columns=[\"video_id\", \"chunk_number\", \"output\", \"error?\"])\n","    # 2. load already processed data and append new results (if it exists)\n","    if os.path.exists(progress_file_path):\n","        progress_df = pd.read_csv(progress_file_path, sep=\";\")\n","        progress_df = pd.concat([progress_df, new_results_df], ignore_index=True)\n","        # drop duplicate error rows (duplicates here should only happen for previous error rows which we included and got errors again)\n","        progress_df = progress_df.drop_duplicates(subset=[\"video_id\", \"chunk_number\", \"error?\"], keep=\"last\")\n","    else:\n","        progress_df = new_results_df\n","\n","    #3. optionally, if there are new results for examples which previously caused errors, replace the error rows (otherwise: keep both the error row and the new result row in the file)\n","    if replace_previous_errors:\n","        # note: since we used pd.concat above we know that new results are appended at the end of the df, which allows us to use keep=\"last\" here to keep the new result row (without sorting first)\n","        progress_df = progress_df.drop_duplicates(subset=[\"video_id\", \"chunk_number\"], keep=\"last\")\n","    # 3. save\n","    progress_df.to_csv(progress_file_path, sep=\";\", index=False)\n","    print(f\"  - Saved {len(new_results)} new results to {progress_file_path}\")\n","\n","print(f\"*** STARTING INFERENCE FOR {len(data)} EXAMPLES (run name: {run_name}) ***\\n{'-'*60}\")\n","\n","# inference loop\n","new_results = []\n","start_time = time.time()\n","for i, example in enumerate(data):\n","\n","    video_id = example['video_id']\n","    uploader_id = example['uploader_id']\n","    chunk_number = example['chunk_number']\n","\n","    # model call\n","    try:\n","        if guided_generation:\n","            if apply_reset_generator_fix:\n","                generator = get_generator(generator_whitespace_pattern)\n","            # outlines model call -> dump json to string\n","            result = generator(example['prompt'], temperature=temperature, top_k=top_k, max_tokens=max_tokens)\n","            result = json.dumps(result)\n","\n","        else:\n","            # llamacpp model call -> try to extract and validate the json (the model output might contain additional text or invalid json)\n","            result = llm(example['prompt'], temperature=temperature, top_k=top_k, max_tokens=max_tokens)['choices'][0]['text']\n","            result = extract_json_from_output(result) # returns string or raises error\n","            # validate\n","            result = json.loads(result)\n","            jsonschema.validate(instance=result, schema=output_json_schema) # raises error if instance doesn't match schema\n","            result = json.dumps(result) # we convert back to string to ensure clean structure (no newlines etc.)\n","        # no error: append result\n","        new_results.append((video_id, chunk_number, result, False))\n","\n","    except Exception as e:\n","\n","        if print_errors:\n","            print(f\"Error at iteration {i} (video_id: {video_id}, chunk_number: {chunk_number}) - {e} \")\n","\n","        new_results.append((video_id, chunk_number, None, True))\n","\n","    # save progress\n","    if (i+1) % save_interval == 0:\n","        update_progress_file(new_results, progress_file_path, replace_previous_errors)\n","        new_results = []\n","\n","    # print progress\n","    if (i+1) % print_progress_interval == 0:\n","        print(f\"Processed {i+1} total examples in {get_elapsed_time_str(start_time)}\")\n","\n","# save remaining examples\n","update_progress_file(new_results, progress_file_path, replace_previous_errors)\n","\n","print(f\"{'-'*60}\\nFinished processing {len(data)} examples in {get_elapsed_time_str(start_time)}\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb","timestamp":1710611387839}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"047cd3bef411436ca400b93d4a8fafc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"122cfec467474fefb08127ed93fcaa48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c670430ab28042c1bb427824b7c079f2","IPY_MODEL_feaf6b4d940744c3b5c4384ea2e16c0e","IPY_MODEL_d37451fdaeaa4b2fb88e562eb96c749b"],"layout":"IPY_MODEL_6b8748f4e0d64a4fae3b59d56d441ceb"}},"204eed6fa243409181334a3985920cb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27786aab484549839111ab7e58c9c969":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e624b39851b472db4be90a8610a1e92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30986d0d2b1a44559e7b98dd8e04ce53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ad8fc9d6caa471ebe66234468903a44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89fde86e21924f188a0ef82e18c67910","IPY_MODEL_5f1847a1815b4b23b37a7eeaf5feafeb","IPY_MODEL_767dbf51c3414bf3993ae26152a8676f"],"layout":"IPY_MODEL_7cb6b062b49d4195a92c74668bc8623c"}},"4c279b9be46c45fe94db1ed4e6ddfdfd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a30de45790d4aad9b1d5fd311fbb6f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b90cbf4005e4772a0919fb8ef646396":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e2eee0cacb74a6098dc7c2e7c46fb0b","IPY_MODEL_6bda70f8b8274abfa5f5b3a2af1f6ccc","IPY_MODEL_bf1ff710157c4990bd6d3687187781ea"],"layout":"IPY_MODEL_30986d0d2b1a44559e7b98dd8e04ce53"}},"5f1847a1815b4b23b37a7eeaf5feafeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af400bf729aa4c48894681896f69d45b","max":8540770656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_047cd3bef411436ca400b93d4a8fafc9","value":8540770656}},"6142eb8262c847e6b3f331387b409794":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62b8ce395fc44632a9ddbdbe2e2abc3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b8748f4e0d64a4fae3b59d56d441ceb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bda70f8b8274abfa5f5b3a2af1f6ccc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a30de45790d4aad9b1d5fd311fbb6f3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5af385619e047e8bb1c9449e4ec09a7","value":2}},"767dbf51c3414bf3993ae26152a8676f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c279b9be46c45fe94db1ed4e6ddfdfd","placeholder":"​","style":"IPY_MODEL_62b8ce395fc44632a9ddbdbe2e2abc3c","value":" 8.54G/8.54G [03:06&lt;00:00, 40.1MB/s]"}},"7cb6b062b49d4195a92c74668bc8623c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"859d3abac4f546019e5257fbc81b1703":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89fde86e21924f188a0ef82e18c67910":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab0a3fac07fb4809a92c8b6fea892f3b","placeholder":"​","style":"IPY_MODEL_2e624b39851b472db4be90a8610a1e92","value":"(…)8b_instruct_ft_v4_q8_0-unsloth.Q8_0.gguf: 100%"}},"9e2eee0cacb74a6098dc7c2e7c46fb0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6142eb8262c847e6b3f331387b409794","placeholder":"​","style":"IPY_MODEL_204eed6fa243409181334a3985920cb9","value":"Map: 100%"}},"ab0a3fac07fb4809a92c8b6fea892f3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af400bf729aa4c48894681896f69d45b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0258df1777c4e448325b7812f0b5ee0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2eafc028aa246698536d04ad5f56e3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf1ff710157c4990bd6d3687187781ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2eafc028aa246698536d04ad5f56e3b","placeholder":"​","style":"IPY_MODEL_b0258df1777c4e448325b7812f0b5ee0","value":" 2/2 [00:00&lt;00:00, 32.24 examples/s]"}},"c670430ab28042c1bb427824b7c079f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f17f705e6586414d886c3ac37995b859","placeholder":"​","style":"IPY_MODEL_f2b012730faf4a7c9d71e6a6d7936a3d","value":"Map: 100%"}},"d37451fdaeaa4b2fb88e562eb96c749b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdb95683a629419eb2f4caf591f1bc32","placeholder":"​","style":"IPY_MODEL_ea04c0d4cda24ec88921fb1b7e809359","value":" 5/5 [00:00&lt;00:00, 127.61 examples/s]"}},"e5af385619e047e8bb1c9449e4ec09a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea04c0d4cda24ec88921fb1b7e809359":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f17f705e6586414d886c3ac37995b859":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2b012730faf4a7c9d71e6a6d7936a3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdb95683a629419eb2f4caf591f1bc32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feaf6b4d940744c3b5c4384ea2e16c0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_859d3abac4f546019e5257fbc81b1703","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27786aab484549839111ab7e58c9c969","value":5}}}}},"nbformat":4,"nbformat_minor":0}
