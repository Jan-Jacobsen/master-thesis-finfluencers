{"cells":[{"cell_type":"markdown","metadata":{"id":"IiU93J623t6H"},"source":["This notebook is intended to run on google colab."]},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"StBCcp4sziw9"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19724,"status":"ok","timestamp":1715641683900,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"_0o0h9FVUAb1","outputId":"a32c3bff-6e4c-495b-c6d8-09446028b4c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive and define paths (do this in first cell because colab makes us click to confirm in a context window...)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","dir_path = \"/content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks\"\n","data_path = f\"{dir_path}/data\"\n","output_path = f\"{dir_path}/outputs\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456700,"status":"ok","timestamp":1715642140598,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"SX1w4ZSm-LIp","outputId":"30eaca61-41c9-47bc-ddc7-4ef094077a2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","Collecting llama-cpp-python==0.2.72\n","  Downloading llama_cpp_python-0.2.72.tar.gz (49.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 MB\u001b[0m \u001b[31m209.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Running command pip subprocess to install build dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting scikit-build-core[pyproject]>=0.9.2\n","    Downloading scikit_build_core-0.9.3-py3-none-any.whl (151 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.6/151.6 kB 4.5 MB/s eta 0:00:00\n","  Collecting exceptiongroup>=1.0 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n","  Collecting packaging>=21.3 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading packaging-24.0-py3-none-any.whl (53 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 8.2 MB/s eta 0:00:00\n","  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","  Collecting tomli>=1.2.2 (from scikit-build-core[pyproject]>=0.9.2)\n","    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n","  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core\n","  Successfully installed exceptiongroup-1.2.1 packaging-24.0 pathspec-0.12.1 scikit-build-core-0.9.3 tomli-2.0.1\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Getting requirements to build wheel\n","  Could not determine CMake version via --version, got '' 'Traceback (most recent call last):\\n  File \"/usr/local/bin/cmake\", line 5, in <module>\\n    from cmake import cmake\\nModuleNotFoundError: No module named \\'cmake\\'\\n'\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Running command pip subprocess to install backend dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting cmake>=3.21\n","    Downloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/26.7 MB 54.2 MB/s eta 0:00:00\n","  Collecting ninja>=1.5\n","    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 32.2 MB/s eta 0:00:00\n","  Installing collected packages: ninja, cmake\n","    Creating /tmp/pip-build-env-1w441b6s/normal/local/bin\n","    changing mode of /tmp/pip-build-env-1w441b6s/normal/local/bin/ninja to 755\n","    changing mode of /tmp/pip-build-env-1w441b6s/normal/local/bin/cmake to 755\n","    changing mode of /tmp/pip-build-env-1w441b6s/normal/local/bin/cpack to 755\n","    changing mode of /tmp/pip-build-env-1w441b6s/normal/local/bin/ctest to 755\n","  Successfully installed cmake-3.29.3 ninja-1.11.1.1\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Preparing metadata (pyproject.toml)\n","  *** scikit-build-core 0.9.3 using CMake 3.29.3 (metadata_wheel)\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.72)\n","  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n","Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.72)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m252.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.72)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m210.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python==0.2.72)\n","  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m325.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.72)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Running command Building wheel for llama-cpp-python (pyproject.toml)\n","  *** scikit-build-core 0.9.3 using CMake 3.29.3 (wheel)\n","  *** Configuring CMake...\n","  loading initial cache file /tmp/tmputq3_96p/build/CMakeInit.txt\n","  -- The C compiler identification is GNU 11.4.0\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","  -- Found Threads: TRUE\n","  CMake Warning at vendor/llama.cpp/CMakeLists.txt:389 (message):\n","    LLAMA_CUBLAS is deprecated and will be removed in the future.\n","\n","    Use LLAMA_CUDA instead\n","\n","\n","  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n","  -- CUDA found\n","  -- The CUDA compiler identification is NVIDIA 12.2.140\n","  -- Detecting CUDA compiler ABI info\n","  -- Detecting CUDA compiler ABI info - done\n","  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","  -- Detecting CUDA compile features\n","  -- Detecting CUDA compile features - done\n","  -- Using CUDA architectures: 52;61;70\n","  -- CUDA host compiler is GNU 11.4.0\n","\n","  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n","  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n","  -- x86 detected\n","  CMake Warning (dev) at CMakeLists.txt:26 (install):\n","    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n","  This warning is for project developers.  Use -Wno-dev to suppress it.\n","\n","  CMake Warning (dev) at CMakeLists.txt:35 (install):\n","    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n","  This warning is for project developers.  Use -Wno-dev to suppress it.\n","\n","  -- Configuring done (4.6s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/tmputq3_96p/build\n","  *** Building project with Ninja...\n","  Change Dir: '/tmp/tmputq3_96p/build'\n","\n","  Run Build Command(s): /tmp/pip-build-env-1w441b6s/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n","  [1/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-alloc.c\n","  [2/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-backend.c\n","  [3/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/acc.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n","  [4/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-quants.c\n","  [5/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/alibi.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n","  [6/55] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml.c\n","  [7/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/arange.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n","  [8/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/argsort.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n","  [9/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/clamp.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n","  [10/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/binbcast.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n","  [11/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/concat.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n","  [12/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/diagmask.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n","  [13/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/cpy.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n","  [14/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/convert.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n","  [15/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/dmmv.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n","  [16/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/getrows.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n","  [17/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/im2col.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n","  [18/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/mmq.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n","  [19/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/norm.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n","  [20/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/pad.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n","  [21/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/pool2d.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n","  [22/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/quantize.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n","  [23/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/rope.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n","  [24/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/scale.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n","  [25/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/softmax.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n","  [26/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/sumrows.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n","  [27/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/tsembd.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n","  [28/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/unary.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n","  [29/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/upscale.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n","  [30/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n","  [31/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/fattn.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o\n","  [32/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/sgemm.cpp\n","  [33/55] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o.d -x cu -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/ggml-cuda/mmvq.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n","  [34/55] : && /tmp/pip-build-env-1w441b6s/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n","  [35/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n","  [36/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/unicode.cpp\n","  [37/55] cd /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp && /tmp/pip-build-env-1w441b6s/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  [38/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/build-info.cpp\n","  [39/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/unicode-data.cpp\n","  [40/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/sampling.cpp\n","  [41/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/console.cpp\n","  [42/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/grammar-parser.cpp\n","  [43/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/common.cpp\n","  [44/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/train.cpp\n","  [45/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n","  [46/55] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/llava.cpp\n","  [47/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/ngram-cache.cpp\n","  [48/55] /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/common/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/llava-cli.cpp\n","  [49/55] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DGGML_USE_LLAMAFILE -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/llama.cpp\n","  [50/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n","  [51/55] : && /tmp/pip-build-env-1w441b6s/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n","  [52/55] /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/vendor/llama.cpp/examples/llava/clip.cpp\n","  [53/55] : && /tmp/pip-build-env-1w441b6s/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n","  [54/55] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmputq3_96p/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl  /usr/lib/x86_64-linux-gnu/librt.a && :\n","  [55/55] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o  -Wl,-rpath,/tmp/tmputq3_96p/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl  /usr/lib/x86_64-linux-gnu/librt.a && :\n","\n","  *** Installing project into wheel...\n","  -- Install configuration: \"Release\"\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/lib/libggml_shared.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmputq3_96p/wheel/platlib/lib/libggml_shared.so\" to \"\"\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/include/ggml.h\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/include/ggml-alloc.h\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/include/ggml-backend.h\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/include/ggml-cuda.h\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/lib/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmputq3_96p/wheel/platlib/lib/libllama.so\" to \"\"\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/include/llama.h\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/bin/convert.py\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/bin/convert-lora-to-ggml.py\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmputq3_96p/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n","  -- Installing: /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/llama_cpp/libllama.so\" to \"\"\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/lib/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmputq3_96p/wheel/platlib/lib/libllava.so\" to \"\"\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/bin/llava-cli\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmputq3_96p/wheel/platlib/bin/llava-cli\" to \"\"\n","  -- Installing: /tmp/tmputq3_96p/wheel/platlib/llama_cpp/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/tmputq3_96p/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n","  -- Installing: /tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/llama_cpp/libllava.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-exktj7k5/llama-cpp-python_02160eea2d51414e9c60d96161e187e2/llama_cpp/libllava.so\" to \"\"\n","  *** Making wheel...\n","  *** Created llama_cpp_python-0.2.72-cp310-cp310-linux_x86_64.whl...\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.72-cp310-cp310-linux_x86_64.whl size=55352816 sha256=19bdb119649ba26e24fe6038f55d8013fd99369fdf4177d2e883f7418944bb06\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7i3erx5y/wheels/13/46/18/b6a11a030833f8c1c11651a80748bbcb2d567c8530c1b103e5\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.11.0\n","    Uninstalling typing_extensions-4.11.0:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.11.0.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n","      Successfully uninstalled typing_extensions-4.11.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Removing file or directory /usr/local/bin/f2py\n","      Removing file or directory /usr/local/bin/f2py3\n","      Removing file or directory /usr/local/bin/f2py3.10\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n","      Successfully uninstalled numpy-1.25.2\n","  changing mode of /usr/local/bin/f2py to 755\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.4\n","    Uninstalling Jinja2-3.1.4:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2-3.1.4.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n","      Successfully uninstalled Jinja2-3.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.72 numpy-1.26.4 typing-extensions-4.11.0\n","Collecting outlines==0.0.41\n","  Downloading outlines-0.0.41-py3-none-any.whl (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting interegular (from outlines==0.0.41)\n","  Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (3.1.4)\n","Collecting lark (from outlines==0.0.41)\n","  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (1.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (1.26.4)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (2.2.1)\n","Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (5.6.3)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (2.7.1)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (0.58.1)\n","Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (0.35.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (4.19.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.41) (2.31.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.41) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.41) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines==0.0.41) (4.11.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.41) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.41) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.41) (2023.12.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.41) (0.18.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.41) (0.41.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.41) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.41) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.41) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outlines==0.0.41) (2024.2.2)\n","Installing collected packages: lark, interegular, outlines\n","Successfully installed interegular-0.3.3 lark-1.1.9 outlines-0.0.41\n"]}],"source":["# prepare environment\n","!pip install datasets -q\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.72 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install outlines==0.0.41 # use older version to avoid \"cannot convert token to bytes\" error when creating generator (see https://github.com/outlines-dev/outlines/issues/820)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9332,"status":"ok","timestamp":1715642149925,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"_NZQlhG497R0"},"outputs":[],"source":["import torch\n","import outlines\n","\n","import pandas as pd\n","from datasets import Dataset\n","import json\n","import jsonschema\n","import time\n","import os\n","\n","# import stuff from custom LLM_utils module\n","import sys\n","sys.path.append('/content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/modules/')\n","from LLM_utils import output_json_schema_string, format_prompt, extract_json_from_output # needs to be uploaded to colab OR imported from mounted drive OR downloaded from github\n","output_json_schema = json.loads(output_json_schema_string) # will need this for validation when not using guided generation"]},{"cell_type":"markdown","source":["### Model Loading"],"metadata":{"id":"XY3cT-qPzoYq"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["86b14b30b691460d9e54de16461d0899","1422029e84144042a88bc8993b016ef8","ce1bc4ab6bb04cdd92c635d92367be33","41a83ecdcba84fcab42f00572662d0e8","19eb5713cdbb4a1aad6ffd65271d89e1","65cdbc3d98b34294a5bee312f2e7d97a","cc4062e3a4f841aebddfda66dc0ac88f","10f0f990520c45219926ce5085eebd8c","883e1696bab6450db8d0e2bdf59f7142","576199fbc35d412e94a35eac53086a5e","3de3eea4ce0040e9bee94d14d113dc95"]},"executionInfo":{"elapsed":86073,"status":"ok","timestamp":1715642236585,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"IFw00KXv-ywK","outputId":"ac0e802f-0c3e-43cf-d372-36bd4f2b65bd"},"outputs":[{"output_type":"display_data","data":{"text/plain":["mistral-7b-instruct-v0.2.Q8_0.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b14b30b691460d9e54de16461d0899"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/./mistral-7b-instruct-v0.2.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 7\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n","llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 32768\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 32768\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.30 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =   132.81 MiB\n","llm_load_tensors:      CUDA0 buffer size =  7205.83 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 4096\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 1000000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n","llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n","Available chat formats from metadata: chat_template.default\n","Guessed chat format: mistral-instruct\n"]}],"source":["# loading model\n","from llama_cpp import Llama\n","\n","############################################################################################################\n","# TheBloke repo: mistral quants\n","repo_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n","filename = \"mistral-7b-instruct-v0.2.Q8_0.gguf\"\n","\n","# bartkowski repo: llama3 quants\n","#repo_id = \"bartowski/Meta-Llama-3-8B-Instruct-GGUF\"\n","#filename = \"Meta-Llama-3-8B-Instruct-Q8_0.gguf\"\n","\n","# my repo: llama3 ft v3\n","#repo_id = \"JanJacobsen/llama3_8b_instruct_ft_v3_q8_0\"\n","#filename = \"llama3_8b_instruct_ft_v3_q8_0-unsloth.Q8_0.gguf\"\n","\n","# my repo: llama3\n","#repo_id = \"JanJacobsen/llama3_8b_instruct_q8_0\"\n","#filename = \"llama3_8b_instruct_q8_0-unsloth.Q8_0.gguf\"\n","\n","# my repo: mistral\n","\n","\n","\n","############################################################################################################\n","\n","llm = Llama.from_pretrained(\n","    repo_id=repo_id,\n","    filename=filename,\n","    device=\"cuda\",\n","    n_gpu_layers=-1, # offload entire model to gpu\n","    n_ctx = 4*1024\n",")\n","llm.verbose = False\n","# llama_cpp model to outlines model\n","model = outlines.models.LlamaCpp(llm)\n"]},{"cell_type":"markdown","source":["### Testing generation before main inference loop"],"metadata":{"id":"_ZDFI-y-0BNi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["aaa65fd117fb4995a1d58c4adf1318ce","60e3b81a8dea432785f51fed722b072b","fb2de11ea0a74a6a9500aac2c3319004","7c48258d73294f1c8d67ec9ea8f41864","6c87b4f2eca44387a31116fe5955d984","a7507fc2c42a48d19ef12309096c5457","60ec28a341164ff6b28393577c0e4804","7b6242123e124f43baca92d190f19d67","ac93e51efe4949c38a2810d79fbf1780","63cc5ae792084162a42ff90c31dc637c","e945eaf96d9c4ef8a5830b4ec42200eb"]},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1715625236967,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"CZOoLtTvhdYX","outputId":"6a0f923d-1839-4be1-a998-beedac253606"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/150 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa65fd117fb4995a1d58c4adf1318ce"}},"metadata":{}}],"source":["if False:\n","    # load data for testing\n","    data_file_path = f\"{data_path}/VAL_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\"\n","\n","    data = Dataset.from_pandas(pd.read_csv(data_file_path, sep=\";\"))\n","\n","    # add prompts\n","    data = data.map(format_prompt, fn_kwargs={\"prompt_format\": \"mistral\",\n","                                                        \"include_answer_tease\": True,\n","                                                        \"include_label\": False,\n","                                                        \"include_bos\": True,\n","                                                        \"include_eos\": False})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bcz3h_VNJM_e"},"outputs":[],"source":["if False:\n","    # check an example prompt\n","    prompt = data[20]['prompt']\n","    print(prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dd5U-iL_rsv"},"outputs":[],"source":["if False: # test 10 generations (model directly)\n","    prompt = data[22]['prompt']\n","    temp = 0.01\n","    for i in range(10):\n","        result = llm.create_completion(prompt, temperature=temp, top_k=50, max_tokens=1000)\n","        print(result['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TTD5yyud_Cy"},"outputs":[],"source":["if False: # test generator creation\n","    # create outlines generator with custom json schema constraints\n","    generator = outlines.generate.json(model, output_json_schema_string, whitespace_pattern=r\"[ \\n\\t]?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1kIMOxJNfbB"},"outputs":[],"source":["if False:\n","    # test example prompt\n","    result = generator(prompt, temperature=0.01, top_k=50, max_tokens=1000)\n","    result = json.dumps(result)\n","    print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhFan45mkB0R"},"outputs":[],"source":["if False: # test generator in loop (do we have to recreate each time?)\n","    prompt = data[22]['prompt']\n","    test_recreation = False\n","    for i in range(10):\n","        if test_recreation:\n","            generator = outlines.generate.json(model, output_json_schema_string, whitespace_pattern=r\"[ \\n\\t]?\")\n","        result = generator(prompt, temperature=1.5, top_k=50, max_tokens=1000)\n","        result = json.dumps(result)\n","        print(result)"]},{"cell_type":"markdown","source":["### Prepare Data & Inference"],"metadata":{"id":"67pRJRhD0KPf"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616,"referenced_widgets":["27ab4a1aa02949dbadaafc12cb46ae1c","c040ebdf2fc0474e98f5b95566f3e523","6b0eb6d188ae4a348cc6edf298225ce9","b85205fce08f415cb00a287ed0b79739","be4a3d73713940d190ec70aa7f34aab8","7e1dfd27c49a4a859f20fd231db5b6e3","a97afdae59654c01bfda4336418d3496","a48316707ddb43628fb0d5ea41c7cc04","d226f04515cb4e50849feaf9f7737cf9","af37c2645946436c9398f61fdd21a9a9","9135d45c32e54e32bf97048c1a7aba4f","a52a4848a24646e8b1a111fff70dc079","861872fbb56c498eb5a89bb2cfb8e078","d2339f71a5384a4fa1807f46a53ed66b","da787e78436a41fa82605d3cb3e23d8d","0cd2788e10bb46f997bbb481517451c1","bee7857f9c62450387c6ba65913a38d1","80cca3fb62034e6fa8d4e7d1ef81e8f2","78307ed7f3904ab08f99d10f122e4488","32663aa4f2184feab4b5c0f4e6922348","ce570b6effba42e783770de78086eb9d","3b5062f4f7354467acbe06e930ec6dad"]},"executionInfo":{"elapsed":29487,"status":"ok","timestamp":1715645515356,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"laMVh5TPJM_e","outputId":"5d330d1f-5727-4260-ed3e-56796a13fff3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Progress file /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_mistral_q8_0_llamacpp_guided.csv does not exist, creating new one.\n","Loaded dataset with 150 samples from /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/data/VAL_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\n","Loaded progress file with 0 processed examples from /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_mistral_q8_0_llamacpp_guided.csv\n","Examples yet to process in dataset: 150\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/150 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ab4a1aa02949dbadaafc12cb46ae1c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52a4848a24646e8b1a111fff70dc079"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/outlines/fsm/regex.py:663: NumbaPendingDeprecationWarning: \n","Encountered the use of a type that is scheduled for deprecation: type 'reflected set' found for argument 'fsm_finals' of function '_walk_fsm'.\n","\n","For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n","\n","File \"../usr/local/lib/python3.10/dist-packages/outlines/fsm/regex.py\", line 415:\n","@numba.njit(nogil=True, cache=True)\n","def _walk_fsm(\n","^\n","\n","  state_seq = _walk_fsm(\n","/usr/local/lib/python3.10/dist-packages/numba/core/ir_utils.py:2172: NumbaPendingDeprecationWarning: \n","Encountered the use of a type that is scheduled for deprecation: type 'reflected set' found for argument 'fsm_finals' of function 'state_scan_tokens'.\n","\n","For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n","\n","File \"../usr/local/lib/python3.10/dist-packages/outlines/fsm/regex.py\", line 651:\n","@numba.njit(cache=True, nogil=True)\n","def state_scan_tokens(\n","^\n","\n","  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"]},{"output_type":"stream","name":"stdout","text":["Created outlines generator in 29.01 seconds.\n","Preparation complete for run: val_mistral_q8_0_llamacpp_guided\n"]}],"source":["##########################################################################################################################################################\n","# ------------------- run name (adjust for every new run!) ------------------ #\n","run_type = \"val\" # \"val\" (= test) or \"inf\"\n","run_model = \"mistral\" # mistral/llama3/... + _ft_version if finetuned\n","run_model_quant = \"q8_0\"\n","run_framework = \"llamacpp\" # llamacpp/vllm/transformers/...\n","additional_suffix = \"_guided\"\n","# --------------------------------------------------------------------------- #\n","run_name = f\"{run_type}_{run_model}_{run_model_quant}_{run_framework}{additional_suffix}\"\n","data_file_path = f\"{data_path}/{run_type.upper()}_transcript_chunks_nvids45968_chunksize2048_overlap50_tokMistral_with_metadata_for_prompt.csv\"\n","progress_file_path = f\"{output_path}/{run_name}.csv\" # file does not need to exist yet\n","\n","# prompt formatting arguments (for our custom function)\n","prompt_format_kwargs={\n","    \"prompt_format\": \"mistral\",\n","    \"include_bos\": True, # does llamacpp add it or not?\n","    \"include_answer_tease\": True,\n","    \"include_label\": False, # never for inference, only used for finetuning\n","    \"include_eos\": False}\n","\n","# model parameters (might be model-specific)\n","temperature = 0.01 # very low / zero for our task\n","top_k = 50 # relatively high value to avoid problems with guided generation (i.e. what if token required by schema is not in top k?)\n","seed = 42 # relevant for sampling (CAUTION: can't be used with llamacpp & outlines==0.0.36)\n","max_tokens = 1250 # might have to be set for llamacpp to override default? unsure...\n","\n","# how are we calling the model? with guided generation (outlines) active, or directly via llama_cpp?\n","guided_generation = True\n","\n","# outlines-specific\n","apply_reset_generator_fix = True # in some configurations with outlines & llamacpp the outlines.generator.json() needs to be reset before each new call\n","generator_whitespace_pattern = r\"[ \\n\\t]?\" # should we allow whitespaces/newlines/etc. being generated within the json structure? probably yes, but limit to zero or one at a time\n","def get_generator(whitespace_pattern): # to make sure we call it with the same parameters everywhere (e.g. whitespace pattern)\n","    return outlines.generate.json(model, output_json_schema_string, whitespace_pattern=whitespace_pattern)\n","\n","# other run parameters\n","skip_previous_errors = True # skip examples with True in 'error?' column?\n","replace_previous_errors = True # only relevant if skip_previous_errors is False: should previous errors be replaced with new outputs (if available) in progress file?\n","save_interval = 50 # update progress file every n examples (note: should not be set too low because saving to and loading from drive might not actually update files instantly? note sure...)\n","print_progress_interval = 10 # print progress every n examples\n","print_errors = True # useful to catch issues, but disable for final inference runs with confirmed problem-free models/code (to avoid crashes due to huge cell output)\n","\n","##########################################################################################################################################################\n","\n","# check if files exist\n","import os\n","if not os.path.exists(data_file_path):\n","    raise ValueError(f\"Data file {data_file_path} does not exist!\")\n","if not os.path.exists(progress_file_path):\n","    print(f\"Progress file {progress_file_path} does not exist, creating new one.\")\n","    pd.DataFrame(columns=[\"video_id\", \"chunk_number\", \"output\", \"error?\"]).to_csv(progress_file_path, sep=\";\", index=False)\n","\n","# load data\n","data = pd.read_csv(data_file_path, sep=\";\")\n","print(f\"Loaded dataset with {len(data)} samples from {data_file_path}\")\n","\n","# load progress data\n","progress = pd.read_csv(progress_file_path, sep=\";\")\n","print(f\"Loaded progress file with {len(progress)} processed examples from {progress_file_path}\")\n","\n","# merge progress data with main data (-> adds 'output' and 'error?' columns from progress df)\n","data = pd.merge(data, progress, on=[\"video_id\", \"chunk_number\"], how=\"left\")\n","\n","# filter out already processed examples (and optionally previous examples which caused errors)\n","data = data[data['output'].isnull()]\n","if skip_previous_errors:\n","    data = data[data['error?'] != True] # None and False should both stay in, only filter out True\n","print(f\"Examples yet to process in dataset: {len(data)}\")\n","\n","# convert to hf dataset\n","data = Dataset.from_pandas(data)\n","\n","# add prompts\n","data = data.map(format_prompt, fn_kwargs=prompt_format_kwargs)\n","\n","# create outlines generator (and check time)\n","if guided_generation:\n","    start_time = time.time()\n","    generator = get_generator(generator_whitespace_pattern)\n","    print(f\"Created outlines generator in {round(time.time() - start_time, 2)} seconds.\")\n","\n","print(f\"Preparation complete for run: {run_name}\")"]},{"cell_type":"markdown","source":["### Inference Loop"],"metadata":{"id":"CF9dKXx00TFD"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":539057,"status":"ok","timestamp":1715646080782,"user":{"displayName":"J J","userId":"04185061231825563434"},"user_tz":-120},"id":"BgdQttaOCHLM","outputId":"9c07dd5f-d6b2-4dc6-b87f-147bc16fbe7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["*** STARTING INFERENCE FOR 150 EXAMPLES (run name: val_mistral_q8_0_llamacpp_guided) ***\n","------------------------------------------------------------\n","Processed 10 total examples in 0 h, 0 min, 30.43 sec\n","Processed 20 total examples in 0 h, 1 min, 3.09 sec\n","Processed 30 total examples in 0 h, 1 min, 49.22 sec\n","Processed 40 total examples in 0 h, 2 min, 42.63 sec\n","  - Saved 50 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_mistral_q8_0_llamacpp_guided.csv\n","Processed 50 total examples in 0 h, 3 min, 11.74 sec\n","Processed 60 total examples in 0 h, 3 min, 46.75 sec\n","Processed 70 total examples in 0 h, 4 min, 33.47 sec\n","Processed 80 total examples in 0 h, 5 min, 8.7 sec\n","Processed 90 total examples in 0 h, 5 min, 50.95 sec\n","  - Saved 50 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_mistral_q8_0_llamacpp_guided.csv\n","Processed 100 total examples in 0 h, 6 min, 16.03 sec\n","Processed 110 total examples in 0 h, 6 min, 46.16 sec\n","Processed 120 total examples in 0 h, 7 min, 14.36 sec\n","Processed 130 total examples in 0 h, 7 min, 54.5 sec\n","Processed 140 total examples in 0 h, 8 min, 35.52 sec\n","  - Saved 50 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_mistral_q8_0_llamacpp_guided.csv\n","Processed 150 total examples in 0 h, 8 min, 58.75 sec\n","  - Saved 0 new results to /content/drive/MyDrive/Data_Science_Studies/thesis_colab_notebooks/outputs/val_mistral_q8_0_llamacpp_guided.csv\n","------------------------------------------------------------\n","Finished processing 150 examples in 0 h, 8 min, 58.76 sec\n"]}],"source":["# <- STARTS ITERATION\n","\n","# helper function to print elapsed time\n","def get_elapsed_time_str(start_time):\n","    hours, rem = divmod(time.time() - start_time, 3600)\n","    minutes, seconds = divmod(rem, 60)\n","    return f\"{int(hours)} h, {int(minutes)} min, {round(seconds, 2)} sec\"\n","\n","# helper function for saving progress\n","def update_progress_file(new_results, progress_file_path, replace_previous_errors):\n","#    1. convert to pandas df\n","    new_results_df = pd.DataFrame(new_results, columns=[\"video_id\", \"chunk_number\", \"output\", \"error?\"])\n","    # 2. load already processed data and append new results (if it exists)\n","    if os.path.exists(progress_file_path):\n","        progress_df = pd.read_csv(progress_file_path, sep=\";\")\n","        progress_df = pd.concat([progress_df, new_results_df], ignore_index=True)\n","        # drop duplicate error rows (duplicates here should only happen for previous error rows which we included and got errors again)\n","        progress_df = progress_df.drop_duplicates(subset=[\"video_id\", \"chunk_number\", \"error?\"], keep=\"last\")\n","    else:\n","        progress_df = new_results_df\n","\n","    #3. optionally, if there are new results for examples which previously caused errors, replace the error rows (otherwise: keep both the error row and the new result row in the file)\n","    if replace_previous_errors:\n","        # note: since we used pd.concat above we know that new results are appended at the end of the df, which allows us to use keep=\"last\" here to keep the new result row (without sorting first)\n","        progress_df = progress_df.drop_duplicates(subset=[\"video_id\", \"chunk_number\"], keep=\"last\")\n","    # 3. save\n","    progress_df.to_csv(progress_file_path, sep=\";\", index=False)\n","    print(f\"  - Saved {len(new_results)} new results to {progress_file_path}\")\n","\n","print(f\"*** STARTING INFERENCE FOR {len(data)} EXAMPLES (run name: {run_name}) ***\\n{'-'*60}\")\n","\n","# inference loop\n","new_results = []\n","start_time = time.time()\n","for i, example in enumerate(data):\n","\n","    video_id = example['video_id']\n","    uploader_id = example['uploader_id']\n","    chunk_number = example['chunk_number']\n","\n","    # model call\n","    try:\n","        if guided_generation:\n","            if apply_reset_generator_fix:\n","                generator = get_generator(generator_whitespace_pattern)\n","            # outlines model call -> dump json to string\n","            result = generator(example['prompt'], temperature=temperature, top_k=top_k, max_tokens=max_tokens)\n","            result = json.dumps(result)\n","\n","        else:\n","            # llamacpp model call -> try to extract and validate the json (the model output might contain additional text or invalid json)\n","            result = llm(example['prompt'], temperature=temperature, top_k=top_k, max_tokens=max_tokens)['choices'][0]['text']\n","            result = extract_json_from_output(result) # returns string or raises error\n","            # validate\n","            result = json.loads(result)\n","            jsonschema.validate(instance=result, schema=output_json_schema) # raises error if instance doesn't match schema\n","            result = json.dumps(result) # we convert back to string to ensure clean structure (no newlines etc.)\n","        # no error: append result\n","        new_results.append((video_id, chunk_number, result, False))\n","\n","    except Exception as e:\n","\n","        if print_errors:\n","            print(f\"Error at iteration {i} (video_id: {video_id}, chunk_number: {chunk_number}) - {e} \")\n","\n","        new_results.append((video_id, chunk_number, None, True))\n","\n","    # save progress\n","    if (i+1) % save_interval == 0:\n","        update_progress_file(new_results, progress_file_path, replace_previous_errors)\n","        new_results = []\n","\n","\n","\n","    # print progress\n","    if (i+1) % print_progress_interval == 0:\n","        print(f\"Processed {i+1} total examples in {get_elapsed_time_str(start_time)}\")\n","\n","\n","# save remaining examples\n","update_progress_file(new_results, progress_file_path, replace_previous_errors)\n","\n","print(f\"{'-'*60}\\nFinished processing {len(data)} examples in {get_elapsed_time_str(start_time)}\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb","timestamp":1710611387839}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"aaa65fd117fb4995a1d58c4adf1318ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60e3b81a8dea432785f51fed722b072b","IPY_MODEL_fb2de11ea0a74a6a9500aac2c3319004","IPY_MODEL_7c48258d73294f1c8d67ec9ea8f41864"],"layout":"IPY_MODEL_6c87b4f2eca44387a31116fe5955d984"}},"60e3b81a8dea432785f51fed722b072b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7507fc2c42a48d19ef12309096c5457","placeholder":"​","style":"IPY_MODEL_60ec28a341164ff6b28393577c0e4804","value":"Map: 100%"}},"fb2de11ea0a74a6a9500aac2c3319004":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b6242123e124f43baca92d190f19d67","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac93e51efe4949c38a2810d79fbf1780","value":150}},"7c48258d73294f1c8d67ec9ea8f41864":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63cc5ae792084162a42ff90c31dc637c","placeholder":"​","style":"IPY_MODEL_e945eaf96d9c4ef8a5830b4ec42200eb","value":" 150/150 [00:00&lt;00:00, 1917.52 examples/s]"}},"6c87b4f2eca44387a31116fe5955d984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7507fc2c42a48d19ef12309096c5457":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60ec28a341164ff6b28393577c0e4804":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b6242123e124f43baca92d190f19d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac93e51efe4949c38a2810d79fbf1780":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63cc5ae792084162a42ff90c31dc637c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e945eaf96d9c4ef8a5830b4ec42200eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86b14b30b691460d9e54de16461d0899":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1422029e84144042a88bc8993b016ef8","IPY_MODEL_ce1bc4ab6bb04cdd92c635d92367be33","IPY_MODEL_41a83ecdcba84fcab42f00572662d0e8"],"layout":"IPY_MODEL_19eb5713cdbb4a1aad6ffd65271d89e1"}},"1422029e84144042a88bc8993b016ef8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65cdbc3d98b34294a5bee312f2e7d97a","placeholder":"​","style":"IPY_MODEL_cc4062e3a4f841aebddfda66dc0ac88f","value":"mistral-7b-instruct-v0.2.Q8_0.gguf: 100%"}},"ce1bc4ab6bb04cdd92c635d92367be33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10f0f990520c45219926ce5085eebd8c","max":7695857952,"min":0,"orientation":"horizontal","style":"IPY_MODEL_883e1696bab6450db8d0e2bdf59f7142","value":7695857952}},"41a83ecdcba84fcab42f00572662d0e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_576199fbc35d412e94a35eac53086a5e","placeholder":"​","style":"IPY_MODEL_3de3eea4ce0040e9bee94d14d113dc95","value":" 7.70G/7.70G [00:47&lt;00:00, 193MB/s]"}},"19eb5713cdbb4a1aad6ffd65271d89e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65cdbc3d98b34294a5bee312f2e7d97a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc4062e3a4f841aebddfda66dc0ac88f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10f0f990520c45219926ce5085eebd8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"883e1696bab6450db8d0e2bdf59f7142":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"576199fbc35d412e94a35eac53086a5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3de3eea4ce0040e9bee94d14d113dc95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27ab4a1aa02949dbadaafc12cb46ae1c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c040ebdf2fc0474e98f5b95566f3e523","IPY_MODEL_6b0eb6d188ae4a348cc6edf298225ce9","IPY_MODEL_b85205fce08f415cb00a287ed0b79739"],"layout":"IPY_MODEL_be4a3d73713940d190ec70aa7f34aab8"}},"c040ebdf2fc0474e98f5b95566f3e523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e1dfd27c49a4a859f20fd231db5b6e3","placeholder":"​","style":"IPY_MODEL_a97afdae59654c01bfda4336418d3496","value":"Map: 100%"}},"6b0eb6d188ae4a348cc6edf298225ce9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a48316707ddb43628fb0d5ea41c7cc04","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d226f04515cb4e50849feaf9f7737cf9","value":150}},"b85205fce08f415cb00a287ed0b79739":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af37c2645946436c9398f61fdd21a9a9","placeholder":"​","style":"IPY_MODEL_9135d45c32e54e32bf97048c1a7aba4f","value":" 150/150 [00:00&lt;00:00, 2465.33 examples/s]"}},"be4a3d73713940d190ec70aa7f34aab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e1dfd27c49a4a859f20fd231db5b6e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97afdae59654c01bfda4336418d3496":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a48316707ddb43628fb0d5ea41c7cc04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d226f04515cb4e50849feaf9f7737cf9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af37c2645946436c9398f61fdd21a9a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9135d45c32e54e32bf97048c1a7aba4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a52a4848a24646e8b1a111fff70dc079":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_861872fbb56c498eb5a89bb2cfb8e078","IPY_MODEL_d2339f71a5384a4fa1807f46a53ed66b","IPY_MODEL_da787e78436a41fa82605d3cb3e23d8d"],"layout":"IPY_MODEL_0cd2788e10bb46f997bbb481517451c1"}},"861872fbb56c498eb5a89bb2cfb8e078":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bee7857f9c62450387c6ba65913a38d1","placeholder":"​","style":"IPY_MODEL_80cca3fb62034e6fa8d4e7d1ef81e8f2","value":""}},"d2339f71a5384a4fa1807f46a53ed66b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78307ed7f3904ab08f99d10f122e4488","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32663aa4f2184feab4b5c0f4e6922348","value":0}},"da787e78436a41fa82605d3cb3e23d8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce570b6effba42e783770de78086eb9d","placeholder":"​","style":"IPY_MODEL_3b5062f4f7354467acbe06e930ec6dad","value":" 0/0 [00:00&lt;?, ?it/s]"}},"0cd2788e10bb46f997bbb481517451c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bee7857f9c62450387c6ba65913a38d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80cca3fb62034e6fa8d4e7d1ef81e8f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78307ed7f3904ab08f99d10f122e4488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"32663aa4f2184feab4b5c0f4e6922348":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce570b6effba42e783770de78086eb9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b5062f4f7354467acbe06e930ec6dad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}